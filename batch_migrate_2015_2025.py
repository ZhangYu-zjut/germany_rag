#!/usr/bin/env python3
"""
2015-2025å¹´å¾·å›½è®®ä¼šæ•°æ®å¤§è§„æ¨¡æ‰¹é‡è¿ç§»ç³»ç»Ÿ
ä½¿ç”¨æœ€æ–°çš„GPUä¼˜åŒ–å‚æ•°ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œæ™ºèƒ½è¿›åº¦ç®¡ç†
"""

import json
import os
import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import threading
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

from src.vectordb.qdrant_client import create_qdrant_client
from src.llm.embeddings import GeminiEmbeddingClient
from src.utils.logger import logger
from src.data_loader.splitter import ParliamentTextSplitter

@dataclass
class MigrationTask:
    """è¿ç§»ä»»åŠ¡æ•°æ®ç±»"""
    year: int
    file_path: Path
    file_size_mb: float
    estimated_records: int
    status: str = "pending"  # pending, processing, completed, failed
    start_time: float = 0.0
    end_time: float = 0.0
    actual_records: int = 0
    chunks_count: int = 0
    error_message: str = ""

class BatchMigrator2015to2025:
    """2015-2025å¹´æ‰¹é‡è¿ç§»å™¨"""
    
    def __init__(
        self,
        data_dir: str = "./data/pp_json_49-21",
        collection_name: str = "german_parliament",
        embedding_batch_size: int = 800,  # ğŸš€ ä½¿ç”¨æœ€æ–°GPUä¼˜åŒ–å‚æ•°
        qdrant_batch_size: int = 200,     # ä¼˜åŒ–Qdrantæ’å…¥
        max_concurrent_years: int = 1,    # é»˜è®¤ä¸²è¡Œå¤„ç†ï¼Œé¿å…GPUç«äº‰
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        force_recreate_collection: bool = False,
        resume_from_checkpoint: bool = True
    ):
        self.data_dir = Path(data_dir)
        self.collection_name = collection_name
        self.embedding_batch_size = embedding_batch_size
        self.qdrant_batch_size = qdrant_batch_size
        self.max_concurrent_years = max_concurrent_years
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.force_recreate_collection = force_recreate_collection
        self.resume_from_checkpoint = resume_from_checkpoint
        
        # è¿›åº¦ç®¡ç†
        self.progress_file = Path("batch_migration_progress.json")
        self.completed_years = set()
        self.failed_years = set()
        
        # åˆå§‹åŒ–ç»„ä»¶
        logger.info("[BatchMigrator] ğŸš€ åˆå§‹åŒ–å¤§è§„æ¨¡è¿ç§»ç³»ç»Ÿ...")
        
        # å†…å­˜ä¼˜åŒ–ï¼šæ£€æŸ¥å¯ç”¨å†…å­˜
        import psutil
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        logger.info(f"[BatchMigrator] ğŸ’¾ å¯ç”¨å†…å­˜: {available_memory_gb:.1f}GB")
        
        if available_memory_gb < 8.0:
            logger.warning("âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³8GBï¼Œå°†ä½¿ç”¨ä¿å®ˆå‚æ•°")
            self.embedding_batch_size = min(self.embedding_batch_size, 200)
            self.qdrant_batch_size = min(self.qdrant_batch_size, 50)
            logger.info(f"   è°ƒæ•´åembedding batch_size: {self.embedding_batch_size}")
            logger.info(f"   è°ƒæ•´åqdrant batch_size: {self.qdrant_batch_size}")
        
        # è®¾ç½®Qdrantç¯å¢ƒ
        os.environ["QDRANT_MODE"] = "local"
        os.environ["QDRANT_LOCAL_PATH"] = "./data/qdrant"
        
        logger.info("[BatchMigrator] ğŸ”— åˆ›å»ºQdrantå®¢æˆ·ç«¯...")
        self.qdrant_client = create_qdrant_client()
        
        logger.info("[BatchMigrator] ğŸ¤– åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯...")
        try:
            self.embedding_client = GeminiEmbeddingClient(embedding_mode="local")
            logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # å¼ºåˆ¶é‡Šæ”¾å†…å­˜
            import gc
            gc.collect()
            raise
        self.text_splitter = ParliamentTextSplitter(
            chunk_size=self.chunk_size, 
            chunk_overlap=self.chunk_overlap
        )
        
        logger.info(f"[BatchMigrator] âœ… åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - GPUä¼˜åŒ–batch_size: {self.embedding_batch_size}")
        logger.info(f"   - Qdrant batch_size: {self.qdrant_batch_size}")
        logger.info(f"   - æœ€å¤§å¹¶å‘å¹´ä»½: {self.max_concurrent_years}")
        
    def discover_data_files(self, year_range: Tuple[int, int] = (2015, 2025)) -> List[MigrationTask]:
        """å‘ç°å¹¶åˆ†ææ•°æ®æ–‡ä»¶"""
        
        logger.info(f"[BatchMigrator] ğŸ” å‘ç°{year_range[0]}-{year_range[1]}å¹´æ•°æ®æ–‡ä»¶...")
        
        tasks = []
        
        for year in range(year_range[0], year_range[1] + 1):
            # ç‰¹æ®Šå¤„ç†2021å¹´åˆå¹¶æ–‡ä»¶
            if year == 2021:
                file_path = self.data_dir / "pp_2021_merged.json"
            else:
                file_path = self.data_dir / f"pp_{year}.json"
            
            if file_path.exists():
                file_size = file_path.stat().st_size
                file_size_mb = file_size / (1024 * 1024)
                
                # æ ¹æ®æ–‡ä»¶å¤§å°ä¼°ç®—è®°å½•æ•° (ç»éªŒå€¼ï¼š1MB â‰ˆ 250æ¡è®°å½•)
                estimated_records = int(file_size_mb * 250)
                
                task = MigrationTask(
                    year=year,
                    file_path=file_path,
                    file_size_mb=file_size_mb,
                    estimated_records=estimated_records
                )
                tasks.append(task)
                
                logger.info(f"   ğŸ“ {year}: {file_size_mb:.1f}MB (~{estimated_records:,}æ¡è®°å½•)")
            else:
                logger.warning(f"   âš ï¸  {year}: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
        
        logger.info(f"[BatchMigrator] å‘ç° {len(tasks)} ä¸ªå¹´ä»½çš„æ•°æ®æ–‡ä»¶")
        
        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå°æ–‡ä»¶ä¼˜å…ˆï¼Œä¾¿äºå¿«é€Ÿçœ‹åˆ°æ•ˆæœï¼‰
        tasks.sort(key=lambda x: x.file_size_mb)
        
        return tasks
    
    def load_checkpoint(self) -> Dict[str, Any]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        
        if self.resume_from_checkpoint and self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    checkpoint = json.load(f)
                
                self.completed_years = set(checkpoint.get('completed_years', []))
                self.failed_years = set(checkpoint.get('failed_years', []))
                
                logger.info(f"[BatchMigrator] ğŸ“‹ åŠ è½½æ£€æŸ¥ç‚¹:")
                logger.info(f"   - å·²å®Œæˆå¹´ä»½: {sorted(self.completed_years)}")
                logger.info(f"   - å¤±è´¥å¹´ä»½: {sorted(self.failed_years)}")
                
                return checkpoint
                
            except Exception as e:
                logger.error(f"[BatchMigrator] âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {str(e)}")
                
        return {"completed_years": [], "failed_years": []}
    
    def save_checkpoint(self, tasks: List[MigrationTask]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        
        checkpoint = {
            "timestamp": datetime.now().isoformat(),
            "completed_years": list(self.completed_years),
            "failed_years": list(self.failed_years),
            "tasks_status": [
                {
                    "year": task.year,
                    "status": task.status,
                    "actual_records": task.actual_records,
                    "chunks_count": task.chunks_count,
                    "duration": task.end_time - task.start_time if task.end_time > 0 else 0,
                    "error_message": task.error_message
                }
                for task in tasks
            ]
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, indent=2, ensure_ascii=False)
            logger.debug(f"[BatchMigrator] ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"[BatchMigrator] âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {str(e)}")
    
    def ensure_collection(self) -> bool:
        """ç¡®ä¿Qdranté›†åˆå­˜åœ¨"""
        
        try:
            logger.info(f"[BatchMigrator] ğŸ—ï¸  ç¡®ä¿é›†åˆå­˜åœ¨: {self.collection_name}")
            
            success = self.qdrant_client.create_collection_for_german_parliament(
                collection_name=self.collection_name,
                force_recreate=self.force_recreate_collection
            )
            
            if success:
                # è·å–å½“å‰é›†åˆä¿¡æ¯
                try:
                    info = self.qdrant_client.get_collection_info(self.collection_name)
                    logger.info(f"   âœ… é›†åˆçŠ¶æ€: {info['points_count']} ä¸ªæ•°æ®ç‚¹")
                    return True
                except Exception as e:
                    logger.warning(f"   âš ï¸  æ— æ³•è·å–é›†åˆä¿¡æ¯: {str(e)}")
                    return True
            else:
                logger.error(f"[BatchMigrator] âŒ é›†åˆåˆ›å»ºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"[BatchMigrator] âŒ é›†åˆæ“ä½œå¼‚å¸¸: {str(e)}")
            return False
    
    def migrate_single_year(self, task: MigrationTask) -> bool:
        """è¿ç§»å•ä¸ªå¹´ä»½çš„æ•°æ®"""
        
        logger.info(f"[BatchMigrator] ğŸš€ å¼€å§‹è¿ç§» {task.year} å¹´æ•°æ®")
        logger.info(f"   æ–‡ä»¶: {task.file_path}")
        logger.info(f"   å¤§å°: {task.file_size_mb:.1f}MB")
        
        task.status = "processing"
        task.start_time = time.time()
        
        try:
            # 1. è¯»å–JSONæ•°æ®
            logger.info(f"[{task.year}] ğŸ“– è¯»å–JSONæ–‡ä»¶...")
            with open(task.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            transcript = data.get('transcript', [])
            task.actual_records = len(transcript)
            
            logger.info(f"[{task.year}] ğŸ“Š å®é™…è®°å½•æ•°: {task.actual_records:,}")
            
            # 2. æ–‡æœ¬åˆ†å—å¤„ç†
            logger.info(f"[{task.year}] âœ‚ï¸ å¼€å§‹æ–‡æœ¬åˆ†å—...")
            all_chunks = []
            
            for record in transcript:
                if not isinstance(record, dict):
                    continue
                
                text_content = record.get('speech', '')
                if not text_content or len(text_content.strip()) < 50:
                    continue
                
                # æ–‡æœ¬åˆ†å—
                chunks = self.text_splitter.text_splitter.split_text(text_content)
                
                for chunk_idx, chunk in enumerate(chunks):
                    if len(chunk.strip()) < 30:
                        continue
                    
                    metadata = record.get("metadata", {})
                    chunk_data = {
                        "text": chunk,
                        "metadata": {
                            "year": int(metadata.get("year", task.year)) if metadata.get("year") else task.year,
                            "month": int(metadata.get("month", 0)) if metadata.get("month") else None,
                            "day": int(metadata.get("day", 0)) if metadata.get("day") else None,
                            "speaker": metadata.get("speaker", ""),
                            "party": metadata.get("group", ""),
                            "group": metadata.get("group", ""),
                            "group_chinese": metadata.get("group_chinese", ""),
                            "session": metadata.get("session", ""),
                            "lp": int(metadata.get("lp", 0)) if metadata.get("lp") else None,
                            "text_id": metadata.get("id", ""),
                            "source_file": task.file_path.name,
                            "topics": self._extract_topics(chunk)
                        }
                    }
                    all_chunks.append(chunk_data)
            
            task.chunks_count = len(all_chunks)
            logger.info(f"[{task.year}] âœ… åˆ†å—å®Œæˆ: {task.chunks_count:,} chunks")
            
            # 3. æ‰¹é‡ç”Ÿæˆembeddingå¹¶æ’å…¥
            logger.info(f"[{task.year}] ğŸ§  æ‰¹é‡ç”Ÿæˆembedding...")
            
            current_point_id = int(time.time() * 1000000) % 1000000000  # åŸºäºæ—¶é—´æˆ³ç”Ÿæˆèµ·å§‹ID
            
            for i in range(0, len(all_chunks), self.embedding_batch_size):
                batch_chunks = all_chunks[i : i + self.embedding_batch_size]
                texts_to_embed = [chunk["text"] for chunk in batch_chunks]
                
                # æ‰¹é‡ç”Ÿæˆembedding
                vectors = self.embedding_client.embed_batch(
                    texts_to_embed,
                    batch_size=self.embedding_batch_size
                )
                
                # å‡†å¤‡æ•°æ®ç‚¹
                points_to_upsert = []
                for j, (chunk_data, vector) in enumerate(zip(batch_chunks, vectors)):
                    if self._is_valid_vector(vector):
                        payload = chunk_data["metadata"]
                        payload["text"] = chunk_data["text"]
                        
                        points_to_upsert.append({
                            "id": current_point_id,
                            "vector": vector,
                            "payload": payload
                        })
                        current_point_id += 1
                
                    # æ‰¹é‡æ’å…¥åˆ°Qdrant
                if points_to_upsert:
                    try:
                        self.qdrant_client.upsert_german_parliament_data(
                            collection_name=self.collection_name,
                            data_points=points_to_upsert
                        )
                    except Exception as e:
                        logger.error(f"[{task.year}] âŒ Qdrantæ’å…¥å¤±è´¥: {str(e)}")
                        # é‡è¯•æœºåˆ¶
                        for retry in range(3):
                            try:
                                logger.info(f"[{task.year}] ğŸ”„ é‡è¯•æ’å…¥ ({retry+1}/3)")
                                time.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿
                                self.qdrant_client.upsert_german_parliament_data(
                                    collection_name=self.collection_name,
                                    data_points=points_to_upsert
                                )
                                break
                            except Exception as retry_e:
                                logger.warning(f"[{task.year}] âš ï¸  é‡è¯•å¤±è´¥: {str(retry_e)}")
                                if retry == 2:  # æœ€åä¸€æ¬¡é‡è¯•
                                    raise retry_e
                
                # è¿›åº¦æŠ¥å‘Šå’Œæ£€æŸ¥ç‚¹ä¿å­˜
                processed = min(i + self.embedding_batch_size, len(all_chunks))
                progress = (processed / len(all_chunks)) * 100
                logger.info(f"[{task.year}] ğŸ“Š è¿›åº¦: {progress:.1f}% ({processed}/{len(all_chunks)})")
                
                # æ¯10%æˆ–æ¯5000ä¸ªchunksä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
                if processed % 5000 == 0 or progress % 10 == 0:
                    logger.info(f"[{task.year}] ğŸ’¾ ä¿å­˜è¿›åº¦æ£€æŸ¥ç‚¹...")
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    temp_task = task
                    temp_task.chunks_count = processed
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„è¿›åº¦ä¿å­˜é€»è¾‘
                
                # å†…å­˜æ¸…ç† (æ¯20æ‰¹æ¬¡)
                if (i // self.embedding_batch_size) % 20 == 0:
                    import gc
                    gc.collect()
                    logger.debug(f"[{task.year}] ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")
            
            task.end_time = time.time()
            task.status = "completed"
            
            duration = task.end_time - task.start_time
            logger.info(f"[BatchMigrator] ğŸ‰ {task.year}å¹´è¿ç§»å®Œæˆ!")
            logger.info(f"   è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
            logger.info(f"   è®°å½•æ•°: {task.actual_records:,}")
            logger.info(f"   chunksæ•°: {task.chunks_count:,}")
            logger.info(f"   å¹³å‡é€Ÿåº¦: {task.chunks_count/duration:.1f} chunks/ç§’")
            
            return True
            
        except Exception as e:
            task.end_time = time.time()
            task.status = "failed"
            task.error_message = str(e)
            
            logger.error(f"[BatchMigrator] âŒ {task.year}å¹´è¿ç§»å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            
            return False
    
    def _extract_topics(self, text: str) -> List[str]:
        """ç®€å•çš„ä¸»é¢˜æå–"""
        topics = []
        keywords_map = {
            "Klimaschutz": ["klimaschutz", "klima", "umwelt", "co2", "emission"],
            "Digitalisierung": ["digital", "internet", "computer", "technologie"],
            "Wirtschaft": ["wirtschaft", "unternehmen", "arbeitsplatz", "job"],
            "Bildung": ["bildung", "schule", "universitÃ¤t", "student"],
            "Gesundheit": ["gesundheit", "medizin", "krankenhaus", "arzt"],
            "Migration": ["migration", "flÃ¼chtling", "asyl", "integration"],
            "Energie": ["energie", "strom", "erneuerbare", "atomkraft"]
        }
        text_lower = text.lower()
        for topic, keywords in keywords_map.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        return topics
    
    def _is_valid_vector(self, vector: List[float]) -> bool:
        """æ£€æŸ¥å‘é‡æ˜¯å¦æœ‰æ•ˆ"""
        import numpy as np
        if not isinstance(vector, (list, np.ndarray)):
            return False
        if len(vector) != 1024:  # BGE-M3 å‘é‡ç»´åº¦
            return False
        vec_np = np.array(vector)
        if np.any(np.isnan(vec_np)) or np.any(np.isinf(vec_np)):
            return False
        if np.all(vec_np == 0):
            return False
        return True
    
    def execute_batch_migration(self, year_range: Tuple[int, int] = (2015, 2025)) -> bool:
        """æ‰§è¡Œæ‰¹é‡è¿ç§»"""
        
        logger.info(f"[BatchMigrator] ğŸ¯ å¼€å§‹æ‰§è¡Œå¤§è§„æ¨¡æ‰¹é‡è¿ç§»")
        logger.info(f"ç›®æ ‡å¹´ä»½èŒƒå›´: {year_range[0]}-{year_range[1]}")
        logger.info("=" * 80)
        
        # 1. å‘ç°æ•°æ®æ–‡ä»¶
        tasks = self.discover_data_files(year_range)
        if not tasks:
            logger.error("[BatchMigrator] âŒ æœªå‘ç°ä»»ä½•æ•°æ®æ–‡ä»¶")
            return False
        
        # 2. åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self.load_checkpoint()
        
        # 3. ç¡®ä¿é›†åˆå­˜åœ¨
        if not self.ensure_collection():
            logger.error("[BatchMigrator] âŒ é›†åˆåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # 4. è¿‡æ»¤å·²å®Œæˆçš„ä»»åŠ¡
        pending_tasks = [task for task in tasks if task.year not in self.completed_years]
        
        logger.info(f"[BatchMigrator] ğŸ“‹ ä»»åŠ¡ç»Ÿè®¡:")
        logger.info(f"   æ€»ä»»åŠ¡æ•°: {len(tasks)}")
        logger.info(f"   å·²å®Œæˆ: {len(self.completed_years)}")
        logger.info(f"   å¾…å¤„ç†: {len(pending_tasks)}")
        logger.info(f"   å¤±è´¥ä»»åŠ¡: {len(self.failed_years)}")
        
        if not pending_tasks:
            logger.info("[BatchMigrator] ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ!")
            return True
        
        # 5. æ‰§è¡Œè¿ç§»
        start_time = time.time()
        successful_count = 0
        failed_count = 0
        
        for i, task in enumerate(pending_tasks):
            logger.info(f"\n[BatchMigrator] ğŸ“‹ ä»»åŠ¡è¿›åº¦: {i+1}/{len(pending_tasks)}")
            
            success = self.migrate_single_year(task)
            
            if success:
                self.completed_years.add(task.year)
                successful_count += 1
                logger.info(f"âœ… {task.year}å¹´è¿ç§»æˆåŠŸ")
            else:
                self.failed_years.add(task.year)
                failed_count += 1
                logger.error(f"âŒ {task.year}å¹´è¿ç§»å¤±è´¥")
            
            # ä¿å­˜è¿›åº¦
            self.save_checkpoint(tasks)
        
        # 6. è¿ç§»æ€»ç»“
        total_duration = time.time() - start_time
        
        logger.info(f"\n" + "=" * 80)
        logger.info(f"ğŸŠ æ‰¹é‡è¿ç§»å®Œæˆï¼")
        logger.info(f"=" * 80)
        logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        logger.info(f"   æˆåŠŸè¿ç§»: {successful_count} ä¸ªå¹´ä»½")
        logger.info(f"   è¿ç§»å¤±è´¥: {failed_count} ä¸ªå¹´ä»½")
        logger.info(f"   æ€»è€—æ—¶: {total_duration/60:.1f} åˆ†é’Ÿ")
        
        if successful_count > 0:
            avg_time_per_year = total_duration / successful_count
            logger.info(f"   å¹³å‡æ¯å¹´: {avg_time_per_year/60:.1f} åˆ†é’Ÿ")
        
        # ç»Ÿè®¡æ€»æ•°æ®é‡
        total_records = sum(task.actual_records for task in tasks if task.status == "completed")
        total_chunks = sum(task.chunks_count for task in tasks if task.status == "completed")
        
        logger.info(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        logger.info(f"   æ€»è®°å½•æ•°: {total_records:,}")
        logger.info(f"   æ€»chunksæ•°: {total_chunks:,}")
        
        # è·å–æœ€ç»ˆé›†åˆçŠ¶æ€
        try:
            final_info = self.qdrant_client.get_collection_info(self.collection_name)
            logger.info(f"ğŸ—ï¸  æœ€ç»ˆé›†åˆçŠ¶æ€:")
            logger.info(f"   é›†åˆåç§°: {self.collection_name}")
            logger.info(f"   æ•°æ®ç‚¹æ•°: {final_info['points_count']:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {final_info['vector_params']['size']}")
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
        
        logger.info("=" * 80)
        
        return failed_count == 0

def signal_handler(signum, frame):
    """ä¼˜é›…å¤„ç†ä¸­æ–­ä¿¡å·"""
    logger.warning(f"âš ï¸  æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦å¹¶é€€å‡º...")
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
    sys.exit(1)

def main():
    """ä¸»å‡½æ•°"""
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç³»ç»Ÿç»ˆæ­¢ä¿¡å·
    
    parser = argparse.ArgumentParser(description="å¾·å›½è®®ä¼š2015-2025å¹´æ•°æ®æ‰¹é‡è¿ç§»ç³»ç»Ÿ")
    
    parser.add_argument("--start-year", type=int, default=2015, help="èµ·å§‹å¹´ä»½")
    parser.add_argument("--end-year", type=int, default=2025, help="ç»“æŸå¹´ä»½")
    parser.add_argument("--data-dir", type=str, default="./data/pp_json_49-21", help="æ•°æ®ç›®å½•")
    parser.add_argument("--collection", type=str, default="german_parliament", help="Qdranté›†åˆåç§°")
    parser.add_argument("--embedding-batch-size", type=int, default=800, help="Embeddingæ‰¹å¤„ç†å¤§å° (GPUä¼˜åŒ–)")
    parser.add_argument("--qdrant-batch-size", type=int, default=200, help="Qdrantæ’å…¥æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--force-recreate", action="store_true", help="å¼ºåˆ¶é‡å»ºé›†åˆ")
    parser.add_argument("--no-resume", action="store_true", help="ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤")
    
    args = parser.parse_args()
    
    logger.info(f"ğŸš€ å¯åŠ¨2015-2025å¹´å¤§è§„æ¨¡æ•°æ®è¿ç§»ç³»ç»Ÿ")
    logger.info(f"å‚æ•°é…ç½®:")
    logger.info(f"   å¹´ä»½èŒƒå›´: {args.start_year}-{args.end_year}")
    logger.info(f"   æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"   Embedding batch_size: {args.embedding_batch_size} (GPUä¼˜åŒ–)")
    logger.info(f"   Qdrant batch_size: {args.qdrant_batch_size}")
    logger.info(f"   å¼ºåˆ¶é‡å»º: {args.force_recreate}")
    logger.info(f"   æ–­ç‚¹ç»­ä¼ : {not args.no_resume}")
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = BatchMigrator2015to2025(
        data_dir=args.data_dir,
        collection_name=args.collection,
        embedding_batch_size=args.embedding_batch_size,
        qdrant_batch_size=args.qdrant_batch_size,
        force_recreate_collection=args.force_recreate,
        resume_from_checkpoint=not args.no_resume
    )
    
    # æ‰§è¡Œè¿ç§»
    try:
        success = migrator.execute_batch_migration((args.start_year, args.end_year))
        
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®è¿ç§»æˆåŠŸå®Œæˆï¼")
            sys.exit(0)
        else:
            logger.error("âŒ éƒ¨åˆ†æ•°æ®è¿ç§»å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­è¿ç§»ï¼Œè¿›åº¦å·²ä¿å­˜")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ è¿ç§»ç³»ç»Ÿå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
