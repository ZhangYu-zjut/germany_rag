#!/usr/bin/env python3
"""
Pinecone + OpenAI text-embedding-3-large æ€§èƒ½æµ‹è¯•
æµ‹è¯•2015å¹´æ•°æ®å®Œæ•´æµç¨‹ï¼šåˆ†å— -> OpenAI embedding -> Pineconeå­˜å‚¨
"""

import os
import sys
import json
import time
import requests
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter

logger = setup_logger()

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    stage_name: str
    start_time: float
    end_time: float
    duration: float
    records_processed: int = 0
    chunks_generated: int = 0
    vectors_created: int = 0
    
    @property
    def duration_minutes(self) -> float:
        return self.duration / 60
    
    @property
    def records_per_second(self) -> float:
        return self.records_processed / self.duration if self.duration > 0 else 0
    
    @property
    def chunks_per_second(self) -> float:
        return self.chunks_generated / self.duration if self.duration > 0 else 0

class PineconePerformanceTest:
    """Pineconeæ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, year: int = 2015):
        self.year = year
        self.metrics: List[PerformanceMetrics] = []
        
        # Pineconeé…ç½®
        self.pinecone_api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pinecone_host = os.getenv("PINECONE_HOST")
        self.pinecone_region = os.getenv("PINECONE_REGION", "us-east-1")
        
        # OpenAIé…ç½®
        self.openai_api_key = os.getenv("OPENAI_API_KEY") or os.getenv("GEMINI_API_KEY")  # å¯èƒ½å­˜å‚¨åœ¨GEMINI_API_KEYä¸­
        self.openai_base_url = "https://api.openai.com/v1"
        self.embedding_model = "text-embedding-3-large"
        self.embedding_dimension = 1024
        
        # ç´¢å¼•åç§°
        self.index_name = f"german-parliament-{year}"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–Pinecone {year}å¹´æ•°æ®æ€§èƒ½æµ‹è¯•")
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        logger.info("ğŸ” éªŒè¯é…ç½®...")
        
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_VECTOR_DATABASE_API_KEY æœªé…ç½®")
        
        if not self.pinecone_host:
            raise ValueError("PINECONE_HOST æœªé…ç½®")
            
        if not self.openai_api_key:
            raise ValueError("OpenAI API Key æœªé…ç½®ï¼ˆOPENAI_API_KEY æˆ– GEMINI_API_KEYï¼‰")
        
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"  Pinecone Host: {self.pinecone_host}")
        logger.info(f"  Pinecone Region: {self.pinecone_region}")
        logger.info(f"  OpenAI Model: {self.embedding_model}")
        logger.info(f"  Embedding Dimension: {self.embedding_dimension}")
    
    def _record_metric(self, stage_name: str, start_time: float, end_time: float, **kwargs):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetrics(
            stage_name=stage_name,
            start_time=start_time,
            end_time=end_time,
            duration=end_time - start_time,
            **kwargs
        )
        self.metrics.append(metric)
        
        logger.info(f"âœ… {stage_name}: {metric.duration:.2f}ç§’ ({metric.duration_minutes:.2f}åˆ†é’Ÿ)")
        if metric.records_processed > 0:
            logger.info(f"   å¤„ç†é€Ÿåº¦: {metric.records_per_second:.1f} è®°å½•/ç§’")
        if metric.chunks_generated > 0:
            logger.info(f"   åˆ†å—é€Ÿåº¦: {metric.chunks_per_second:.1f} chunks/ç§’")
        if metric.vectors_created > 0:
            logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {metric.vectors_created/metric.duration:.1f} å‘é‡/ç§’")
    
    def stage_1_data_loading(self) -> List[Dict]:
        """é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ"""
        logger.info("ğŸ“– é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ")
        
        data_file = project_root / f"data/pp_json_49-21/pp_{self.year}.json"
        
        if not data_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°{self.year}å¹´æ•°æ®æ–‡ä»¶: {data_file}")
        
        start_time = time.time()
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        transcript = data.get('transcript', [])
        
        end_time = time.time()
        
        self._record_metric(
            "æ•°æ®åŠ è½½å’ŒJSONè§£æ",
            start_time,
            end_time,
            records_processed=len(transcript)
        )
        
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {len(transcript):,}æ¡è®°å½•, æ–‡ä»¶å¤§å°: {data_file.stat().st_size / (1024*1024):.1f}MB")
        
        return transcript
    
    def stage_2_text_chunking(self, transcript: List[Dict]) -> List[Dict]:
        """é˜¶æ®µ2: æ–‡æœ¬åˆ†å—"""
        logger.info("âœ‚ï¸  é˜¶æ®µ2: æ–‡æœ¬åˆ†å—")
        
        start_time = time.time()
        
        all_chunks = []
        valid_records = 0
        
        for i, record in enumerate(transcript):
            if not isinstance(record, dict):
                continue
            
            speech_text = record.get('speech', '').strip()
            if not speech_text or len(speech_text) < 10:
                continue
                
            valid_records += 1
            
            # åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(speech_text)
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = record.get('metadata', {})
            base_metadata = {
                "year": metadata.get('year', self.year),
                "month": str(metadata.get('month', '')),
                "day": str(metadata.get('day', '')),
                "speaker": str(metadata.get('speaker', '')),
                "party": str(metadata.get('party', '')),
                "text_id": str(metadata.get('id', f"{self.year}_{i}")),
                "record_index": i
            }
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": j,
                    "total_chunks": len(chunks),
                    "chunk_id": f"{self.year}_{i}_{j}"
                })
                
                all_chunks.append({
                    "id": f"{self.year}_{i}_{j}",
                    "text": chunk,
                    "metadata": chunk_metadata
                })
        
        end_time = time.time()
        
        self._record_metric(
            "æ–‡æœ¬åˆ†å—",
            start_time,
            end_time,
            records_processed=valid_records,
            chunks_generated=len(all_chunks)
        )
        
        logger.info(f"ğŸ“Š åˆ†å—ç»“æœ: {valid_records:,}æ¡æœ‰æ•ˆè®°å½• â†’ {len(all_chunks):,}ä¸ªchunks")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ¡è®°å½•: {len(all_chunks)/valid_records:.1f}ä¸ªchunks")
        
        return all_chunks
    
    def stage_3_openai_embedding(self, chunks: List[Dict]) -> List[List[float]]:
        """é˜¶æ®µ3: OpenAI embeddingç”Ÿæˆ"""
        logger.info(f"ğŸ§  é˜¶æ®µ3: OpenAI {self.embedding_model} embeddingç”Ÿæˆ")
        
        start_time = time.time()
        
        texts = [chunk["text"] for chunk in chunks]
        
        # ä½¿ç”¨OpenAI APIè¿›è¡Œembedding
        embeddings = self._generate_openai_embeddings_batch(
            texts,
            batch_size=1000,  # OpenAIæ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡
            max_workers=10,
            request_delay=1.0  # OpenAIéœ€è¦æ›´ä¿å®ˆçš„å»¶è¿Ÿ
        )
        
        end_time = time.time()
        
        self._record_metric(
            f"OpenAI {self.embedding_model} å‘é‡åŒ–",
            start_time,
            end_time,
            vectors_created=len(embeddings)
        )
        
        logger.info(f"ğŸ“Š å‘é‡åŒ–ç»“æœ: {len(embeddings):,}ä¸ª{self.embedding_dimension}ç»´å‘é‡")
        
        return embeddings
    
    def _generate_openai_embeddings_batch(
        self,
        texts: List[str],
        batch_size: int = 1000,
        max_workers: int = 10,
        request_delay: float = 1.0
    ) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆOpenAI embeddings"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        def embed_single_batch(batch_texts: List[str]) -> List[List[float]]:
            try:
                url = f"{self.openai_base_url}/embeddings"
                
                payload = {
                    "model": self.embedding_model,
                    "input": batch_texts,
                    "dimensions": self.embedding_dimension
                }
                
                headers = {
                    "Authorization": f"Bearer {self.openai_api_key}",
                    "Content-Type": "application/json"
                }
                
                response = requests.post(url, json=payload, headers=headers, timeout=120)
                
                if response.status_code == 200:
                    data = response.json()
                    return [item["embedding"] for item in data["data"]]
                else:
                    logger.error(f"âŒ OpenAI APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
            except Exception as e:
                logger.error(f"âŒ OpenAI embeddingæ‰¹æ¬¡å¼‚å¸¸: {str(e)}")
                return []
        
        all_embeddings = []
        total_batches = len(texts) // batch_size + (1 if len(texts) % batch_size else 0)
        
        logger.info(f"ğŸ”„ å¼€å§‹OpenAIæ‰¹é‡embedding: {len(texts):,}ä¸ªæ–‡æœ¬, {total_batches}ä¸ªæ‰¹æ¬¡")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {}
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                future = executor.submit(embed_single_batch, batch)
                future_to_batch[future] = i // batch_size + 1
                
                # å»¶è¿Ÿæäº¤ä»¥é¿å…APIé™åˆ¶
                if request_delay > 0:
                    time.sleep(request_delay)
            
            # æ”¶é›†ç»“æœ
            completed_batches = 0
            for future in as_completed(future_to_batch):
                batch_num = future_to_batch[future]
                try:
                    batch_embeddings = future.result()
                    if batch_embeddings:
                        all_embeddings.extend(batch_embeddings)
                        completed_batches += 1
                        
                        if completed_batches % 5 == 0:  # æ¯5ä¸ªæ‰¹æ¬¡æŠ¥å‘Šè¿›åº¦
                            progress = (completed_batches / total_batches) * 100
                            logger.info(f"ğŸ“ˆ OpenAI Embeddingè¿›åº¦: {progress:.1f}% ({completed_batches}/{total_batches})")
                    else:
                        logger.warning(f"âš ï¸  æ‰¹æ¬¡{batch_num}è¿”å›ç©ºç»“æœ")
                        
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡{batch_num}å¤„ç†å¤±è´¥: {str(e)}")
        
        logger.info(f"âœ… OpenAI Embeddingå®Œæˆ: {len(all_embeddings):,}/{len(texts):,}ä¸ªå‘é‡")
        return all_embeddings
    
    def stage_4_create_pinecone_index(self):
        """é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼•"""
        logger.info("ğŸ“¦ é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼•")
        
        start_time = time.time()
        
        # Pineconeå®¢æˆ·ç«¯åˆå§‹åŒ–
        try:
            import pinecone
            from pinecone import Pinecone, ServerlessSpec
            
            # åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯
            pc = Pinecone(api_key=self.pinecone_api_key)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
            existing_indexes = pc.list_indexes()
            index_names = [idx.name for idx in existing_indexes]
            
            if self.index_name in index_names:
                logger.info(f"âš ï¸  ç´¢å¼• {self.index_name} å·²å­˜åœ¨ï¼Œåˆ é™¤æ—§ç´¢å¼•...")
                pc.delete_index(self.index_name)
                time.sleep(10)  # ç­‰å¾…åˆ é™¤å®Œæˆ
            
            # åˆ›å»ºæ–°ç´¢å¼•
            logger.info(f"ğŸ”¨ åˆ›å»ºæ–°ç´¢å¼•: {self.index_name}")
            pc.create_index(
                name=self.index_name,
                dimension=self.embedding_dimension,
                metric="cosine",
                spec=ServerlessSpec(
                    cloud="aws",
                    region=self.pinecone_region
                )
            )
            
            # ç­‰å¾…ç´¢å¼•å‡†å¤‡å°±ç»ª
            logger.info("â³ ç­‰å¾…ç´¢å¼•åˆå§‹åŒ–...")
            time.sleep(30)  # Pineconeç´¢å¼•éœ€è¦æ—¶é—´åˆå§‹åŒ–
            
            # è¿æ¥åˆ°ç´¢å¼•
            self.index = pc.Index(self.index_name)
            
            end_time = time.time()
            
            self._record_metric("åˆ›å»ºPineconeç´¢å¼•", start_time, end_time)
            
            logger.info(f"âœ… Pineconeç´¢å¼•åˆ›å»ºæˆåŠŸ: {self.index_name}")
            
        except ImportError:
            logger.error("âŒ è¯·å®‰è£…pinecone-client: pip install pinecone-client")
            raise
        except Exception as e:
            logger.error(f"âŒ Pineconeç´¢å¼•åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def stage_5_pinecone_storage(self, chunks: List[Dict], embeddings: List[List[float]]):
        """é˜¶æ®µ5: å‘é‡å­˜å‚¨åˆ°Pinecone"""
        logger.info("ğŸ’¾ é˜¶æ®µ5: å‘é‡å­˜å‚¨åˆ°Pinecone")
        
        start_time = time.time()
        
        batch_size = 100  # Pineconeæ¨èæ‰¹æ¬¡å¤§å°
        total_batches = len(chunks) // batch_size + (1 if len(chunks) % batch_size else 0)
        
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å­˜å‚¨: {len(chunks):,}ä¸ªå‘é‡, {total_batches}ä¸ªæ‰¹æ¬¡")
        
        stored_count = 0
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            
            # æ„å»ºPineconeå‘é‡æ ¼å¼
            vectors = []
            for chunk, embedding in zip(batch_chunks, batch_embeddings):
                vectors.append({
                    "id": chunk["id"],
                    "values": embedding,
                    "metadata": {
                        "text": chunk["text"][:1000],  # Pinecone metadataæœ‰å¤§å°é™åˆ¶
                        **{k: str(v)[:100] for k, v in chunk["metadata"].items()}  # ç¡®ä¿metadataå­—ç¬¦ä¸²ä¸å¤ªé•¿
                    }
                })
            
            # æ’å…¥åˆ°Pinecone
            try:
                self.index.upsert(vectors=vectors)
                stored_count += len(vectors)
                
                if (i // batch_size + 1) % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡æŠ¥å‘Šè¿›åº¦
                    progress = (stored_count / len(chunks)) * 100
                    logger.info(f"ğŸ“ˆ å­˜å‚¨è¿›åº¦: {progress:.1f}% ({stored_count:,}/{len(chunks):,})")
                
                # Pineconeé™åˆ¶ï¼šé¿å…è¿‡å¿«è¯·æ±‚
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡å­˜å‚¨å¤±è´¥: {str(e)}")
                # ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
        
        end_time = time.time()
        
        self._record_metric(
            "å‘é‡å­˜å‚¨åˆ°Pinecone",
            start_time,
            end_time,
            vectors_created=stored_count
        )
        
        logger.info(f"âœ… å­˜å‚¨å®Œæˆ: {stored_count:,}ä¸ªå‘é‡æˆåŠŸå­˜å‚¨åˆ°Pinecone")
        
        # ç­‰å¾…ç´¢å¼•å®Œæˆ
        logger.info("â³ ç­‰å¾…Pineconeç´¢å¼•å®Œæˆ...")
        time.sleep(10)
    
    def stage_6_verification(self):
        """é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ"""
        logger.info("ğŸ” é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ")
        
        start_time = time.time()
        
        try:
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.index.describe_index_stats()
            
            total_vectors = stats['total_vector_count']
            dimension = stats['dimension']
            
            logger.info(f"ğŸ“Š Pineconeç´¢å¼•éªŒè¯ç»“æœ:")
            logger.info(f"   æ€»å‘é‡æ•°: {total_vectors:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {dimension}")
            logger.info(f"   ç´¢å¼•çŠ¶æ€: å°±ç»ª")
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            if total_vectors > 0:
                logger.info("ğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
                # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å‘é‡ï¼ˆå…¨1å‘é‡ï¼‰
                test_vector = [0.001] * self.embedding_dimension
                
                search_results = self.index.query(
                    vector=test_vector,
                    top_k=3,
                    include_metadata=True
                )
                
                logger.info(f"âœ… æœç´¢æµ‹è¯•æˆåŠŸï¼Œè¿”å› {len(search_results.matches)} ä¸ªç»“æœ")
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        end_time = time.time()
        
        self._record_metric("éªŒè¯å­˜å‚¨ç»“æœ", start_time, end_time)
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Pinecone + OpenAIå®Œæ•´æµç¨‹æ€§èƒ½æµ‹è¯•")
        logger.info("=" * 80)
        logger.info(f"æµ‹è¯•å¹´ä»½: {self.year}")
        logger.info(f"å‘é‡æ•°æ®åº“: Pinecone")
        logger.info(f"Embeddingæ¨¡å‹: OpenAI {self.embedding_model}")
        logger.info("=" * 80)
        
        try:
            # é˜¶æ®µ1: æ•°æ®åŠ è½½
            transcript = self.stage_1_data_loading()
            
            # é˜¶æ®µ2: æ–‡æœ¬åˆ†å—
            chunks = self.stage_2_text_chunking(transcript)
            
            # é˜¶æ®µ3: OpenAI embedding
            embeddings = self.stage_3_openai_embedding(chunks)
            
            # é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼•
            self.stage_4_create_pinecone_index()
            
            # é˜¶æ®µ5: å‘é‡å­˜å‚¨
            self.stage_5_pinecone_storage(chunks, embeddings)
            
            # é˜¶æ®µ6: éªŒè¯ç»“æœ
            self.stage_6_verification()
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            self.generate_performance_report()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š Pinecone + OpenAI æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 80)
        
        total_time = sum(metric.duration for metric in self.metrics)
        total_records = self.metrics[0].records_processed if self.metrics else 0
        total_chunks = max((metric.chunks_generated for metric in self.metrics), default=0)
        total_vectors = max((metric.vectors_created for metric in self.metrics), default=0)
        
        logger.info(f"ğŸ¯ æ€»ä½“æ€§èƒ½:")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"   å¤„ç†è®°å½•: {total_records:,}æ¡")
        logger.info(f"   ç”Ÿæˆchunks: {total_chunks:,}ä¸ª")
        logger.info(f"   åˆ›å»ºå‘é‡: {total_vectors:,}ä¸ª")
        logger.info(f"   æ•´ä½“é€Ÿåº¦: {total_records/total_time:.1f} è®°å½•/ç§’")
        logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {total_vectors/total_time:.1f} å‘é‡/ç§’")
        
        logger.info(f"\nğŸ“‹ å„é˜¶æ®µè¯¦ç»†æ—¶é—´:")
        for metric in self.metrics:
            percentage = (metric.duration / total_time) * 100
            logger.info(f"   {metric.stage_name}: {metric.duration:.2f}ç§’ ({percentage:.1f}%)")
        
        # è®¡ç®—æˆæœ¬ä¼°ç®—
        # OpenAI text-embedding-3-large å®šä»·: $0.13/1M tokens
        # å‡è®¾æ¯ä¸ªchunkå¹³å‡100 tokens
        estimated_tokens = total_chunks * 100
        embedding_cost = (estimated_tokens / 1_000_000) * 0.13
        
        logger.info(f"\nğŸ’° æˆæœ¬ä¼°ç®— ({self.year}å¹´):")
        logger.info(f"   é¢„ä¼°tokens: {estimated_tokens:,}")
        logger.info(f"   OpenAI embeddingæˆæœ¬: ${embedding_cost:.3f}")
        
        # å…¨é‡æ•°æ®é¢„ä¼°
        if total_records > 0:
            total_data_records = 835689  # æ‰€æœ‰å¹´ä»½æ€»è®°å½•æ•°
            scale_factor = total_data_records / total_records
            
            estimated_total_time = total_time * scale_factor
            estimated_total_cost = embedding_cost * scale_factor
            
            logger.info(f"\nğŸ”® å…¨é‡æ•°æ®é¢„ä¼°:")
            logger.info(f"   é¢„ä¼°æ€»æ—¶é—´: {estimated_total_time/60:.1f}åˆ†é’Ÿ ({estimated_total_time/3600:.2f}å°æ—¶)")
            logger.info(f"   é¢„ä¼°æ€»æˆæœ¬: ${estimated_total_cost:.2f}")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"pinecone_performance_report_{self.year}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# Pinecone + OpenAI {self.year}å¹´æ•°æ®æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"## é…ç½®ä¿¡æ¯\n")
            f.write(f"- **å‘é‡æ•°æ®åº“**: Pinecone\n")
            f.write(f"- **Embeddingæ¨¡å‹**: OpenAI {self.embedding_model}\n")
            f.write(f"- **å‘é‡ç»´åº¦**: {self.embedding_dimension}\n")
            f.write(f"- **æµ‹è¯•å¹´ä»½**: {self.year}\n\n")
            
            f.write(f"## æ€»ä½“æ€§èƒ½\n")
            f.write(f"- **æ€»è€—æ—¶**: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)\n")
            f.write(f"- **å¤„ç†è®°å½•**: {total_records:,}æ¡\n")
            f.write(f"- **ç”Ÿæˆchunks**: {total_chunks:,}ä¸ª\n")
            f.write(f"- **åˆ›å»ºå‘é‡**: {total_vectors:,}ä¸ª\n")
            f.write(f"- **æ•´ä½“é€Ÿåº¦**: {total_records/total_time:.1f} è®°å½•/ç§’\n")
            f.write(f"- **å‘é‡åŒ–é€Ÿåº¦**: {total_vectors/total_time:.1f} å‘é‡/ç§’\n\n")
            
            f.write(f"## å„é˜¶æ®µè¯¦ç»†æ—¶é—´\n\n")
            for metric in self.metrics:
                percentage = (metric.duration / total_time) * 100
                f.write(f"### {metric.stage_name}\n")
                f.write(f"- **è€—æ—¶**: {metric.duration:.2f}ç§’ ({percentage:.1f}%)\n")
                if metric.records_processed > 0:
                    f.write(f"- **å¤„ç†é€Ÿåº¦**: {metric.records_per_second:.1f} è®°å½•/ç§’\n")
                if metric.chunks_generated > 0:
                    f.write(f"- **åˆ†å—é€Ÿåº¦**: {metric.chunks_per_second:.1f} chunks/ç§’\n")
                if metric.vectors_created > 0:
                    f.write(f"- **å‘é‡åŒ–é€Ÿåº¦**: {metric.vectors_created/metric.duration:.1f} å‘é‡/ç§’\n")
                f.write("\n")
            
            f.write(f"## æˆæœ¬åˆ†æ\n")
            f.write(f"- **é¢„ä¼°tokens**: {estimated_tokens:,}\n")
            f.write(f"- **å•å¹´embeddingæˆæœ¬**: ${embedding_cost:.3f}\n")
            if total_records > 0:
                f.write(f"- **å…¨é‡æ•°æ®é¢„ä¼°æˆæœ¬**: ${estimated_total_cost:.2f}\n")
            f.write("\n")
        
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pinecone + OpenAIæ€§èƒ½æµ‹è¯•")
    parser.add_argument("--year", type=int, default=2015, help="æµ‹è¯•å¹´ä»½")
    
    args = parser.parse_args()
    
    test = PineconePerformanceTest(year=args.year)
    success = test.run_complete_test()
    
    if success:
        logger.info("\nğŸ‰ Pineconeæ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        return 0
    else:
        logger.error("\nâŒ Pineconeæ€§èƒ½æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())

