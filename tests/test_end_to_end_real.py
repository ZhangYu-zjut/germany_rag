"""
ç«¯åˆ°ç«¯çœŸå®ç¯å¢ƒæµ‹è¯•
ä½¿ç”¨2018-2020å¹´çš„çœŸå®æ•°æ®ï¼ŒéªŒè¯å®Œæ•´æµç¨‹ï¼š
1. æ•°æ®åŠ è½½å’Œç´¢å¼•æ„å»º
2. å®Œæ•´å·¥ä½œæµè¿è¡Œ
3. è¾“å‡ºè´¨é‡éªŒè¯

æ–°å¢åŠŸèƒ½ï¼š
- ä¸­é—´æ–­ç‚¹æ£€æµ‹å’Œç¼“å­˜æœºåˆ¶
- æ™ºèƒ½è·³è¿‡å·²å®Œæˆçš„æ­¥éª¤
- æ”¯æŒä»ä»»æ„æ­¥éª¤å¼€å§‹è¿è¡Œ
"""

import sys
import os
import pickle
import hashlib
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.logger import logger
from src.data_loader import ParliamentDataLoader, ParliamentTextSplitter, MetadataMapper
from src.llm import GeminiEmbeddingClient
from src.vectordb import MilvusClient, MilvusCollectionManager
from src.graph.workflow import QuestionAnswerWorkflow
from src.graph.state import create_initial_state


# ========== ç¼“å­˜ç®¡ç† ==========

def get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•"""
    cache_dir = Path("cache/e2e_test")
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def get_data_hash(years: list, chunk_size: int, chunk_overlap: int):
    """ç”Ÿæˆæ•°æ®é…ç½®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜é”®"""
    config_str = f"years:{','.join(sorted(years))}_chunk:{chunk_size}_overlap:{chunk_overlap}"
    return hashlib.md5(config_str.encode()).hexdigest()[:12]


def save_to_cache(data, cache_key: str, step_name: str):
    """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
    cache_dir = get_cache_dir()
    cache_file = cache_dir / f"{step_name}_{cache_key}.pkl"
    
    print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {cache_file}")
    with open(cache_file, 'wb') as f:
        pickle.dump({
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'cache_key': cache_key
        }, f)
    return cache_file


def load_from_cache(cache_key: str, step_name: str):
    """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
    cache_dir = get_cache_dir()
    cache_file = cache_dir / f"{step_name}_{cache_key}.pkl"
    
    if not cache_file.exists():
        return None
    
    try:
        with open(cache_file, 'rb') as f:
            cached = pickle.load(f)
        
        print(f"ğŸ“‚ åŠ è½½ç¼“å­˜: {cache_file}")
        print(f"   ç¼“å­˜æ—¶é—´: {cached['timestamp']}")
        return cached['data']
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
        return None


def check_milvus_collection_status():
    """æ£€æŸ¥Milvus collectionçŠ¶æ€"""
    try:
        collection_name = os.getenv("MILVUS_COLLECTION_NAME", "parliament_speeches")
        
        with MilvusClient() as client:
            from pymilvus import utility
            
            if not utility.has_collection(collection_name):
                return {"exists": False, "count": 0}
            
            # è·å–collection
            from pymilvus import Collection
            collection = Collection(collection_name)
            count = collection.num_entities
            
            return {
                "exists": True, 
                "count": count,
                "collection_name": collection_name
            }
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥MilvusçŠ¶æ€å¤±è´¥: {e}")
        return {"exists": False, "count": 0, "error": str(e)}


def build_index_for_years(years: list, force_rebuild: bool = False):
    """
    ä¸ºæŒ‡å®šå¹´ä»½æ„å»ºMilvusç´¢å¼•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    
    Args:
        years: å¹´ä»½åˆ—è¡¨ï¼Œå¦‚ ['2018', '2019', '2020']
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    
    Returns:
        (speeches_count, chunks_count, vectors_count)
    """
    print("\n" + "="*80)
    print("ğŸ“š æ­¥éª¤1: æ„å»ºç´¢å¼• - æ™ºèƒ½æ–­ç‚¹æ£€æµ‹æ¨¡å¼")
    print("="*80)
    
    # è·å–é…ç½®å‚æ•°
    chunk_size = int(os.getenv("CHUNK_SIZE", "1000"))
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "200"))
    cache_key = get_data_hash(years, chunk_size, chunk_overlap)
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - å¤„ç†å¹´ä»½: {years}")
    print(f"   - æ–‡æœ¬å—å¤§å°: {chunk_size}")
    print(f"   - é‡å é•¿åº¦: {chunk_overlap}")
    print(f"   - ç¼“å­˜é”®: {cache_key}")
    print(f"   - å¼ºåˆ¶é‡å»º: {'æ˜¯' if force_rebuild else 'å¦'}")
    
    # ========== æ£€æŸ¥MilvusçŠ¶æ€ ==========
    print(f"\nğŸ” æ£€æŸ¥ç°æœ‰ç´¢å¼•çŠ¶æ€...")
    milvus_status = check_milvus_collection_status()
    
    if milvus_status["exists"] and milvus_status["count"] > 0 and not force_rebuild:
        print(f"âœ… å‘ç°ç°æœ‰ç´¢å¼•:")
        print(f"   - Collection: {milvus_status.get('collection_name')}")
        print(f"   - å‘é‡æ•°é‡: {milvus_status['count']}")
        
        try:
            use_existing = input("\nğŸ¤” æ˜¯å¦ä½¿ç”¨ç°æœ‰ç´¢å¼•ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower()
        except EOFError:
            # éäº¤äº’æ¨¡å¼ä¸‹é»˜è®¤ä½¿ç”¨ç°æœ‰ç´¢å¼•
            use_existing = 'y'
            print("y (éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©)")
        
        if not use_existing or use_existing == 'y':
            print("âœ… ä½¿ç”¨ç°æœ‰ç´¢å¼•ï¼Œè·³è¿‡æ„å»ºæ­¥éª¤")
            # éœ€è¦è·å–åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯æ¥è¿”å›æ­£ç¡®çš„è®¡æ•°
            try:
                # å¿«é€ŸåŠ è½½æ•°æ®ä»¥è·å–ç»Ÿè®¡ä¿¡æ¯
                data_dir = Path("data/pp_json_49-21")
                loader = ParliamentDataLoader(
                    data_dir=str(data_dir),
                    data_mode="PART",
                    years=years
                )
                speeches = loader.load_data()
                splitter = ParliamentTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                chunks = splitter.split_speeches(speeches)
                
                return len(speeches), len(chunks), milvus_status["count"]
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–æ•°æ®ç»Ÿè®¡: {e}")
                return 0, 0, milvus_status["count"]
    
    # ========== æ­¥éª¤1.1-1.3: æ•°æ®å‡†å¤‡ ==========
    enriched_chunks = None
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å‡†å¤‡ç¼“å­˜
    if not force_rebuild:
        enriched_chunks = load_from_cache(cache_key, "data_prepared")
    
    if enriched_chunks is None:
        print("\n[1.1] åŠ è½½æ•°æ®...")
        data_dir = Path("data/pp_json_49-21")
        
        loader = ParliamentDataLoader(
            data_dir=str(data_dir),
            data_mode="PART",
            years=years
        )
        
        speeches = loader.load_data()
        stats = loader.get_statistics(speeches)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   - æ¼”è®²è®°å½•: {len(speeches)} æ¡")
        print(f"   - å¹´ä»½åˆ†å¸ƒ: {list(stats['years'].keys())}")
        print(f"   - å‘è¨€äººæ•°: {stats['speakers_count']}")
        
        if len(speeches) == 0:
            raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        
        print("\n[1.2] æ–‡æœ¬åˆ†å—...")
        splitter = ParliamentTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = splitter.split_speeches(speeches)
        print(f"âœ… åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        print("\n[1.3] å…ƒæ•°æ®ä¸°å¯Œ...")
        mapper = MetadataMapper()
        enriched_chunks = mapper.enrich_chunks(chunks)
        print(f"âœ… å…ƒæ•°æ®ä¸°å¯Œå®Œæˆ")
        
        # ä¿å­˜æ•°æ®å‡†å¤‡ç»“æœåˆ°ç¼“å­˜
        save_to_cache(enriched_chunks, cache_key, "data_prepared")
        save_to_cache({
            'speeches_count': len(speeches),
            'chunks_count': len(chunks)
        }, cache_key, "data_stats")
    else:
        print("ğŸ“‚ ä»ç¼“å­˜åŠ è½½æ•°æ®å‡†å¤‡ç»“æœ...")
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(enriched_chunks)} ä¸ªenriched chunks")
    
    # ========== æ­¥éª¤1.4: ç”ŸæˆEmbedding ==========
    embedded_chunks = None
    
    # æ£€æŸ¥æ˜¯å¦æœ‰Embeddingç¼“å­˜
    if not force_rebuild:
        embedded_chunks = load_from_cache(cache_key, "embeddings")
    
    if embedded_chunks is None:
        print("\n[1.4] ç”ŸæˆEmbedding...")
        embedding_client = GeminiEmbeddingClient()
        
        # æ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
        embedding_mode = os.getenv("EMBEDDING_MODE", "deepinfra")
        if embedding_mode == "local":
            print("â³ å¼€å§‹æ‰¹é‡Embeddingï¼ˆæœ¬åœ°BGE-M3æ¨¡å¼ï¼šGPUåŠ é€Ÿï¼‰...")
            print(f"   æ€»æ•°æ®é‡: {len(enriched_chunks)} æ¡")
            print(f"   æ‰¹å¤„ç†å¤§å°: 64æ¡/æ‰¹ï¼ˆGPUä¼˜åŒ–ï¼‰")
            print(f"   æ‰§è¡Œæ–¹å¼: æœ¬åœ°GPUåŠ é€Ÿï¼ˆæ— éœ€å¹¶å‘å’Œå»¶è¿Ÿï¼‰")
            batch_size = 64  # GPU å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
            max_workers = 1  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å¹¶å‘
            request_delay = 0.0  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å»¶è¿Ÿ
        else:
            print("â³ å¼€å§‹æ‰¹é‡Embeddingï¼ˆäº‘æœåŠ¡APIæ¨¡å¼ï¼šé«˜å¹¶å‘+å¤§æ‰¹æ¬¡ï¼‰...")
            print(f"   æ€»æ•°æ®é‡: {len(enriched_chunks)} æ¡")
            print(f"   æ‰¹å¤„ç†å¤§å°: 100æ¡/æ‰¹ï¼ˆå¹³è¡¡æ¨¡å¼ï¼‰")
            print(f"   æ‰§è¡Œæ–¹å¼: é«˜å¹¶å‘æ‰§è¡Œ (å¹¶å‘æ•°: 10)")
            print(f"   æ‰¹æ¬¡é—´å»¶è¿Ÿ: 1.0ç§’ï¼ˆé¿å…è§¦å‘é€Ÿç‡é™åˆ¶ï¼‰")
            batch_size = 100
            max_workers = 10
            request_delay = 1.0
        
        embedded_chunks = embedding_client.embed_chunks(
            enriched_chunks,
            batch_size=batch_size,
            max_workers=max_workers,
            request_delay=request_delay
        )
    
    actual_dim = len(embedded_chunks[0]['vector'])
    print(f"âœ… Embeddingå®Œæˆ:")
    print(f"   - å‘é‡æ•°é‡: {len(embedded_chunks)}")
    print(f"   - å‘é‡ç»´åº¦: {actual_dim}")
    
        # ä¿å­˜Embeddingç»“æœåˆ°ç¼“å­˜
        save_to_cache(embedded_chunks, cache_key, "embeddings")
    else:
        print("ğŸ“‚ ä»ç¼“å­˜åŠ è½½Embeddingç»“æœ...")
        actual_dim = len(embedded_chunks[0]['vector'])
        print(f"âœ… EmbeddingåŠ è½½å®Œæˆ:")
        print(f"   - å‘é‡æ•°é‡: {len(embedded_chunks)}")
        print(f"   - å‘é‡ç»´åº¦: {actual_dim}")
    
    # ========== æ­¥éª¤1.5: å­˜å‚¨åˆ°Milvus ==========
    print("\n[1.5] å­˜å‚¨åˆ°Milvus...")
    
    try:
        with MilvusClient() as client:
            # æ£€æŸ¥Collectionæ˜¯å¦å·²å­˜åœ¨
            collection_name = os.getenv("MILVUS_COLLECTION_NAME", "parliament_speeches")
            from pymilvus import utility
            
            if utility.has_collection(collection_name):
                if force_rebuild:
                    print(f"ğŸ”„ å¼ºåˆ¶é‡å»ºæ¨¡å¼ï¼šåˆ é™¤ç°æœ‰Collection '{collection_name}'...")
                    utility.drop_collection(collection_name)
                else:
                    print(f"âš ï¸  Collection '{collection_name}' å·²å­˜åœ¨")
                    try:
                        recreate = input("æ˜¯å¦é‡æ–°åˆ›å»ºï¼Ÿ(y/nï¼Œé»˜è®¤n): ").strip().lower()
                    except EOFError:
                        # éäº¤äº’æ¨¡å¼ä¸‹é»˜è®¤ä¸é‡æ–°åˆ›å»º
                        recreate = 'n'
                        print("n (éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©)")
                    
                    if recreate == 'y':
                utility.drop_collection(collection_name)
                    else:
                        print("â­ï¸  è·³è¿‡Milvusåˆ›å»ºï¼Œä½¿ç”¨ç°æœ‰Collection")
                        
                        # è·å–ç°æœ‰æ•°æ®ç»Ÿè®¡
                        stats_cache = load_from_cache(cache_key, "data_stats")
                        if stats_cache:
                            speeches_count = stats_cache['speeches_count']
                            chunks_count = stats_cache['chunks_count']
                        else:
                            speeches_count = 0
                            chunks_count = len(enriched_chunks)
                        
                        from pymilvus import Collection
                        collection = Collection(collection_name)
                        return speeches_count, chunks_count, collection.num_entities
            
            # åˆ›å»ºCollection Manager
            manager = MilvusCollectionManager()
            
            # åˆ›å»ºCollectionï¼ˆä¼ å…¥å®é™…ç»´åº¦ï¼‰
            manager.create_collection(dimension=actual_dim)
            
            # æ’å…¥æ•°æ®ï¼ˆåˆ†æ‰¹æ’å…¥ï¼Œé¿å…gRPCæ¶ˆæ¯å¤§å°é™åˆ¶ï¼‰
            print("â³ æ’å…¥æ•°æ®åˆ°Milvusï¼ˆåˆ†æ‰¹æ’å…¥ï¼‰...")
            print(f"   æ€»æ•°æ®é‡: {len(embedded_chunks)} æ¡")
            print(f"   åˆ†æ‰¹å¤§å°: 5000æ¡/æ‰¹ï¼ˆé¿å…gRPCæ¶ˆæ¯å¤§å°é™åˆ¶ï¼‰")
            manager.insert_data(embedded_chunks, batch_size=5000)
            
            # åˆ›å»ºç´¢å¼•
            print("â³ åˆ›å»ºç´¢å¼•...")
            manager.create_index()
            
            # åŠ è½½åˆ°å†…å­˜
            print("â³ åŠ è½½Collectionåˆ°å†…å­˜...")
            manager.collection.load()
            
            # éªŒè¯
            count = manager.collection.num_entities
            print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ:")
            print(f"   - Collectionåç§°: {collection_name}")
            print(f"   - å‘é‡æ•°é‡: {count}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats_cache = load_from_cache(cache_key, "data_stats")
            if stats_cache:
                speeches_count = stats_cache['speeches_count']
                chunks_count = stats_cache['chunks_count']
            else:
                speeches_count = 0
                chunks_count = len(enriched_chunks)
            
            return speeches_count, chunks_count, count
            
    except Exception as e:
        logger.error(f"âŒ Milvusæ“ä½œå¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("  1. MilvusæœåŠ¡æ˜¯å¦å·²å¯åŠ¨")
        print("  2. Milvusè¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®ï¼ˆ.envæ–‡ä»¶ï¼‰")
        raise


def test_workflow(questions: list):
    """
    æµ‹è¯•å®Œæ•´å·¥ä½œæµ
    
    Args:
        questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
    """
    print("\n" + "="*80)
    print("ğŸš€ æ­¥éª¤2: è¿è¡Œå®Œæ•´å·¥ä½œæµæµ‹è¯•")
    print("="*80)
    
    # åˆå§‹åŒ–å·¥ä½œæµ
    print("\n[2.1] åˆå§‹åŒ–å·¥ä½œæµ...")
    try:
        workflow = QuestionAnswerWorkflow()
        print("âœ… å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # æµ‹è¯•æ¯ä¸ªé—®é¢˜
    results = []
    
    for i, question in enumerate(questions, 1):
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•é—®é¢˜ {i}/{len(questions)}")
        print(f"{'='*80}")
        print(f"\né—®é¢˜: {question}")
        print(f"\nå¼€å§‹å¤„ç†...")
        
        try:
            # è¿è¡Œå·¥ä½œæµï¼ˆç›´æ¥ä¼ é€’é—®é¢˜å­—ç¬¦ä¸²ï¼‰
            final_state = workflow.run(question)
            
            # æ£€æŸ¥ç»“æœ
            if final_state.get("error"):
                print(f"\nâŒ å¤„ç†å¤±è´¥:")
                print(f"   é”™è¯¯ç±»å‹: {final_state.get('error_type', 'UNKNOWN')}")
                print(f"   é”™è¯¯ä¿¡æ¯: {final_state.get('error', 'N/A')}")
                results.append({
                    "question": question,
                    "success": False,
                    "error": final_state.get("error"),
                    "error_type": final_state.get("error_type")
                })
            else:
                final_answer = final_state.get("final_answer", "")
                intent = final_state.get("intent", "unknown")
                question_type = final_state.get("question_type", "unknown")
                
                print(f"\nâœ… å¤„ç†æˆåŠŸ:")
                print(f"   æ„å›¾: {intent}")
                print(f"   é—®é¢˜ç±»å‹: {question_type}")
                print(f"   ç­”æ¡ˆé•¿åº¦: {len(final_answer)} å­—ç¬¦")
                print(f"\nç­”æ¡ˆé¢„è§ˆ:")
                print("-" * 80)
                # æ˜¾ç¤ºå‰500å­—ç¬¦
                preview = final_answer[:500] + "..." if len(final_answer) > 500 else final_answer
                print(preview)
                print("-" * 80)
                
                results.append({
                    "question": question,
                    "success": True,
                    "intent": intent,
                    "question_type": question_type,
                    "answer_length": len(final_answer),
                    "answer_preview": preview
                })
        
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•é—®é¢˜å¤±è´¥: {e}", exc_info=True)
            results.append({
                "question": question,
                "success": False,
                "error": str(e)
            })
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*80)
    
    passed = sum(1 for r in results if r.get("success", False))
    total = len(results)
    
    for i, result in enumerate(results, 1):
        status = "âœ… PASS" if result.get("success") else "âŒ FAIL"
        print(f"\n{status} - é—®é¢˜ {i}: {result['question'][:60]}...")
        
        if result.get("success"):
            print(f"   æ„å›¾: {result.get('intent')}")
            print(f"   ç±»å‹: {result.get('question_type')}")
            print(f"   ç­”æ¡ˆé•¿åº¦: {result.get('answer_length')} å­—ç¬¦")
        else:
            print(f"   é”™è¯¯: {result.get('error', 'N/A')}")
    
    print(f"\næ€»è®¡: {passed}/{total} é€šè¿‡ ({passed/total*100:.1f}%)")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ§ª ç«¯åˆ°ç«¯çœŸå®ç¯å¢ƒæµ‹è¯• - æ™ºèƒ½æ–­ç‚¹æ¨¡å¼")
    print("="*80)
    print("\nã€æµ‹è¯•ç›®æ ‡ã€‘")
    print("1. åŠ è½½2018-2020å¹´çš„çœŸå®æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰")
    print("2. æ„å»ºMilvuså‘é‡ç´¢å¼•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
    print("3. è¿è¡Œå®Œæ•´å·¥ä½œæµ")
    print("4. éªŒè¯è¾“å‡ºè´¨é‡")
    
    print("\nã€ç¯å¢ƒå˜é‡æ§åˆ¶ã€‘")
    print("- FORCE_REBUILD=true    # å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ­¥éª¤")
    print("- SKIP_INDEX_BUILD=true # è·³è¿‡ç´¢å¼•æ„å»ºï¼Œç›´æ¥æµ‹è¯•å·¥ä½œæµ")
    print("- SKIP_WORKFLOW=true    # åªæ„å»ºç´¢å¼•ï¼Œè·³è¿‡å·¥ä½œæµæµ‹è¯•")
    
    # è·å–æ§åˆ¶é€‰é¡¹
    force_rebuild = os.getenv("FORCE_REBUILD", "").lower() == "true"
    skip_index_build = os.getenv("SKIP_INDEX_BUILD", "").lower() == "true"
    skip_workflow = os.getenv("SKIP_WORKFLOW", "").lower() == "true"
    
    if force_rebuild:
        print("ğŸ”„ å¼ºåˆ¶é‡å»ºæ¨¡å¼ï¼šå°†é‡æ–°æ„å»ºæ‰€æœ‰æ­¥éª¤")
    if skip_index_build:
        print("â­ï¸  è·³è¿‡ç´¢å¼•æ„å»ºæ¨¡å¼ï¼šç›´æ¥è¿è¡Œå·¥ä½œæµæµ‹è¯•")
    if skip_workflow:
        print("ğŸ—ï¸  åªæ„å»ºç´¢å¼•æ¨¡å¼ï¼šè·³è¿‡å·¥ä½œæµæµ‹è¯•")
    
    print("\nã€å‰ç½®æ¡ä»¶æ£€æŸ¥ã€‘")
    print("æ£€æŸ¥Milvusè¿æ¥...")
    
    # ç¡®ä¿ä½¿ç”¨Milvus Liteæ¨¡å¼
    if os.getenv("MILVUS_MODE") != "lite":
        print("âš ï¸  æ£€æµ‹åˆ°MILVUS_MODEä¸æ˜¯liteï¼Œè®¾ç½®ä¸ºliteæ¨¡å¼...")
        os.environ["MILVUS_MODE"] = "lite"
        print("âœ… å·²è®¾ç½®MILVUS_MODE=lite")
    
    try:
        with MilvusClient() as client:
            print("âœ… Milvus Liteè¿æ¥æˆåŠŸ")
            print(f"   æ¨¡å¼: {client.mode}")
            print(f"   æ•°æ®è·¯å¾„: {os.getenv('MILVUS_LITE_PATH', './milvus_data/milvus_lite.db')}")
    except Exception as e:
        print(f"âŒ Milvusè¿æ¥å¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("  1. å·²å®‰è£…milvus-lite: pip install milvus-lite")
        print("  2. .envæ–‡ä»¶ä¸­MILVUS_MODEè®¾ç½®ä¸ºliteï¼ˆæˆ–ä¼šåœ¨è¿è¡Œæ—¶è‡ªåŠ¨è®¾ç½®ï¼‰")
        return 1
    
    print("æ£€æŸ¥ç¬¬ä¸‰æ–¹LLM APIé…ç½®...")
    llm_available = False
    try:
        from src.llm.client import GeminiLLMClient
        llm = GeminiLLMClient()
        test_response = llm.invoke("æµ‹è¯•")
        print("âœ… ç¬¬ä¸‰æ–¹LLM APIè¿æ¥æˆåŠŸ")
        llm_available = True
    except Exception as e:
        print(f"âš ï¸  ç¬¬ä¸‰æ–¹LLM APIè¿æ¥å¤±è´¥: {e}")
        print("\næç¤º:")
        print("  1. å¦‚æœåªæµ‹è¯• embedding æ„å»ºç´¢å¼•ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
        print("  2. å¦‚æœè¦æµ‹è¯•å®Œæ•´å·¥ä½œæµï¼Œéœ€è¦é…ç½®:")
        print("     - .envæ–‡ä»¶ä¸­é…ç½®OPENAI_API_KEYï¼ˆä½œä¸ºç¬¬ä¸‰æ–¹ä»£ç†å¯†é’¥ï¼‰")
        print("     - .envæ–‡ä»¶ä¸­é…ç½®THIRD_PARTY_BASE_URL")
        print("     - ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("\nç»§ç»­è¿è¡Œ embedding æµ‹è¯•...")
        
        # å¦‚æœç”¨æˆ·åªæƒ³æµ‹è¯• embeddingï¼Œå¯ä»¥ç»§ç»­
        try:
            user_input = input("\næ˜¯å¦ç»§ç»­åªæµ‹è¯• embeddingï¼Ÿï¼ˆy/nï¼Œé»˜è®¤yï¼‰: ").strip().lower()
        except EOFError:
            # éäº¤äº’æ¨¡å¼ä¸‹é»˜è®¤ç»§ç»­æµ‹è¯•
            user_input = 'y'
            print("y (éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©)")
            
        if user_input and user_input != 'y':
            print("å·²å–æ¶ˆæµ‹è¯•")
        return 1
    
    # ========== æ­¥éª¤1: æ„å»ºç´¢å¼• ==========
    speeches_count = chunks_count = vectors_count = 0
    
    if not skip_index_build:
    try:
        # è®¾ç½®è¦å¤„ç†çš„å¹´ä»½èŒƒå›´ï¼ˆå¯ä»¥ä¿®æ”¹è¿™é‡Œæ¥æ”¹å˜å¤„ç†çš„æ•°æ®èŒƒå›´ï¼‰
        # æ ¼å¼ï¼šå¹´ä»½åˆ—è¡¨ï¼Œä¾‹å¦‚ ['2018', '2019', '2020'] æˆ– ['2021'] æˆ– ['2015', '2016', '2017', '2018', '2019', '2020']
        # years = ['2018', '2019', '2020']  # ğŸ‘ˆ åœ¨è¿™é‡Œä¿®æ”¹å¹´ä»½èŒƒå›´
        years = ['2018'] # å…ˆç”¨2018å¹´çš„æ•°æ®å¤„ç†ï¼Œæµ‹è¯•æ•´ä¸ªæµç¨‹ï¼Œåç»­å¯ä»¥æ‰©å±•ã€‚
            
            speeches_count, chunks_count, vectors_count = build_index_for_years(years, force_rebuild=force_rebuild)
        
        print("\n" + "="*80)
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ¼”è®²è®°å½•: {speeches_count} æ¡")
        print(f"   - æ–‡æœ¬å—: {chunks_count} ä¸ª")
        print(f"   - å‘é‡æ•°é‡: {vectors_count} ä¸ª")
        
    except Exception as e:
        error_msg = str(e)
        # é¿å…æ ¼å¼åŒ–é”™è¯¯ï¼ˆå¦‚æœé”™è¯¯æ¶ˆæ¯åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰
        logger.error(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {error_msg}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return 1
    else:
        print("\n" + "="*80)
        print("â­ï¸  è·³è¿‡ç´¢å¼•æ„å»ºæ­¥éª¤")
        print("="*80)
        
        # æ£€æŸ¥ç°æœ‰ç´¢å¼•çŠ¶æ€
        milvus_status = check_milvus_collection_status()
        if milvus_status["exists"] and milvus_status["count"] > 0:
            vectors_count = milvus_status["count"]
            print(f"âœ… å‘ç°ç°æœ‰ç´¢å¼•: {vectors_count} ä¸ªå‘é‡")
        else:
            print("âš ï¸  æœªå‘ç°ç°æœ‰ç´¢å¼•ï¼Œå·¥ä½œæµæµ‹è¯•å¯èƒ½å¤±è´¥")
            
        if skip_workflow:
            print("âœ… åªæ‰§è¡Œç´¢å¼•æ£€æŸ¥ï¼Œæµ‹è¯•å®Œæˆ")
            return 0
    
    # ========== æ­¥éª¤2: æµ‹è¯•å·¥ä½œæµ ==========
    if skip_workflow:
        print("\n" + "="*80)
        print("â­ï¸  è·³è¿‡å·¥ä½œæµæµ‹è¯•ï¼ˆç”¨æˆ·è®¾ç½®ï¼‰")
        print("="*80)
        print("\nâœ… ç´¢å¼•æ„å»ºæµ‹è¯•å®Œæˆï¼")
        return 0
    
    # åªæœ‰åœ¨ LLM å¯ç”¨æ—¶æ‰æµ‹è¯•å·¥ä½œæµ
    if not llm_available:
        print("\n" + "="*80)
        print("âš ï¸  è·³è¿‡å·¥ä½œæµæµ‹è¯•ï¼ˆLLM API æœªé…ç½®ï¼‰")
        print("="*80)
        print("\nâœ… Embedding ç´¢å¼•æ„å»ºæµ‹è¯•å®Œæˆï¼")
        print("æç¤ºï¼šé…ç½® OPENAI_API_KEY åå¯ä»¥è¿è¡Œå®Œæ•´å·¥ä½œæµæµ‹è¯•")
        return 0
    
    # å®šä¹‰æµ‹è¯•é—®é¢˜
    test_questions = [
        # ç®€å•é—®é¢˜
        "2019å¹´å¾·å›½è®®ä¼šè®¨è®ºäº†å“ªäº›ä¸»è¦è®®é¢˜ï¼Ÿ",
        "2020å¹´ç»¿å…šåœ¨æ°”å€™ä¿æŠ¤æ–¹é¢çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        
        # å¤æ‚é—®é¢˜ - å˜åŒ–ç±»
        "åœ¨2018-2020å¹´æœŸé—´ï¼ŒCDU/CSUåœ¨éš¾æ°‘æ”¿ç­–ä¸Šçš„ç«‹åœºæœ‰ä½•å˜åŒ–ï¼Ÿ",
        
        # å¤æ‚é—®é¢˜ - å¯¹æ¯”ç±»
        "å¯¹æ¯”CDU/CSUã€SPDå’Œç»¿å…šåœ¨2019å¹´æ•°å­—åŒ–æ”¿ç­–ä¸Šçš„ç«‹åœºå·®å¼‚",
        
        # å¤æ‚é—®é¢˜ - æ€»ç»“ç±»
        "è¯·æ€»ç»“2018-2020å¹´æœŸé—´ï¼Œå¾·å›½è®®ä¼šåœ¨æ°”å€™ä¿æŠ¤æ–¹é¢çš„ä¸»è¦è®¨è®º",
    ]
    
    try:
        results = test_workflow(test_questions)
        
        # æœ€ç»ˆæ€»ç»“
        print("\n" + "="*80)
        print("ğŸ‰ ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆï¼")
        print("="*80)
        
        passed = sum(1 for r in results if r.get("success", False))
        total = len(results)
        
        if passed == total:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
            return 0
        else:
            print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

