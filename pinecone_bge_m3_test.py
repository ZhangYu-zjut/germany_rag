#!/usr/bin/env python3
"""
Pinecone + BGE-M3 Custom Embedding æ€§èƒ½æµ‹è¯•
æœ€ä¼˜æ–¹æ¡ˆï¼šæœ¬åœ°BGE-M3ç”Ÿæˆembedding + Pineconeå­˜å‚¨
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter
from src.llm.embeddings import GeminiEmbeddingClient

logger = setup_logger()

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    stage_name: str
    start_time: float
    end_time: float
    duration: float
    records_processed: int = 0
    chunks_generated: int = 0
    vectors_created: int = 0
    
    @property
    def duration_minutes(self) -> float:
        return self.duration / 60

class PineconeBGEM3Test:
    """Pinecone + BGE-M3 æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, year: int = 2015):
        self.year = year
        self.metrics: List[PerformanceMetrics] = []
        
        # Pineconeé…ç½®
        self.pinecone_api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pinecone_host = os.getenv("PINECONE_HOST")
        self.pinecone_region = os.getenv("PINECONE_REGION", "us-east-1")
        
        # BGE-M3é…ç½®
        self.embedding_dimension = 1024
        
        # ç´¢å¼•åç§°
        self.index_name = f"german-parliament-{year}"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
        # åˆå§‹åŒ–BGE-M3å®¢æˆ·ç«¯ï¼ˆæœ¬åœ°ï¼‰
        self.embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–Pinecone + BGE-M3 {year}å¹´æ•°æ®æ€§èƒ½æµ‹è¯•")
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        logger.info("ğŸ” éªŒè¯é…ç½®...")
        
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_VECTOR_DATABASE_API_KEY æœªé…ç½®")
        
        if not self.pinecone_host:
            raise ValueError("PINECONE_HOST æœªé…ç½®")
        
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"  Pinecone Host: {self.pinecone_host}")
        logger.info(f"  Embedding Model: BGE-M3 (æœ¬åœ°)")
        logger.info(f"  Embedding Dimension: {self.embedding_dimension}")
    
    def _record_metric(self, stage_name: str, start_time: float, end_time: float, **kwargs):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetrics(
            stage_name=stage_name,
            start_time=start_time,
            end_time=end_time,
            duration=end_time - start_time,
            **kwargs
        )
        self.metrics.append(metric)
        
        logger.info(f"âœ… {stage_name}: {metric.duration:.2f}ç§’ ({metric.duration_minutes:.2f}åˆ†é’Ÿ)")
        if metric.records_processed > 0:
            logger.info(f"   å¤„ç†é€Ÿåº¦: {metric.records_processed/metric.duration:.1f} è®°å½•/ç§’")
        if metric.chunks_generated > 0:
            logger.info(f"   åˆ†å—é€Ÿåº¦: {metric.chunks_generated/metric.duration:.1f} chunks/ç§’")
        if metric.vectors_created > 0:
            logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {metric.vectors_created/metric.duration:.1f} å‘é‡/ç§’")
    
    def stage_1_data_loading(self) -> List[Dict]:
        """é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ"""
        logger.info("ğŸ“– é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ")
        
        data_file = project_root / f"data/pp_json_49-21/pp_{self.year}.json"
        
        if not data_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°{self.year}å¹´æ•°æ®æ–‡ä»¶: {data_file}")
        
        start_time = time.time()
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        transcript = data.get('transcript', [])
        
        end_time = time.time()
        
        self._record_metric(
            "æ•°æ®åŠ è½½å’ŒJSONè§£æ",
            start_time,
            end_time,
            records_processed=len(transcript)
        )
        
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {len(transcript):,}æ¡è®°å½•, æ–‡ä»¶å¤§å°: {data_file.stat().st_size / (1024*1024):.1f}MB")
        
        return transcript
    
    def stage_2_text_chunking(self, transcript: List[Dict]) -> List[Dict]:
        """é˜¶æ®µ2: æ–‡æœ¬åˆ†å—"""
        logger.info("âœ‚ï¸  é˜¶æ®µ2: æ–‡æœ¬åˆ†å—")
        
        start_time = time.time()
        
        all_chunks = []
        valid_records = 0
        
        for i, record in enumerate(transcript):
            if not isinstance(record, dict):
                continue
            
            speech_text = record.get('speech', '').strip()
            if not speech_text or len(speech_text) < 10:
                continue
                
            valid_records += 1
            
            # åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(speech_text)
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = record.get('metadata', {})
            base_metadata = {
                "year": metadata.get('year', self.year),
                "month": str(metadata.get('month', '')),
                "day": str(metadata.get('day', '')),
                "speaker": str(metadata.get('speaker', '')),
                "party": str(metadata.get('party', '')),
                "text_id": str(metadata.get('id', f"{self.year}_{i}")),
                "record_index": i
            }
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": j,
                    "total_chunks": len(chunks),
                    "chunk_id": f"{self.year}_{i}_{j}"
                })
                
                all_chunks.append({
                    "id": f"{self.year}_{i}_{j}",
                    "text": chunk,
                    "metadata": chunk_metadata
                })
        
        end_time = time.time()
        
        self._record_metric(
            "æ–‡æœ¬åˆ†å—",
            start_time,
            end_time,
            records_processed=valid_records,
            chunks_generated=len(all_chunks)
        )
        
        logger.info(f"ğŸ“Š åˆ†å—ç»“æœ: {valid_records:,}æ¡æœ‰æ•ˆè®°å½• â†’ {len(all_chunks):,}ä¸ªchunks")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ¡è®°å½•: {len(all_chunks)/valid_records:.1f}ä¸ªchunks")
        
        return all_chunks
    
    def stage_3_bge_m3_embedding(self, chunks: List[Dict]) -> List[List[float]]:
        """é˜¶æ®µ3: BGE-M3 æœ¬åœ°embeddingç”Ÿæˆ"""
        logger.info("ğŸ§  é˜¶æ®µ3: BGE-M3 æœ¬åœ°embeddingç”Ÿæˆ")
        
        start_time = time.time()
        
        texts = [chunk["text"] for chunk in chunks]
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„BGE-M3å‚æ•°
        embeddings = self.embedding_client.embed_batch(
            texts,
            batch_size=800,  # ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°
            max_workers=20,
            request_delay=0.5
        )
        
        end_time = time.time()
        
        self._record_metric(
            "BGE-M3 æœ¬åœ°å‘é‡åŒ–",
            start_time,
            end_time,
            vectors_created=len(embeddings if embeddings else [])
        )
        
        if embeddings:
            logger.info(f"ğŸ“Š å‘é‡åŒ–ç»“æœ: {len(embeddings):,}ä¸ª{self.embedding_dimension}ç»´å‘é‡")
        else:
            logger.error("âŒ BGE-M3 embeddingç”Ÿæˆå¤±è´¥")
            
        return embeddings or []
    
    def stage_4_create_pinecone_index(self):
        """é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼•ï¼ˆManual Configurationï¼‰"""
        logger.info("ğŸ“¦ é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼• (Manual Configuration)")
        
        start_time = time.time()
        
        try:
            import pinecone
            from pinecone import Pinecone, ServerlessSpec
            
            # åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯
            pc = Pinecone(api_key=self.pinecone_api_key)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
            existing_indexes = pc.list_indexes()
            index_names = [idx.name for idx in existing_indexes]
            
            if self.index_name in index_names:
                logger.info(f"âš ï¸  ç´¢å¼• {self.index_name} å·²å­˜åœ¨ï¼Œåˆ é™¤æ—§ç´¢å¼•...")
                pc.delete_index(self.index_name)
                time.sleep(10)  # ç­‰å¾…åˆ é™¤å®Œæˆ
            
            # åˆ›å»ºæ–°ç´¢å¼• - Manual Configuration
            logger.info(f"ğŸ”¨ åˆ›å»ºæ–°ç´¢å¼• (Manual Configuration): {self.index_name}")
            logger.info(f"   ç»´åº¦: {self.embedding_dimension} (BGE-M3)")
            logger.info(f"   åº¦é‡: cosine")
            
            pc.create_index(
                name=self.index_name,
                dimension=self.embedding_dimension,  # BGE-M3çš„1024ç»´
                metric="cosine",
                spec=ServerlessSpec(
                    cloud="aws",
                    region=self.pinecone_region
                )
            )
            
            # ç­‰å¾…ç´¢å¼•å‡†å¤‡å°±ç»ª
            logger.info("â³ ç­‰å¾…ç´¢å¼•åˆå§‹åŒ–...")
            time.sleep(30)  # Pineconeç´¢å¼•éœ€è¦æ—¶é—´åˆå§‹åŒ–
            
            # è¿æ¥åˆ°ç´¢å¼•
            self.index = pc.Index(self.index_name)
            
            end_time = time.time()
            
            self._record_metric("åˆ›å»ºPineconeç´¢å¼• (Manual)", start_time, end_time)
            
            logger.info(f"âœ… Pinecone Manual Configurationç´¢å¼•åˆ›å»ºæˆåŠŸ: {self.index_name}")
            
        except ImportError:
            logger.error("âŒ è¯·å®‰è£…pinecone-client: pip install pinecone-client")
            raise
        except Exception as e:
            logger.error(f"âŒ Pineconeç´¢å¼•åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def stage_5_pinecone_storage(self, chunks: List[Dict], embeddings: List[List[float]]):
        """é˜¶æ®µ5: è‡ªå®šä¹‰å‘é‡å­˜å‚¨åˆ°Pinecone"""
        logger.info("ğŸ’¾ é˜¶æ®µ5: è‡ªå®šä¹‰å‘é‡å­˜å‚¨åˆ°Pinecone")
        
        start_time = time.time()
        
        if len(chunks) != len(embeddings):
            logger.error(f"âŒ æ•°æ®ä¸åŒ¹é…: {len(chunks)} chunks vs {len(embeddings)} embeddings")
            return
        
        batch_size = 100  # Pineconeæ¨èæ‰¹æ¬¡å¤§å°
        total_batches = len(chunks) // batch_size + (1 if len(chunks) % batch_size else 0)
        
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å­˜å‚¨: {len(chunks):,}ä¸ªè‡ªå®šä¹‰å‘é‡, {total_batches}ä¸ªæ‰¹æ¬¡")
        
        stored_count = 0
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            
            # æ„å»ºPineconeå‘é‡æ ¼å¼
            vectors = []
            for chunk, embedding in zip(batch_chunks, batch_embeddings):
                # ç¡®ä¿embeddingæ˜¯æœ‰æ•ˆçš„
                if not embedding or len(embedding) != self.embedding_dimension:
                    logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆå‘é‡: {chunk['id']}")
                    continue
                
                vectors.append({
                    "id": chunk["id"],
                    "values": embedding,
                    "metadata": {
                        "text": chunk["text"][:1000],  # Pinecone metadataæœ‰å¤§å°é™åˆ¶
                        **{k: str(v)[:100] for k, v in chunk["metadata"].items()}  # ç¡®ä¿metadataå­—ç¬¦ä¸²ä¸å¤ªé•¿
                    }
                })
            
            # æ’å…¥åˆ°Pinecone
            if vectors:  # åªæœ‰å½“æœ‰æœ‰æ•ˆå‘é‡æ—¶æ‰æ’å…¥
                try:
                    self.index.upsert(vectors=vectors)
                    stored_count += len(vectors)
                    
                    if (i // batch_size + 1) % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡æŠ¥å‘Šè¿›åº¦
                        progress = (stored_count / len(chunks)) * 100
                        logger.info(f"ğŸ“ˆ å­˜å‚¨è¿›åº¦: {progress:.1f}% ({stored_count:,}/{len(chunks):,})")
                    
                    # Pineconeé™åˆ¶ï¼šé¿å…è¿‡å¿«è¯·æ±‚
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡å­˜å‚¨å¤±è´¥: {str(e)}")
                    # ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
        
        end_time = time.time()
        
        self._record_metric(
            "è‡ªå®šä¹‰å‘é‡å­˜å‚¨åˆ°Pinecone",
            start_time,
            end_time,
            vectors_created=stored_count
        )
        
        logger.info(f"âœ… å­˜å‚¨å®Œæˆ: {stored_count:,}ä¸ªBGE-M3å‘é‡æˆåŠŸå­˜å‚¨åˆ°Pinecone")
        
        # ç­‰å¾…ç´¢å¼•å®Œæˆ
        logger.info("â³ ç­‰å¾…Pineconeç´¢å¼•å®Œæˆ...")
        time.sleep(10)
    
    def stage_6_verification(self):
        """é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ"""
        logger.info("ğŸ” é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ")
        
        start_time = time.time()
        
        try:
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.index.describe_index_stats()
            
            total_vectors = stats['total_vector_count']
            dimension = stats['dimension']
            
            logger.info(f"ğŸ“Š Pineconeç´¢å¼•éªŒè¯ç»“æœ:")
            logger.info(f"   æ€»å‘é‡æ•°: {total_vectors:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {dimension} (BGE-M3)")
            logger.info(f"   ç´¢å¼•çŠ¶æ€: å°±ç»ª")
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            if total_vectors > 0:
                logger.info("ğŸ” æµ‹è¯•BGE-M3å‘é‡æœç´¢åŠŸèƒ½...")
                # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å‘é‡ï¼ˆéšæœºå‘é‡ï¼‰
                import random
                test_vector = [random.uniform(-0.1, 0.1) for _ in range(self.embedding_dimension)]
                
                search_results = self.index.query(
                    vector=test_vector,
                    top_k=3,
                    include_metadata=True
                )
                
                logger.info(f"âœ… BGE-M3å‘é‡æœç´¢æµ‹è¯•æˆåŠŸï¼Œè¿”å› {len(search_results.matches)} ä¸ªç»“æœ")
                
                # æ˜¾ç¤ºæœç´¢ç»“æœç¤ºä¾‹
                for i, match in enumerate(search_results.matches[:2], 1):
                    score = match.score
                    text = match.metadata.get('text', '')[:100] + "..." if len(match.metadata.get('text', '')) > 100 else match.metadata.get('text', '')
                    logger.info(f"   [{i}] ç›¸ä¼¼åº¦: {score:.4f}, æ–‡æœ¬: {text}")
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        end_time = time.time()
        
        self._record_metric("éªŒè¯BGE-M3å­˜å‚¨ç»“æœ", start_time, end_time)
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Pinecone + BGE-M3å®Œæ•´æµç¨‹æ€§èƒ½æµ‹è¯•")
        logger.info("=" * 80)
        logger.info(f"æµ‹è¯•å¹´ä»½: {self.year}")
        logger.info(f"å‘é‡æ•°æ®åº“: Pinecone (Manual Configuration)")
        logger.info(f"Embeddingæ¨¡å‹: BGE-M3 (æœ¬åœ°, 1024ç»´)")
        logger.info("=" * 80)
        
        try:
            # é˜¶æ®µ1: æ•°æ®åŠ è½½
            transcript = self.stage_1_data_loading()
            
            # é˜¶æ®µ2: æ–‡æœ¬åˆ†å—
            chunks = self.stage_2_text_chunking(transcript)
            
            # é˜¶æ®µ3: BGE-M3 embedding
            embeddings = self.stage_3_bge_m3_embedding(chunks)
            
            if not embeddings:
                logger.error("âŒ Embeddingç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                return False
            
            # é˜¶æ®µ4: åˆ›å»ºPineconeç´¢å¼•
            self.stage_4_create_pinecone_index()
            
            # é˜¶æ®µ5: å‘é‡å­˜å‚¨
            self.stage_5_pinecone_storage(chunks, embeddings)
            
            # é˜¶æ®µ6: éªŒè¯ç»“æœ
            self.stage_6_verification()
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            self.generate_performance_report()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š Pinecone + BGE-M3 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 80)
        
        total_time = sum(metric.duration for metric in self.metrics)
        total_records = self.metrics[0].records_processed if self.metrics else 0
        total_chunks = max((metric.chunks_generated for metric in self.metrics), default=0)
        total_vectors = max((metric.vectors_created for metric in self.metrics), default=0)
        
        logger.info(f"ğŸ¯ æ€»ä½“æ€§èƒ½:")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"   å¤„ç†è®°å½•: {total_records:,}æ¡")
        logger.info(f"   ç”Ÿæˆchunks: {total_chunks:,}ä¸ª")
        logger.info(f"   åˆ›å»ºå‘é‡: {total_vectors:,}ä¸ª")
        logger.info(f"   æ•´ä½“é€Ÿåº¦: {total_records/total_time:.1f} è®°å½•/ç§’")
        logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {total_vectors/total_time:.1f} å‘é‡/ç§’")
        
        logger.info(f"\nğŸ“‹ å„é˜¶æ®µè¯¦ç»†æ—¶é—´:")
        for metric in self.metrics:
            percentage = (metric.duration / total_time) * 100
            logger.info(f"   {metric.stage_name}: {metric.duration:.2f}ç§’ ({percentage:.1f}%)")
        
        # æˆæœ¬å¯¹æ¯”
        logger.info(f"\nğŸ’° æˆæœ¬å¯¹æ¯”:")
        logger.info(f"   BGE-M3 embeddingæˆæœ¬: $0 (æœ¬åœ°å…è´¹)")
        logger.info(f"   OpenAI embeddingæˆæœ¬: ~$1.5 (çœä¸‹äº†)")
        logger.info(f"   Pineconeå­˜å‚¨æœˆè´¹: $70")
        
        # å…¨é‡æ•°æ®é¢„ä¼°
        if total_records > 0:
            total_data_records = 835689  # æ‰€æœ‰å¹´ä»½æ€»è®°å½•æ•°
            scale_factor = total_data_records / total_records
            
            estimated_total_time = total_time * scale_factor
            
            logger.info(f"\nğŸ”® å…¨é‡æ•°æ®é¢„ä¼°:")
            logger.info(f"   é¢„ä¼°æ€»æ—¶é—´: {estimated_total_time/60:.1f}åˆ†é’Ÿ ({estimated_total_time/3600:.2f}å°æ—¶)")
            logger.info(f"   é¢„ä¼°æ€»æˆæœ¬: $0 (embeddingå…è´¹) + $70/æœˆ (Pinecone)")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"pinecone_bge_m3_report_{self.year}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# Pinecone + BGE-M3 {self.year}å¹´æ•°æ®æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"## é…ç½®ä¿¡æ¯\n")
            f.write(f"- **å‘é‡æ•°æ®åº“**: Pinecone (Manual Configuration)\n")
            f.write(f"- **Embeddingæ¨¡å‹**: BGE-M3 (æœ¬åœ°)\n")
            f.write(f"- **å‘é‡ç»´åº¦**: {self.embedding_dimension}\n")
            f.write(f"- **æµ‹è¯•å¹´ä»½**: {self.year}\n\n")
            
            f.write(f"## æ€»ä½“æ€§èƒ½\n")
            f.write(f"- **æ€»è€—æ—¶**: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)\n")
            f.write(f"- **å¤„ç†è®°å½•**: {total_records:,}æ¡\n")
            f.write(f"- **ç”Ÿæˆchunks**: {total_chunks:,}ä¸ª\n")
            f.write(f"- **åˆ›å»ºå‘é‡**: {total_vectors:,}ä¸ª\n")
            f.write(f"- **æ•´ä½“é€Ÿåº¦**: {total_records/total_time:.1f} è®°å½•/ç§’\n")
            f.write(f"- **å‘é‡åŒ–é€Ÿåº¦**: {total_vectors/total_time:.1f} å‘é‡/ç§’\n\n")
            
            f.write(f"## å„é˜¶æ®µè¯¦ç»†æ—¶é—´\n\n")
            for metric in self.metrics:
                percentage = (metric.duration / total_time) * 100
                f.write(f"### {metric.stage_name}\n")
                f.write(f"- **è€—æ—¶**: {metric.duration:.2f}ç§’ ({percentage:.1f}%)\n\n")
            
            f.write(f"## æˆæœ¬åˆ†æ\n")
            f.write(f"- **BGE-M3 embedding**: $0 (æœ¬åœ°å…è´¹)\n")
            f.write(f"- **ç›¸æ¯”OpenAIèŠ‚çœ**: ~$1.5/å¹´\n")
            f.write(f"- **Pineconeæœˆè´¹**: $70\n\n")
            
            f.write(f"## ç»“è®º\n")
            f.write(f"BGE-M3 + Pinecone Customæ–¹æ¡ˆéªŒè¯æˆåŠŸï¼Œæˆæœ¬ä½ä¸”æ€§èƒ½ä¼˜ç§€ã€‚\n")
        
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pinecone + BGE-M3 æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--year", type=int, default=2015, help="æµ‹è¯•å¹´ä»½")
    
    args = parser.parse_args()
    
    test = PineconeBGEM3Test(year=args.year)
    success = test.run_complete_test()
    
    if success:
        logger.info("\nğŸ‰ Pinecone + BGE-M3 æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        return 0
    else:
        logger.error("\nâŒ Pinecone + BGE-M3 æ€§èƒ½æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())

