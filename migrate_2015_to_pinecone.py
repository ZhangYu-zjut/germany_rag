#!/usr/bin/env python3
"""
2015å¹´æ•°æ®è¿ç§»åˆ°Pinecone german-bge index
ä½¿ç”¨BGE-M3æ¨¡å‹ (1024ç»´)
Chunké…ç½®: size=4000, overlap=800
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter

logger = setup_logger()

@dataclass
class ProcessingStats:
    """æ•°æ®å¤„ç†ç»Ÿè®¡"""
    total_records: int = 0
    total_chunks: int = 0
    total_vectors: int = 0
    processing_time: float = 0
    embedding_time: float = 0
    upload_time: float = 0

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š 2015å¹´æ•°æ®å¤„ç†ç»Ÿè®¡")
        logger.info("="*60)
        logger.info(f"æ€»è®°å½•æ•°: {self.total_records}")
        logger.info(f"ç”ŸæˆChunks: {self.total_chunks}")
        logger.info(f"ç”Ÿæˆå‘é‡æ•°: {self.total_vectors}")
        logger.info(f"æ•°æ®å¤„ç†æ—¶é—´: {self.processing_time:.2f}ç§’")
        logger.info(f"Embeddingæ—¶é—´: {self.embedding_time:.2f}ç§’")
        logger.info(f"ä¸Šä¼ æ—¶é—´: {self.upload_time:.2f}ç§’")
        logger.info(f"æ€»è€—æ—¶: {self.processing_time + self.embedding_time + self.upload_time:.2f}ç§’")
        logger.info("="*60)

class Migrate2015ToPinecone:
    """2015å¹´æ•°æ®è¿ç§»åˆ°Pinecone"""

    def __init__(self, chunk_size: int = 4000, chunk_overlap: int = 800):
        """åˆå§‹åŒ–è¿ç§»ç³»ç»Ÿ"""
        logger.info("ğŸš€ åˆå§‹åŒ–2015å¹´æ•°æ®è¿ç§»åˆ°Pinecone")
        logger.info(f"ğŸ“ Chunké…ç½®: size={chunk_size}, overlap={chunk_overlap}")

        # Pineconeé…ç½®
        self.pinecone_api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pinecone_host = os.getenv("PINECONE_HOST")

        if not self.pinecone_api_key:
            raise ValueError("âŒ PINECONE_VECTOR_DATABASE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        if not self.pinecone_host:
            raise ValueError("âŒ PINECONE_HOST ç¯å¢ƒå˜é‡æœªè®¾ç½®")

        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        self.text_splitter = ParliamentTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

        # Pineconeå®¢æˆ·ç«¯
        self.pc = None
        self.index = None
        self.index_name = "german-bge"

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = ProcessingStats()

        logger.info("âœ… è¿ç§»ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def init_pinecone(self):
        """åˆå§‹åŒ–Pineconeè¿æ¥"""
        logger.info("ğŸ”— è¿æ¥Pinecone...")

        try:
            from pinecone import Pinecone

            self.pc = Pinecone(api_key=self.pinecone_api_key)

            # è¿æ¥åˆ°german-bge index
            self.index = self.pc.Index(self.index_name, host=self.pinecone_host)

            # è·å–indexç»Ÿè®¡ä¿¡æ¯
            stats = self.index.describe_index_stats()
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°index: {self.index_name}")
            logger.info(f"ğŸ“Š å½“å‰indexç»Ÿè®¡: {stats}")

            return True

        except Exception as e:
            logger.error(f"âŒ Pineconeè¿æ¥å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def load_2015_data(self) -> List[Dict]:
        """åŠ è½½2015å¹´æ•°æ®"""
        logger.info("ğŸ“‚ åŠ è½½2015å¹´æ•°æ®...")

        data_file = Path("data/pp_json_49-21/pp_2015.json")

        if not data_file.exists():
            raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

        start_time = time.time()

        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        transcripts = data.get('transcript', [])

        # è¿‡æ»¤æœ‰æ•ˆè®°å½•ï¼ˆæœ‰speakerå’Œspeechå†…å®¹ï¼‰
        valid_records = []
        for record in transcripts:
            metadata = record.get('metadata', {})
            speech = record.get('speech', '')

            if speech and speech.strip():
                valid_records.append({
                    'metadata': metadata,
                    'speech': speech
                })

        load_time = time.time() - start_time

        logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(transcripts)}æ¡åŸå§‹è®°å½•")
        logger.info(f"âœ… æœ‰æ•ˆè®°å½•: {len(valid_records)}æ¡")
        logger.info(f"â±ï¸  åŠ è½½è€—æ—¶: {load_time:.2f}ç§’")

        self.stats.total_records = len(valid_records)

        return valid_records

    def process_data_to_chunks(self, records: List[Dict]) -> List[Dict]:
        """å°†æ•°æ®å¤„ç†æˆchunks"""
        logger.info("âœ‚ï¸  å¼€å§‹æ–‡æœ¬åˆ†å—...")

        start_time = time.time()

        # ä½¿ç”¨text_splitterå¤„ç†
        chunks = self.text_splitter.split_speeches(records)

        process_time = time.time() - start_time
        self.stats.processing_time = process_time
        self.stats.total_chunks = len(chunks)

        logger.info(f"âœ… åˆ†å—å®Œæˆ: {len(chunks)}ä¸ªchunks")
        logger.info(f"â±ï¸  å¤„ç†è€—æ—¶: {process_time:.2f}ç§’")

        return chunks

    def create_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆembeddings"""
        logger.info("ğŸ§® å¼€å§‹ç”Ÿæˆembeddings...")
        logger.info("ğŸ“ ä½¿ç”¨BGE-M3æœ¬åœ°æ¨¡å‹ (1024ç»´)")

        start_time = time.time()

        try:
            # ä½¿ç”¨æœ¬åœ°BGE-M3æ¨¡å‹
            from FlagEmbedding import BGEM3FlagModel

            logger.info("ğŸ“¥ åŠ è½½BGE-M3æ¨¡å‹...")
            model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # æ‰¹é‡ç”Ÿæˆembeddings
            batch_size = 16
            embedded_chunks = []

            for i in tqdm(range(0, len(chunks), batch_size), desc="ç”Ÿæˆembeddings"):
                batch = chunks[i:i+batch_size]
                texts = [chunk['text'] for chunk in batch]

                # ç”Ÿæˆembeddings
                embeddings = model.encode(texts, batch_size=batch_size)['dense_vecs']

                # æ·»åŠ åˆ°ç»“æœ
                for j, chunk in enumerate(batch):
                    embedded_chunk = chunk.copy()
                    embedded_chunk['values'] = embeddings[j].tolist()
                    embedded_chunks.append(embedded_chunk)

            embed_time = time.time() - start_time
            self.stats.embedding_time = embed_time
            self.stats.total_vectors = len(embedded_chunks)

            logger.info(f"âœ… Embeddingsç”Ÿæˆå®Œæˆ: {len(embedded_chunks)}ä¸ªå‘é‡")
            logger.info(f"â±ï¸  Embeddingè€—æ—¶: {embed_time:.2f}ç§’")

            return embedded_chunks

        except Exception as e:
            logger.error(f"âŒ Embeddingç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def upload_to_pinecone(self, embedded_chunks: List[Dict]):
        """ä¸Šä¼ å‘é‡åˆ°Pinecone"""
        logger.info("ğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°Pinecone...")

        start_time = time.time()

        try:
            batch_size = 100
            total_uploaded = 0

            for i in tqdm(range(0, len(embedded_chunks), batch_size), desc="ä¸Šä¼ å‘é‡"):
                batch = embedded_chunks[i:i+batch_size]

                # å‡†å¤‡upsertæ•°æ®
                vectors = []
                for chunk in batch:
                    metadata = chunk['metadata']
                    vector_id = f"2015_{metadata.get('id', f'chunk_{i}')}"

                    vectors.append({
                        'id': vector_id,
                        'values': chunk['values'],
                        'metadata': {
                            'year': str(metadata.get('year', '2015')),
                            'month': str(metadata.get('month', '')),
                            'day': str(metadata.get('day', '')),
                            'speaker': str(metadata.get('speaker', '')),
                            'group': str(metadata.get('group', '')),
                            'text': chunk['text'][:1000],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                            'session': str(metadata.get('session', '')),
                            'lp': str(metadata.get('lp', ''))
                        }
                    })

                # ä¸Šä¼ åˆ°Pinecone
                self.index.upsert(vectors=vectors)
                total_uploaded += len(vectors)

            upload_time = time.time() - start_time
            self.stats.upload_time = upload_time

            logger.info(f"âœ… ä¸Šä¼ å®Œæˆ: {total_uploaded}ä¸ªå‘é‡")
            logger.info(f"â±ï¸  ä¸Šä¼ è€—æ—¶: {upload_time:.2f}ç§’")

        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def run(self):
        """æ‰§è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ å¼€å§‹2015å¹´æ•°æ®è¿ç§»")
        logger.info("="*60 + "\n")

        try:
            # 1. è¿æ¥Pinecone
            if not self.init_pinecone():
                return False

            # 2. åŠ è½½æ•°æ®
            records = self.load_2015_data()

            # 3. æ•°æ®åˆ†å—
            chunks = self.process_data_to_chunks(records)

            # 4. ç”Ÿæˆembeddings
            embedded_chunks = self.create_embeddings(chunks)

            # 5. ä¸Šä¼ åˆ°Pinecone
            self.upload_to_pinecone(embedded_chunks)

            # 6. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.stats.print_summary()

            logger.info("\nâœ… 2015å¹´æ•°æ®è¿ç§»æˆåŠŸå®Œæˆï¼")

            return True

        except Exception as e:
            logger.error(f"\nâŒ è¿ç§»è¿‡ç¨‹å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

def main():
    """ä¸»å‡½æ•°"""
    migrator = Migrate2015ToPinecone(chunk_size=4000, chunk_overlap=800)
    success = migrator.run()

    if success:
        logger.info("\nğŸ‰ è¿ç§»æˆåŠŸå®Œæˆ!")
        return 0
    else:
        logger.error("\nâŒ è¿ç§»å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())
