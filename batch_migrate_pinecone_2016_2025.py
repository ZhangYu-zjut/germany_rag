#!/usr/bin/env python3
"""
æ‰¹é‡è¿ç§»2016-2025å¹´æ•°æ®åˆ°Pinecone
åŸºäºå·²éªŒè¯çš„migrate_2015_optimal_config.pyé…ç½®
"""

import os
import sys
import time
import json
import gc
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.llm.embeddings import GeminiEmbeddingClient
from src.data_loader.splitter import ParliamentTextSplitter
from pinecone import Pinecone

logger = setup_logger()


def migrate_year(year: int, embedding_client, text_splitter, pinecone_index, skip_if_exists=True):
    """
    è¿ç§»å•ä¸ªå¹´ä»½çš„æ•°æ®

    Args:
        year: å¹´ä»½
        embedding_client: Embeddingå®¢æˆ·ç«¯
        text_splitter: æ–‡æœ¬åˆ†å—å™¨
        pinecone_index: Pineconeç´¢å¼•
        skip_if_exists: å¦‚æœæ•°æ®å·²å­˜åœ¨åˆ™è·³è¿‡

    Returns:
        dict: è¿ç§»ç»“æœç»Ÿè®¡
    """
    logger.info("="*80)
    logger.info(f"ğŸš€ å¼€å§‹è¿ç§» {year} å¹´æ•°æ®")
    logger.info("="*80)

    start_time = time.time()

    try:
        # 1. æ£€æŸ¥æ˜¯å¦å·²è¿ç§»
        if skip_if_exists:
            stats = pinecone_index.describe_index_stats()
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥å¹´ä»½çš„æ•°æ®
            query_result = pinecone_index.query(
                vector=[0.0] * 1024,
                filter={"year": {"$eq": str(year)}},
                top_k=1,
                include_metadata=False
            )
            if query_result.matches:
                logger.info(f"â­ï¸  {year}å¹´æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                return {
                    "year": year,
                    "status": "skipped",
                    "reason": "already_exists"
                }

        # 2. åŠ è½½æ•°æ®
        data_file = project_root / "data" / "pp_json_49-21" / f"pp_{year}.json"

        if not data_file.exists():
            logger.warning(f"âš ï¸  {year}å¹´æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return {
                "year": year,
                "status": "failed",
                "reason": "file_not_found"
            }

        logger.info(f"ğŸ“‚ åŠ è½½{year}å¹´æ•°æ®æ–‡ä»¶...")
        with open(data_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

        if 'transcript' not in raw_data:
            logger.error(f"âŒ {year}å¹´æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯")
            return {
                "year": year,
                "status": "failed",
                "reason": "invalid_format"
            }

        data = raw_data['transcript']
        file_size_mb = data_file.stat().st_size / (1024*1024)

        logger.info(f"âœ… {year}å¹´æ•°æ®åŠ è½½æˆåŠŸ")
        logger.info(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        logger.info(f"   æ•°æ®æ¡æ•°: {len(data)}")

        # 3. åˆ†å—å¤„ç†
        logger.info(f"ğŸ”„ åˆ†å—{year}å¹´æ•°æ®...")
        all_chunks = []
        total_chars = 0

        for i, item in enumerate(data):
            text_id = item.get('text_id', f'{year}_unknown_{i}')
            speech = item.get('speech', '')
            metadata = item.get('metadata', {})

            if not speech.strip():
                continue

            total_chars += len(speech)

            # åˆ†å—
            chunk_texts = text_splitter.text_splitter.split_text(speech)

            # æ·»åŠ å…ƒæ•°æ®
            for j, chunk_text in enumerate(chunk_texts):
                chunk = {
                    'text_id': f"{text_id}_chunk_{j}",
                    'original_text_id': text_id,
                    'chunk_index': j,
                    'text': chunk_text.strip(),
                    'metadata': {
                        # Core time fields (separate for precise filtering)
                        'year': metadata.get('year', str(year)),
                        'month': metadata.get('month', '01'),
                        'day': metadata.get('day', '01'),

                        # Document ID (crucial for citation)
                        'id': metadata.get('id', text_id),

                        # Speaker information
                        'speaker': metadata.get('speaker', ''),
                        'group': metadata.get('group', ''),
                        'lp': metadata.get('lp', ''),

                        # Session information
                        'session': metadata.get('session', ''),

                        # User-friendly source reference: "id + speaker + time"
                        'source_reference': f"{metadata.get('id', text_id)} | {metadata.get('speaker', 'Unknown')} | {metadata.get('year', year)}-{metadata.get('month', '01')}-{metadata.get('day', '01')}",

                        # Technical fields
                        'chunk_size': len(chunk_text.strip()),
                        'total_chunks': len(chunk_texts),
                        'source': 'german_parliament'
                    }
                }
                all_chunks.append(chunk)

        logger.info(f"âœ… {year}å¹´æ•°æ®åˆ†å—å®Œæˆ")
        logger.info(f"   åŸå§‹è®°å½•: {len(data)} æ¡")
        logger.info(f"   ç”Ÿæˆå—æ•°: {len(all_chunks)} ä¸ª")
        logger.info(f"   å¹³å‡å—å¤§å°: {sum(len(c['text']) for c in all_chunks) / len(all_chunks):.0f} å­—ç¬¦")

        # 4. ç”ŸæˆEmbeddings
        logger.info(f"ğŸ§  ç”Ÿæˆ{year}å¹´BGE-M3 embeddings...")
        texts = [chunk['text'] for chunk in all_chunks]

        embedding_start = time.time()
        vectors = embedding_client.embed_batch(
            texts,
            batch_size=128,  # embeddingä¼˜åŒ–
            max_workers=1    # GPUå•çº¿ç¨‹
        )
        embedding_time = time.time() - embedding_start

        logger.info(f"âœ… Embeddingç”Ÿæˆå®Œæˆ")
        logger.info(f"   ç”Ÿæˆæ—¶é—´: {embedding_time:.2f} ç§’")
        logger.info(f"   å¤„ç†é€Ÿåº¦: {len(texts)/embedding_time:.1f} æ¡/ç§’")

        # æ£€æŸ¥NaN
        import math
        nan_count = sum(1 for v in vectors if any(math.isnan(x) or math.isinf(x) for x in v))
        if nan_count > 0:
            logger.error(f"âŒ å‘ç°{nan_count}ä¸ªNaN/Infå‘é‡")
            return {
                "year": year,
                "status": "failed",
                "reason": "nan_vectors"
            }

        # 5. ä¸Šä¼ åˆ°Pinecone
        logger.info(f"ğŸ“¤ ä¸Šä¼ {year}å¹´æ•°æ®åˆ°Pinecone...")

        # è·å–ä¸Šä¼ å‰çŠ¶æ€
        stats_before = pinecone_index.describe_index_stats()
        initial_count = stats_before['total_vector_count']

        # å‡†å¤‡å‘é‡æ•°æ®
        vector_data = []
        timestamp = int(time.time())

        for i, (chunk, vector) in enumerate(zip(all_chunks, vectors)):
            vector_item = {
                "id": f"{year}_{timestamp}_{i}",
                "values": vector,
                "metadata": {
                    **chunk['metadata'],
                    "text": chunk['text'][:1000] + "..." if len(chunk['text']) > 1000 else chunk['text'],
                    "original_text_id": chunk['original_text_id'],
                    "chunk_index": chunk['chunk_index'],
                    "upload_timestamp": timestamp
                }
            }
            vector_data.append(vector_item)

        # æ‰¹é‡ä¸Šä¼ 
        batch_size = 100
        total_batches = (len(vector_data) + batch_size - 1) // batch_size

        upload_start = time.time()
        uploaded_count = 0

        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, len(vector_data))
            batch_vectors = vector_data[batch_start:batch_end]

            try:
                pinecone_index.upsert(vectors=batch_vectors)
                uploaded_count += len(batch_vectors)

                if (batch_idx + 1) % 10 == 0:
                    logger.info(f"   è¿›åº¦: {batch_idx+1}/{total_batches} æ‰¹æ¬¡")
            except Exception as e:
                logger.error(f"   âŒ æ‰¹æ¬¡ {batch_idx+1} ä¸Šä¼ å¤±è´¥: {str(e)}")
                continue

        upload_time = time.time() - upload_start

        logger.info(f"âœ… Pineconeä¸Šä¼ å®Œæˆ")
        logger.info(f"   ä¸Šä¼ å‘é‡æ•°: {uploaded_count}")
        logger.info(f"   ä¸Šä¼ æ—¶é—´: {upload_time:.2f} ç§’")
        logger.info(f"   å¹³å‡é€Ÿåº¦: {uploaded_count/upload_time:.1f} å‘é‡/ç§’")

        # ç­‰å¾…ç´¢å¼•æ›´æ–°
        time.sleep(3)

        # éªŒè¯
        stats_after = pinecone_index.describe_index_stats()
        final_count = stats_after['total_vector_count']

        logger.info(f"ğŸ“Š ä¸Šä¼ éªŒè¯:")
        logger.info(f"   ä¸Šä¼ å‰: {initial_count} å‘é‡")
        logger.info(f"   ä¸Šä¼ å: {final_count} å‘é‡")
        logger.info(f"   å‡€å¢åŠ : {final_count - initial_count} å‘é‡")

        total_time = time.time() - start_time

        logger.info(f"ğŸ‰ {year}å¹´æ•°æ®è¿ç§»å®Œæˆ!")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨åˆ é™¤å˜é‡ä¹‹å‰ï¼‰
        records_count = len(data)
        chunks_count = len(all_chunks)

        # æ¸…ç†å†…å­˜
        del vectors, vector_data, all_chunks, data
        gc.collect()

        return {
            "year": year,
            "status": "success",
            "records": records_count,
            "chunks": chunks_count,
            "vectors": uploaded_count,
            "embedding_time": embedding_time,
            "upload_time": upload_time,
            "total_time": total_time
        }

    except Exception as e:
        logger.error(f"âŒ {year}å¹´æ•°æ®è¿ç§»å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        return {
            "year": year,
            "status": "failed",
            "reason": str(e)
        }


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹2016-2025å¹´æ•°æ®æ‰¹é‡è¿ç§»")
    logger.info("="*80)

    total_start = time.time()

    # åˆå§‹åŒ–
    logger.info("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")

    # Embeddingå®¢æˆ·ç«¯
    embedding_client = GeminiEmbeddingClient(
        embedding_mode="local",
        model_name="BAAI/bge-m3",
        dimensions=1024
    )
    logger.info("âœ… BGE-M3 Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    # Pinecone
    api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
    pc = Pinecone(api_key=api_key)
    index = pc.Index("german-bge")
    logger.info("âœ… Pineconeå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    # æ–‡æœ¬åˆ†å—å™¨
    text_splitter = ParliamentTextSplitter(
        chunk_size=4000,
        chunk_overlap=800
    )
    logger.info("âœ… æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–å®Œæˆ (4000/800)")

    # æ£€æŸ¥åˆå§‹çŠ¶æ€
    stats_initial = index.describe_index_stats()
    logger.info(f"ğŸ“Š è¿ç§»å‰PineconeçŠ¶æ€: {stats_initial['total_vector_count']} å‘é‡")

    # è¦è¿ç§»çš„å¹´ä»½ (2015å·²å®Œæˆï¼Œä»2016å¼€å§‹)
    years = list(range(2016, 2026))  # 2016-2025

    logger.info(f"ğŸ“‹ è®¡åˆ’è¿ç§»å¹´ä»½: {years}")
    logger.info(f"   å…±{len(years)}ä¸ªå¹´ä»½")

    results = []

    # é€å¹´è¿ç§»
    for i, year in enumerate(years, 1):
        logger.info(f"\n\n{'='*80}")
        logger.info(f"ğŸ“… å¤„ç†ç¬¬ {i}/{len(years)} ä¸ªå¹´ä»½: {year}")
        logger.info(f"{'='*80}\n")

        result = migrate_year(
            year=year,
            embedding_client=embedding_client,
            text_splitter=text_splitter,
            pinecone_index=index,
            skip_if_exists=True
        )

        results.append(result)

        # ä¿å­˜è¿›åº¦
        progress_file = project_root / "batch_migration_progress_2016_2025.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ° {progress_file}")

    # æ€»ç»“
    total_time = time.time() - total_start

    logger.info("\n\n" + "="*80)
    logger.info("ğŸ‰ æ‰¹é‡è¿ç§»å®Œæˆ!")
    logger.info("="*80)

    success_count = sum(1 for r in results if r['status'] == 'success')
    failed_count = sum(1 for r in results if r['status'] == 'failed')
    skipped_count = sum(1 for r in results if r['status'] == 'skipped')

    logger.info(f"\nğŸ“Š è¿ç§»ç»Ÿè®¡:")
    logger.info(f"   æˆåŠŸ: {success_count}")
    logger.info(f"   å¤±è´¥: {failed_count}")
    logger.info(f"   è·³è¿‡: {skipped_count}")
    logger.info(f"   æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")

    # è¯¦ç»†ç»“æœ
    logger.info(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for result in results:
        year = result['year']
        status = result['status']

        if status == 'success':
            logger.info(f"   âœ… {year}: {result['vectors']} å‘é‡, {result['total_time']/60:.1f} åˆ†é’Ÿ")
        elif status == 'failed':
            logger.info(f"   âŒ {year}: å¤±è´¥ - {result.get('reason', 'unknown')}")
        elif status == 'skipped':
            logger.info(f"   â­ï¸  {year}: è·³è¿‡ - {result.get('reason', 'unknown')}")

    # æœ€ç»ˆPineconeçŠ¶æ€
    stats_final = index.describe_index_stats()
    logger.info(f"\nğŸ“Š è¿ç§»åPineconeçŠ¶æ€:")
    logger.info(f"   æ€»å‘é‡æ•°: {stats_final['total_vector_count']}")
    logger.info(f"   å‡€å¢åŠ : {stats_final['total_vector_count'] - stats_initial['total_vector_count']}")

    return 0 if failed_count == 0 else 1


if __name__ == "__main__":
    exit(main())
