#!/usr/bin/env python3
"""
éªŒè¯Pineconeæ•°æ®çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import json
from collections import defaultdict
import time

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent
sys.path.append(str(project_root))
load_dotenv(project_root / ".env", override=True)

from pinecone import Pinecone
from src.utils.logger import setup_logger

logger = setup_logger()


def get_year_statistics(index, year: str, sample_size: int = 1000):
    """
    è·å–æŸå¹´çš„ç»Ÿè®¡ä¿¡æ¯
    ç”±äºPineconeçš„top_ké™åˆ¶ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•ä¼°ç®—
    """
    try:
        # å°è¯•è·å–å°½å¯èƒ½å¤šçš„æ ·æœ¬
        result = index.query(
            vector=[0.01] * 1024,
            top_k=10000,  # Pineconeæœ€å¤§é™åˆ¶
            filter={"year": {"$eq": year}},
            include_metadata=True
        )

        if not result.matches:
            return {
                "year": year,
                "count": 0,
                "metadata_issues": [],
                "sample_metadata": []
            }

        # åˆ†æå…ƒæ•°æ®å®Œæ•´æ€§
        metadata_issues = []
        speakers = set()
        parties = set()
        dates = set()

        required_fields = ["speaker", "date", "group", "year", "month", "day"]

        for match in result.matches[:min(100, len(result.matches))]:  # æ£€æŸ¥å‰100ä¸ª
            metadata = match.metadata

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in required_fields:
                if field not in metadata or metadata[field] in [None, "", "æœªçŸ¥", "Unknown"]:
                    metadata_issues.append({
                        "id": match.id,
                        "missing_field": field,
                        "metadata": metadata
                    })

            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            speakers.add(metadata.get("speaker", "æœªçŸ¥"))
            parties.add(metadata.get("group", "æœªçŸ¥"))
            dates.add(metadata.get("date", "æœªçŸ¥"))

        return {
            "year": year,
            "sampled_count": len(result.matches),
            "estimated_total": len(result.matches),  # å¦‚æœ<10000è¯´æ˜æ˜¯å‡†ç¡®æ•°å­—
            "is_exact": len(result.matches) < 10000,
            "unique_speakers": len(speakers),
            "unique_parties": len(parties),
            "unique_dates": len(dates),
            "metadata_issues_count": len(metadata_issues),
            "metadata_issues_sample": metadata_issues[:5],  # åªä¿ç•™å‰5ä¸ªç¤ºä¾‹
            "sample_speakers": list(speakers)[:5],
            "sample_parties": list(parties)[:5],
            "sample_dates": list(dates)[:5]
        }

    except Exception as e:
        logger.error(f"è·å–{year}å¹´ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return {
            "year": year,
            "error": str(e)
        }


def verify_metadata_consistency(index):
    """éªŒè¯å…ƒæ•°æ®ä¸€è‡´æ€§"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ” éªŒè¯Pineconeæ•°æ®å®Œæ•´æ€§")
    logger.info("="*80)

    # 1. è·å–æ€»ä½“ç»Ÿè®¡
    logger.info("\nğŸ“Š 1. æ€»ä½“ç»Ÿè®¡")
    logger.info("-" * 40)

    stats = index.describe_index_stats()
    total_vectors = stats.get("total_vector_count", 0)
    logger.info(f"æ€»å‘é‡æ•°: {total_vectors:,}")

    # 2. æŒ‰å¹´ä»½ç»Ÿè®¡
    logger.info("\nğŸ“Š 2. æŒ‰å¹´ä»½è¯¦ç»†ç»Ÿè®¡")
    logger.info("-" * 40)

    year_stats = {}
    total_sampled = 0

    for year in range(2015, 2026):
        year_str = str(year)
        logger.info(f"\næ­£åœ¨åˆ†æ {year_str}å¹´...")

        stat = get_year_statistics(index, year_str)
        year_stats[year_str] = stat

        if "error" in stat:
            logger.error(f"  âŒ {year_str}: {stat['error']}")
        else:
            count = stat.get("sampled_count", 0)
            total_sampled += count
            is_exact = stat.get("is_exact", False)

            logger.info(f"  {'âœ…' if count > 0 else 'âŒ'} {year_str}: {count:,} vectors {'(ç²¾ç¡®)' if is_exact else '(é‡‡æ ·)'}")
            logger.info(f"     å”¯ä¸€å‘è¨€äºº: {stat['unique_speakers']}")
            logger.info(f"     å”¯ä¸€å…šæ´¾: {stat['unique_parties']}")
            logger.info(f"     å”¯ä¸€æ—¥æœŸ: {stat['unique_dates']}")

            if stat['metadata_issues_count'] > 0:
                logger.warning(f"     âš ï¸  å…ƒæ•°æ®é—®é¢˜: {stat['metadata_issues_count']} ä¸ªå‘é‡å­˜åœ¨ç¼ºå¤±å­—æ®µ")
                logger.warning(f"     ç¤ºä¾‹ç¼ºå¤±å­—æ®µ: {stat['metadata_issues_sample'][0]['missing_field'] if stat['metadata_issues_sample'] else 'N/A'}")

        time.sleep(0.5)  # é¿å…APIé€Ÿç‡é™åˆ¶

    # 3. æ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("ğŸ“ˆ ç»Ÿè®¡æ€»ç»“")
    logger.info("="*80)
    logger.info(f"Pineconeæ€»å‘é‡æ•°: {total_vectors:,}")
    logger.info(f"é‡‡æ ·ç»Ÿè®¡å‘é‡æ•°: {total_sampled:,}")

    if total_sampled < total_vectors:
        logger.warning(f"âš ï¸  å·®å¼‚: {total_vectors - total_sampled:,} ä¸ªå‘é‡æœªè¢«ç»Ÿè®¡ï¼ˆå¯èƒ½è¶…å‡ºæŸ¥è¯¢é™åˆ¶ï¼‰")

    # 4. å…ƒæ•°æ®é—®é¢˜æ±‡æ€»
    logger.info("\n" + "="*80)
    logger.info("ğŸ” å…ƒæ•°æ®è´¨é‡åˆ†æ")
    logger.info("="*80)

    total_issues = sum(stat.get("metadata_issues_count", 0) for stat in year_stats.values() if "error" not in stat)

    if total_issues > 0:
        logger.warning(f"\nâš ï¸  å‘ç° {total_issues} ä¸ªå…ƒæ•°æ®é—®é¢˜")

        for year_str, stat in year_stats.items():
            if "error" not in stat and stat.get("metadata_issues_count", 0) > 0:
                logger.warning(f"\n{year_str}å¹´é—®é¢˜ç¤ºä¾‹:")
                for issue in stat["metadata_issues_sample"]:
                    logger.warning(f"  - ID: {issue['id']}")
                    logger.warning(f"    ç¼ºå¤±å­—æ®µ: {issue['missing_field']}")
                    logger.warning(f"    å…ƒæ•°æ®: {issue['metadata']}")
    else:
        logger.info("\nâœ… æ‰€æœ‰å…ƒæ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— ç¼ºå¤±å­—æ®µ")

    # 5. ä¿å­˜ç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ğŸ’¾ ä¿å­˜éªŒè¯ç»“æœ")
    logger.info("="*80)

    result = {
        "total_vectors": total_vectors,
        "total_sampled": total_sampled,
        "year_statistics": year_stats,
        "metadata_issues_total": total_issues
    }

    output_file = project_root / "data_integrity_report.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return result


def investigate_2025_metadata_issue(index):
    """ä¸“é—¨è°ƒæŸ¥2025å¹´å…ƒæ•°æ®é—®é¢˜"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ” æ·±åº¦è°ƒæŸ¥2025å¹´å…ƒæ•°æ®å¼‚å¸¸")
    logger.info("="*80)

    # è·å–2025å¹´çš„æ ·æœ¬æ•°æ®
    result = index.query(
        vector=[0.01] * 1024,
        top_k=50,
        filter={"year": {"$eq": "2025"}},
        include_metadata=True
    )

    if not result.matches:
        logger.error("âŒ 2025å¹´æ— æ•°æ®")
        return

    logger.info(f"\nè·å–åˆ° {len(result.matches)} ä¸ª2025å¹´æ ·æœ¬")
    logger.info("-" * 40)

    # åˆ†æå…ƒæ•°æ®
    metadata_analysis = defaultdict(int)
    speaker_values = defaultdict(int)
    date_values = defaultdict(int)

    for i, match in enumerate(result.matches[:10], 1):  # è¯¦ç»†åˆ†æå‰10ä¸ª
        metadata = match.metadata

        logger.info(f"\næ ·æœ¬ {i}:")
        logger.info(f"  ID: {match.id}")
        logger.info(f"  Score: {match.score:.4f}")
        logger.info(f"  å…ƒæ•°æ®:")
        for key, value in metadata.items():
            logger.info(f"    {key}: {value}")

            # ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
            if value in [None, "", "æœªçŸ¥", "Unknown"]:
                metadata_analysis[f"{key}_missing"] += 1

            if key == "speaker":
                speaker_values[str(value)] += 1
            if key == "date":
                date_values[str(value)] += 1

    # ç»Ÿè®¡åˆ†æ
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š 2025å¹´å…ƒæ•°æ®ç»Ÿè®¡")
    logger.info("="*80)

    all_speakers = set()
    all_dates = set()
    missing_speaker = 0
    missing_date = 0

    for match in result.matches:
        metadata = match.metadata
        speaker = metadata.get("speaker", "æœªçŸ¥")
        date = metadata.get("date", "æœªçŸ¥")

        all_speakers.add(speaker)
        all_dates.add(date)

        if speaker in [None, "", "æœªçŸ¥", "Unknown"]:
            missing_speaker += 1
        if date in [None, "", "æœªçŸ¥", "Unknown"]:
            missing_date += 1

    logger.info(f"\nå”¯ä¸€å‘è¨€äººæ•°: {len(all_speakers)}")
    logger.info(f"å‘è¨€äººåˆ—è¡¨: {list(all_speakers)[:10]}")
    logger.info(f"ç¼ºå¤±å‘è¨€äººçš„å‘é‡: {missing_speaker}/{len(result.matches)}")

    logger.info(f"\nå”¯ä¸€æ—¥æœŸæ•°: {len(all_dates)}")
    logger.info(f"æ—¥æœŸåˆ—è¡¨: {list(all_dates)[:10]}")
    logger.info(f"ç¼ºå¤±æ—¥æœŸçš„å‘é‡: {missing_date}/{len(result.matches)}")

    # æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶
    logger.info("\n" + "="*80)
    logger.info("ğŸ” æ£€æŸ¥2025å¹´åŸå§‹æ•°æ®æ–‡ä»¶")
    logger.info("="*80)

    data_file = project_root / "data" / "pp_json_49-21" / "pp_2025.json"
    if data_file.exists():
        logger.info(f"âœ… æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {data_file}")

        try:
            with open(data_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            logger.info(f"åŸå§‹æ•°æ®è®°å½•æ•°: {len(data)}")

            # æ£€æŸ¥å‰å‡ æ¡è®°å½•
            logger.info("\nåŸå§‹æ•°æ®ç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:")
            for i, record in enumerate(data[:3], 1):
                logger.info(f"\nè®°å½• {i}:")
                logger.info(f"  speaker: {record.get('speaker', 'æœªçŸ¥')}")
                logger.info(f"  date: {record.get('date', 'æœªçŸ¥')}")
                logger.info(f"  group: {record.get('group', 'æœªçŸ¥')}")
                logger.info(f"  texté•¿åº¦: {len(record.get('text', ''))}")

        except Exception as e:
            logger.error(f"âŒ è¯»å–åŸå§‹æ•°æ®å¤±è´¥: {str(e)}")
    else:
        logger.error(f"âŒ æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {data_file}")


if __name__ == "__main__":
    try:
        # è¿æ¥Pinecone
        logger.info("ğŸ”— è¿æ¥Pinecone...")
        pc = Pinecone(api_key=os.getenv("PINECONE_VECTOR_DATABASE_API_KEY"))
        index = pc.Index("german-bge")
        logger.info("âœ… Pineconeè¿æ¥æˆåŠŸ\n")

        # æ‰§è¡ŒéªŒè¯
        result = verify_metadata_consistency(index)

        # ä¸“é—¨è°ƒæŸ¥2025å¹´
        investigate_2025_metadata_issue(index)

        logger.info("\n" + "="*80)
        logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)
