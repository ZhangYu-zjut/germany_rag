#!/usr/bin/env python3
"""
Pineconeæ€§èƒ½è¯Šæ–­å·¥å…·
åˆ†æå½±å“Pineconeå­˜å‚¨æ€§èƒ½çš„å„ç§å› ç´ 
"""

import os
import sys
import time
import requests
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger

logger = setup_logger()

def check_network_conditions():
    """æ£€æŸ¥ç½‘ç»œæ¡ä»¶å’Œä»£ç†è®¾ç½®"""
    logger.info("ğŸŒ ç½‘ç»œæ¡ä»¶è¯Šæ–­")
    
    # 1. æ£€æŸ¥ä»£ç†è®¾ç½®
    http_proxy = os.environ.get('http_proxy')
    https_proxy = os.environ.get('https_proxy')
    
    logger.info("ğŸ” ä»£ç†è®¾ç½®æ£€æŸ¥:")
    logger.info(f"   HTTPä»£ç†: {http_proxy if http_proxy else 'æœªè®¾ç½®'}")
    logger.info(f"   HTTPSä»£ç†: {https_proxy if https_proxy else 'æœªè®¾ç½®'}")
    
    if http_proxy or https_proxy:
        logger.warning("âš ï¸ å‘ç°ä»£ç†è®¾ç½®ï¼Œè¿™å¯èƒ½ä¸¥é‡å½±å“Pineconeæ€§èƒ½!")
        logger.info("ğŸ’¡ å»ºè®®: æš‚æ—¶ç¦ç”¨ä»£ç†æµ‹è¯•æ€§èƒ½å·®å¼‚")
    
    # 2. ç½‘ç»œå»¶è¿Ÿæµ‹è¯•
    logger.info("ğŸƒ ç½‘ç»œå»¶è¿Ÿæµ‹è¯•:")
    test_urls = [
        ("Pinecone API", "https://api.pinecone.io"),
        ("Google DNS", "https://8.8.8.8"),
        ("Cloudflare DNS", "https://1.1.1.1"),
    ]
    
    for name, url in test_urls:
        try:
            start_time = time.time()
            response = requests.get(url, timeout=10)
            latency = (time.time() - start_time) * 1000
            logger.info(f"   {name}: {latency:.0f}ms (çŠ¶æ€: {response.status_code})")
        except Exception as e:
            logger.error(f"   {name}: è¿æ¥å¤±è´¥ ({str(e)})")
    
    # 3. å¸¦å®½ä¼°æµ‹
    logger.info("ğŸ“Š ç²—ç•¥å¸¦å®½æµ‹è¯•:")
    try:
        test_url = "https://httpbin.org/bytes/1024"  # ä¸‹è½½1KBæ•°æ®
        start_time = time.time()
        response = requests.get(test_url, timeout=10)
        download_time = time.time() - start_time
        
        if download_time > 0:
            bandwidth_kbps = 1 / download_time
            logger.info(f"   ä¼°ç®—å¸¦å®½: {bandwidth_kbps:.1f} KB/s")
            
            if bandwidth_kbps < 100:
                logger.warning("âš ï¸ ç½‘ç»œå¸¦å®½å¯èƒ½è¾ƒä½ï¼Œå½±å“Pineconeæ€§èƒ½")
        
    except Exception as e:
        logger.error(f"   å¸¦å®½æµ‹è¯•å¤±è´¥: {str(e)}")

def analyze_pinecone_plan_limits():
    """åˆ†æPineconeå¥—é¤é™åˆ¶"""
    logger.info("ğŸ’³ Pineconeå¥—é¤å’Œé™åˆ¶åˆ†æ")
    
    try:
        from pinecone import Pinecone
        
        api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        pc = Pinecone(api_key=api_key)
        
        # è·å–ç´¢å¼•ä¿¡æ¯
        index = pc.Index("german-bge")
        
        logger.info("ğŸ“Š å½“å‰ç´¢å¼•é…ç½®:")
        
        # å°è¯•è·å–ç´¢å¼•è¯¦æƒ…
        try:
            # è¿™ä¸ªAPIå¯èƒ½éœ€è¦ä¸åŒçš„æƒé™
            indexes = pc.list_indexes()
            for idx in indexes:
                if idx.name == "german-bge":
                    logger.info(f"   ç´¢å¼•å: {idx.name}")
                    logger.info(f"   ç»´åº¦: {idx.dimension}")
                    logger.info(f"   åº¦é‡: {idx.metric}")
                    logger.info(f"   Host: {idx.host}")
                    
                    # åˆ†æhostä¿¡æ¯æ¨æ–­å¥—é¤
                    if "gcp-starter" in idx.host:
                        plan_type = "Starter (å…è´¹å¥—é¤)"
                        performance_limit = "è¾ƒä½æ€§èƒ½é™åˆ¶"
                    elif "aws" in idx.host or "gcp" in idx.host:
                        plan_type = "Standard/Pro (ä»˜è´¹å¥—é¤)"
                        performance_limit = "æ›´é«˜æ€§èƒ½é™åˆ¶"
                    else:
                        plan_type = "æœªçŸ¥å¥—é¤ç±»å‹"
                        performance_limit = "æœªçŸ¥æ€§èƒ½é™åˆ¶"
                    
                    logger.info(f"   æ¨æ–­å¥—é¤: {plan_type}")
                    logger.info(f"   æ€§èƒ½é¢„æœŸ: {performance_limit}")
                    
        except Exception as e:
            logger.error(f"   è·å–ç´¢å¼•è¯¦æƒ…å¤±è´¥: {str(e)}")
        
        # åˆ†æä¸åŒå¥—é¤çš„ç†è®ºæ€§èƒ½
        logger.info("ğŸ“ˆ ä¸åŒå¥—é¤æ€§èƒ½å¯¹æ¯”:")
        plans = [
            {
                "name": "Starter (å…è´¹)",
                "qps_limit": "5-10 QPS",
                "expected_performance": "10-20 å‘é‡/ç§’",
                "notes": "ä¸¥æ ¼é€Ÿç‡é™åˆ¶"
            },
            {
                "name": "Standard", 
                "qps_limit": "100+ QPS",
                "expected_performance": "50-100 å‘é‡/ç§’",
                "notes": "é€‚ä¸­æ€§èƒ½"
            },
            {
                "name": "Pro/Enterprise",
                "qps_limit": "1000+ QPS", 
                "expected_performance": "200+ å‘é‡/ç§’",
                "notes": "é«˜æ€§èƒ½"
            }
        ]
        
        for plan in plans:
            logger.info(f"   {plan['name']}:")
            logger.info(f"     QPSé™åˆ¶: {plan['qps_limit']}")
            logger.info(f"     é¢„æœŸæ€§èƒ½: {plan['expected_performance']}")
            logger.info(f"     è¯´æ˜: {plan['notes']}")
        
        logger.info("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        logger.info("   1. å¦‚æœä½¿ç”¨å…è´¹å¥—é¤ï¼Œå‡çº§åˆ°ä»˜è´¹å¥—é¤å¯æ˜¾è‘—æå‡æ€§èƒ½")
        logger.info("   2. æ‰¹æ¬¡å¤§å°ä¼˜åŒ–ï¼šä»˜è´¹å¥—é¤å¯æ”¯æŒæ›´å¤§æ‰¹æ¬¡")
        logger.info("   3. å¹¶å‘ä¼˜åŒ–ï¼šä»˜è´¹å¥—é¤æ”¯æŒæ›´é«˜å¹¶å‘")
        
    except Exception as e:
        logger.error(f"âŒ Pineconeå¥—é¤åˆ†æå¤±è´¥: {str(e)}")

def test_optimal_batch_sizes():
    """æµ‹è¯•ä¸åŒå¥—é¤ä¸‹çš„æœ€ä½³æ‰¹æ¬¡å¤§å°"""
    logger.info("ğŸ§ª æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•")
    
    try:
        from pinecone import Pinecone
        from src.llm.embeddings import GeminiEmbeddingClient
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )
        
        api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        pc = Pinecone(api_key=api_key)
        index = pc.Index("german-bge")
        
        # ç”Ÿæˆæµ‹è¯•å‘é‡
        test_texts = [f"æ€§èƒ½æµ‹è¯•å‘é‡{i}" for i in range(20)]
        vectors = embedding_client.embed_batch(test_texts, batch_size=20)
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
        batch_sizes = [5, 10, 25, 50, 100]
        results = []
        
        for batch_size in batch_sizes:
            if batch_size > len(vectors):
                continue
                
            logger.info(f"ğŸ§ª æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # å‡†å¤‡æµ‹è¯•å‘é‡
            test_vectors = []
            timestamp = int(time.time())
            
            for i in range(batch_size):
                test_vectors.append({
                    "id": f"batch_test_{timestamp}_{batch_size}_{i}",
                    "values": vectors[i % len(vectors)],
                    "metadata": {"batch_test": True, "batch_size": batch_size}
                })
            
            # æ‰§è¡Œæµ‹è¯•
            start_time = time.time()
            try:
                upsert_response = index.upsert(vectors=test_vectors)
                upsert_time = time.time() - start_time
                
                speed = batch_size / upsert_time if upsert_time > 0 else 0
                success = True
                error_msg = None
                
            except Exception as e:
                upsert_time = time.time() - start_time
                speed = 0
                success = False
                error_msg = str(e)
            
            result = {
                "batch_size": batch_size,
                "time": upsert_time,
                "speed": speed,
                "success": success,
                "error": error_msg
            }
            results.append(result)
            
            logger.info(f"   ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            logger.info(f"   è€—æ—¶: {upsert_time:.2f}ç§’")
            logger.info(f"   é€Ÿåº¦: {speed:.1f} å‘é‡/ç§’")
            
            if error_msg:
                logger.warning(f"   é”™è¯¯: {error_msg}")
            
            # æ¸…ç†æµ‹è¯•å‘é‡
            try:
                test_ids = [v["id"] for v in test_vectors]
                index.delete(ids=test_ids)
            except:
                pass
            
            time.sleep(1)  # é¿å…APIé™åˆ¶
        
        # åˆ†æç»“æœ
        logger.info("ğŸ“Š æ‰¹æ¬¡å¤§å°æ€§èƒ½æ€»ç»“:")
        logger.info("æ‰¹æ¬¡å¤§å°  |  è€—æ—¶(ç§’)  |  é€Ÿåº¦(å‘é‡/ç§’)  |  çŠ¶æ€")
        logger.info("-" * 50)
        
        for result in results:
            status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
            logger.info(f"   {result['batch_size']:3d}    |   {result['time']:6.2f}   |      {result['speed']:6.1f}       | {status}")
        
        # æ¨èæœ€ä½³é…ç½®
        successful_results = [r for r in results if r["success"]]
        if successful_results:
            best_result = max(successful_results, key=lambda x: x["speed"])
            logger.info(f"ğŸ¯ å½“å‰é…ç½®ä¸‹æœ€ä½³æ‰¹æ¬¡å¤§å°: {best_result['batch_size']} (é€Ÿåº¦: {best_result['speed']:.1f} å‘é‡/ç§’)")
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹æ¬¡å¤§å°æµ‹è¯•å¤±è´¥: {str(e)}")

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    logger.info("ğŸ” Pineconeæ€§èƒ½è¯Šæ–­å·¥å…·")
    logger.info("=" * 60)
    
    # è¯Šæ–­1: ç½‘ç»œæ¡ä»¶
    check_network_conditions()
    
    logger.info("-" * 40)
    
    # è¯Šæ–­2: å¥—é¤åˆ†æ
    analyze_pinecone_plan_limits()
    
    logger.info("-" * 40)
    
    # è¯Šæ–­3: æ‰¹æ¬¡ä¼˜åŒ–æµ‹è¯•
    test_optimal_batch_sizes()
    
    logger.info("=" * 60)
    logger.info("ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®æ€»ç»“:")
    logger.info("1. æ£€æŸ¥å¹¶ç¦ç”¨ç½‘ç»œä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰")
    logger.info("2. ç¡®è®¤Pineconeå¥—é¤ç­‰çº§å’Œé™åˆ¶")
    logger.info("3. æ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´æ‰¹æ¬¡å¤§å°")
    logger.info("4. è€ƒè™‘å¢åŠ åˆ†å—å¤§å°åˆ°4000å­—ç¬¦")

if __name__ == "__main__":
    main()
