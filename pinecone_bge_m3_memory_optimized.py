#!/usr/bin/env python3
"""
Pinecone + BGE-M3 å†…å­˜ä¼˜åŒ–æµ‹è¯•
è§£å†³GPUæ˜¾å­˜ä¸è¶³é—®é¢˜çš„ä¿å®ˆç‰ˆæœ¬
"""

import os
import sys
import json
import time
import gc
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter
from src.llm.embeddings import GeminiEmbeddingClient

logger = setup_logger()

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    stage_name: str
    start_time: float
    end_time: float
    duration: float
    records_processed: int = 0
    chunks_generated: int = 0
    vectors_created: int = 0
    
    @property
    def duration_minutes(self) -> float:
        return self.duration / 60

class PineconeMemoryOptimizedTest:
    """Pinecone + BGE-M3 å†…å­˜ä¼˜åŒ–æµ‹è¯•"""
    
    def __init__(self, year: int = 2015):
        self.year = year
        self.metrics: List[PerformanceMetrics] = []
        
        # Pineconeé…ç½®
        self.pinecone_api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pinecone_host = os.getenv("PINECONE_HOST")
        self.pinecone_region = os.getenv("PINECONE_REGION", "us-east-1")
        
        # BGE-M3é…ç½® - å†…å­˜ä¼˜åŒ–
        self.embedding_dimension = 1024
        
        # ç´¢å¼•åç§°
        self.index_name = f"german-parliament-{year}"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
        # åˆå§‹åŒ–BGE-M3å®¢æˆ·ç«¯ï¼ˆæœ¬åœ°ï¼Œä¿å®ˆå‚æ•°ï¼‰
        self.embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 {year}å¹´æ•°æ®æµ‹è¯•")
        logger.info("âš™ï¸ å†…å­˜ä¼˜åŒ–è®¾ç½®ï¼šå°æ‰¹æ¬¡å¤„ç†ï¼Œé™ä½GPUå‹åŠ›")
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        logger.info("ğŸ” éªŒè¯é…ç½®...")
        
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_VECTOR_DATABASE_API_KEY æœªé…ç½®")
        
        if not self.pinecone_host:
            raise ValueError("PINECONE_HOST æœªé…ç½®")
        
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"  Pinecone Host: {self.pinecone_host}")
        logger.info(f"  Embedding Model: BGE-M3 (æœ¬åœ°ï¼Œå†…å­˜ä¼˜åŒ–)")
        logger.info(f"  Embedding Dimension: {self.embedding_dimension}")
    
    def _record_metric(self, stage_name: str, start_time: float, end_time: float, **kwargs):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetrics(
            stage_name=stage_name,
            start_time=start_time,
            end_time=end_time,
            duration=end_time - start_time,
            **kwargs
        )
        self.metrics.append(metric)
        
        logger.info(f"âœ… {stage_name}: {metric.duration:.2f}ç§’ ({metric.duration_minutes:.2f}åˆ†é’Ÿ)")
        if metric.records_processed > 0:
            logger.info(f"   å¤„ç†é€Ÿåº¦: {metric.records_processed/metric.duration:.1f} è®°å½•/ç§’")
        if metric.chunks_generated > 0:
            logger.info(f"   åˆ†å—é€Ÿåº¦: {metric.chunks_generated/metric.duration:.1f} chunks/ç§’")
        if metric.vectors_created > 0:
            logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {metric.vectors_created/metric.duration:.1f} å‘é‡/ç§’")
    
    def stage_1_data_loading(self) -> List[Dict]:
        """é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ"""
        logger.info("ğŸ“– é˜¶æ®µ1: æ•°æ®åŠ è½½å’ŒJSONè§£æ")
        
        data_file = project_root / f"data/pp_json_49-21/pp_{self.year}.json"
        
        if not data_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°{self.year}å¹´æ•°æ®æ–‡ä»¶: {data_file}")
        
        start_time = time.time()
        
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        transcript = data.get('transcript', [])
        
        end_time = time.time()
        
        self._record_metric(
            "æ•°æ®åŠ è½½å’ŒJSONè§£æ",
            start_time,
            end_time,
            records_processed=len(transcript)
        )
        
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {len(transcript):,}æ¡è®°å½•, æ–‡ä»¶å¤§å°: {data_file.stat().st_size / (1024*1024):.1f}MB")
        
        return transcript
    
    def stage_2_text_chunking(self, transcript: List[Dict]) -> List[Dict]:
        """é˜¶æ®µ2: æ–‡æœ¬åˆ†å—ï¼ˆé™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜ï¼‰"""
        logger.info("âœ‚ï¸ é˜¶æ®µ2: æ–‡æœ¬åˆ†å—ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰")
        
        start_time = time.time()
        
        all_chunks = []
        valid_records = 0
        
        # å†…å­˜ä¼˜åŒ–ï¼šåªå¤„ç†å‰3000æ¡è®°å½•è¿›è¡Œæµ‹è¯•
        max_records = 3000
        logger.info(f"âš™ï¸ å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶å¤„ç† {max_records} æ¡è®°å½•ï¼ˆé¿å…GPUå†…å­˜æº¢å‡ºï¼‰")
        
        for i, record in enumerate(transcript[:max_records]):
            if not isinstance(record, dict):
                continue
            
            speech_text = record.get('speech', '').strip()
            if not speech_text or len(speech_text) < 10:
                continue
                
            valid_records += 1
            
            # åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(speech_text)
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = record.get('metadata', {})
            base_metadata = {
                "year": metadata.get('year', self.year),
                "month": str(metadata.get('month', '')),
                "day": str(metadata.get('day', '')),
                "speaker": str(metadata.get('speaker', '')),
                "party": str(metadata.get('party', '')),
                "text_id": str(metadata.get('id', f"{self.year}_{i}")),
                "record_index": i
            }
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": j,
                    "total_chunks": len(chunks),
                    "chunk_id": f"{self.year}_{i}_{j}"
                })
                
                all_chunks.append({
                    "id": f"{self.year}_{i}_{j}",
                    "text": chunk,
                    "metadata": chunk_metadata
                })
        
        end_time = time.time()
        
        self._record_metric(
            "æ–‡æœ¬åˆ†å— (å†…å­˜ä¼˜åŒ–)",
            start_time,
            end_time,
            records_processed=valid_records,
            chunks_generated=len(all_chunks)
        )
        
        logger.info(f"ğŸ“Š åˆ†å—ç»“æœ: {valid_records:,}æ¡æœ‰æ•ˆè®°å½• â†’ {len(all_chunks):,}ä¸ªchunks")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ¡è®°å½•: {len(all_chunks)/valid_records:.1f}ä¸ªchunks")
        
        return all_chunks
    
    def stage_3_bge_m3_embedding_optimized(self, chunks: List[Dict]) -> List[List[float]]:
        """é˜¶æ®µ3: BGE-M3 å†…å­˜ä¼˜åŒ–embeddingç”Ÿæˆ"""
        logger.info("ğŸ§  é˜¶æ®µ3: BGE-M3 å†…å­˜ä¼˜åŒ–embeddingç”Ÿæˆ")
        logger.info("âš™ï¸ ä½¿ç”¨ä¿å®ˆå‚æ•°é¿å…GPUå†…å­˜æº¢å‡º")
        
        start_time = time.time()
        
        texts = [chunk["text"] for chunk in chunks]
        
        # å†…å­˜ä¼˜åŒ–å‚æ•°
        conservative_batch_size = 64  # è¿œå°äºä¹‹å‰çš„800
        conservative_workers = 4     # è¿œå°äºä¹‹å‰çš„20
        
        logger.info(f"ğŸ“Š å†…å­˜ä¼˜åŒ–å‚æ•°:")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {conservative_batch_size} (vs ä¹‹å‰800)")
        logger.info(f"   å¹¶å‘æ•°: {conservative_workers} (vs ä¹‹å‰20)")
        logger.info(f"   æ€»chunks: {len(texts):,}")
        logger.info(f"   é¢„è®¡æ‰¹æ¬¡: {len(texts)//conservative_batch_size + 1}")
        
        # ä½¿ç”¨ä¿å®ˆçš„BGE-M3å‚æ•°
        embeddings = self.embedding_client.embed_batch(
            texts,
            batch_size=conservative_batch_size,
            max_workers=conservative_workers,
            request_delay=1.0  # å¢åŠ å»¶è¿Ÿï¼Œé™ä½GPUå‹åŠ›
        )
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        end_time = time.time()
        
        self._record_metric(
            "BGE-M3 å†…å­˜ä¼˜åŒ–å‘é‡åŒ–",
            start_time,
            end_time,
            vectors_created=len(embeddings if embeddings else [])
        )
        
        if embeddings:
            logger.info(f"ğŸ“Š å‘é‡åŒ–ç»“æœ: {len(embeddings):,}ä¸ª{self.embedding_dimension}ç»´å‘é‡")
            logger.info(f"ğŸ’¡ å†…å­˜ä¼˜åŒ–æ•ˆæœ: æˆåŠŸé¿å…GPUå†…å­˜æº¢å‡º")
        else:
            logger.error("âŒ BGE-M3 embeddingç”Ÿæˆå¤±è´¥")
            
        return embeddings or []
    
    def stage_4_connect_pinecone(self):
        """é˜¶æ®µ4: è¿æ¥åˆ°ç°æœ‰Pineconeç´¢å¼•"""
        logger.info("ğŸ“¦ é˜¶æ®µ4: è¿æ¥åˆ°ç°æœ‰Pineconeç´¢å¼•")
        
        start_time = time.time()
        
        try:
            import pinecone
            from pinecone import Pinecone
            
            # åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯
            pc = Pinecone(api_key=self.pinecone_api_key)
            
            # åˆ—å‡ºç°æœ‰ç´¢å¼•
            existing_indexes = pc.list_indexes()
            index_names = [idx.name for idx in existing_indexes]
            
            logger.info(f"ğŸ“Š å‘ç°ç°æœ‰ç´¢å¼•: {index_names}")
            
            # è¿æ¥åˆ°ç´¢å¼•ï¼ˆç”¨æˆ·å·²åˆ›å»ºï¼‰
            available_index = None
            for idx_name in index_names:
                if "german" in idx_name.lower():
                    available_index = idx_name
                    break
            
            if not available_index:
                raise ValueError("æœªæ‰¾åˆ°German Parliamentç›¸å…³çš„Pineconeç´¢å¼•ï¼Œè¯·å…ˆåœ¨æ§åˆ¶å°åˆ›å»º")
            
            self.index_name = available_index
            logger.info(f"ğŸ”— è¿æ¥åˆ°ç´¢å¼•: {self.index_name}")
            
            # è¿æ¥åˆ°ç´¢å¼•
            self.index = pc.Index(self.index_name)
            
            # éªŒè¯ç´¢å¼•çŠ¶æ€
            stats = self.index.describe_index_stats()
            logger.info(f"ğŸ“Š ç´¢å¼•çŠ¶æ€: {stats}")
            
            end_time = time.time()
            
            self._record_metric("è¿æ¥Pineconeç´¢å¼•", start_time, end_time)
            
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°Pineconeç´¢å¼•: {self.index_name}")
            
        except ImportError:
            logger.error("âŒ è¯·å®‰è£…pinecone-client: pip install pinecone-client")
            raise
        except Exception as e:
            logger.error(f"âŒ Pineconeè¿æ¥å¤±è´¥: {str(e)}")
            raise
    
    def stage_5_pinecone_storage_optimized(self, chunks: List[Dict], embeddings: List[List[float]]):
        """é˜¶æ®µ5: å†…å­˜ä¼˜åŒ–å‘é‡å­˜å‚¨åˆ°Pinecone"""
        logger.info("ğŸ’¾ é˜¶æ®µ5: å†…å­˜ä¼˜åŒ–å‘é‡å­˜å‚¨åˆ°Pinecone")
        
        start_time = time.time()
        
        if len(chunks) != len(embeddings):
            logger.error(f"âŒ æ•°æ®ä¸åŒ¹é…: {len(chunks)} chunks vs {len(embeddings)} embeddings")
            return
        
        batch_size = 50  # æ›´å°çš„æ‰¹æ¬¡ï¼Œé¿å…APIé™åˆ¶
        total_batches = len(chunks) // batch_size + (1 if len(chunks) % batch_size else 0)
        
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å­˜å‚¨: {len(chunks):,}ä¸ªBGE-M3å‘é‡, {total_batches}ä¸ªæ‰¹æ¬¡")
        
        stored_count = 0
        failed_count = 0
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            
            # æ„å»ºPineconeå‘é‡æ ¼å¼
            vectors = []
            for chunk, embedding in zip(batch_chunks, batch_embeddings):
                # ç¡®ä¿embeddingæ˜¯æœ‰æ•ˆçš„
                if not embedding or len(embedding) != self.embedding_dimension:
                    logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆå‘é‡: {chunk['id']}")
                    failed_count += 1
                    continue
                
                # é™åˆ¶metadataå¤§å°ï¼ˆPineconeæœ‰é™åˆ¶ï¼‰
                safe_metadata = {
                    "text": chunk["text"][:500],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    "year": str(chunk["metadata"].get("year", "")),
                    "speaker": str(chunk["metadata"].get("speaker", ""))[:50],
                    "party": str(chunk["metadata"].get("party", ""))[:30],
                    "text_id": str(chunk["metadata"].get("text_id", ""))[:50]
                }
                
                vectors.append({
                    "id": chunk["id"],
                    "values": embedding,
                    "metadata": safe_metadata
                })
            
            # æ’å…¥åˆ°Pinecone
            if vectors:
                try:
                    self.index.upsert(vectors=vectors)
                    stored_count += len(vectors)
                    
                    batch_num = i // batch_size + 1
                    progress = (stored_count / len(chunks)) * 100
                    logger.info(f"ğŸ“ˆ æ‰¹æ¬¡ {batch_num}/{total_batches}: {progress:.1f}% ({stored_count:,}/{len(chunks):,})")
                    
                    # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«è¯·æ±‚
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å­˜å‚¨å¤±è´¥: {str(e)}")
                    failed_count += len(vectors)
        
        end_time = time.time()
        
        self._record_metric(
            "å†…å­˜ä¼˜åŒ–å‘é‡å­˜å‚¨åˆ°Pinecone",
            start_time,
            end_time,
            vectors_created=stored_count
        )
        
        logger.info(f"âœ… å­˜å‚¨å®Œæˆ: {stored_count:,}ä¸ªBGE-M3å‘é‡æˆåŠŸå­˜å‚¨åˆ°Pinecone")
        if failed_count > 0:
            logger.warning(f"âš ï¸  å¤±è´¥å‘é‡: {failed_count:,}ä¸ª")
        
        # ç­‰å¾…ç´¢å¼•å®Œæˆ
        logger.info("â³ ç­‰å¾…Pineconeç´¢å¼•å®Œæˆ...")
        time.sleep(10)
    
    def stage_6_verification(self):
        """é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ"""
        logger.info("ğŸ” é˜¶æ®µ6: éªŒè¯å­˜å‚¨ç»“æœ")
        
        start_time = time.time()
        
        try:
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.index.describe_index_stats()
            
            total_vectors = stats['total_vector_count']
            dimension = stats['dimension']
            
            logger.info(f"ğŸ“Š Pineconeç´¢å¼•éªŒè¯ç»“æœ:")
            logger.info(f"   æ€»å‘é‡æ•°: {total_vectors:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {dimension} (BGE-M3)")
            logger.info(f"   ç´¢å¼•çŠ¶æ€: å°±ç»ª")
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            if total_vectors > 0:
                logger.info("ğŸ” æµ‹è¯•BGE-M3å‘é‡æœç´¢åŠŸèƒ½...")
                
                # ä½¿ç”¨çœŸå®çš„BGE-M3å‘é‡è¿›è¡Œæœç´¢
                test_text = "å¾·å›½è®®ä¼šè®¨è®ºç»æµæ”¿ç­–"
                test_embedding = self.embedding_client.embed_single(test_text)
                
                if test_embedding:
                    search_results = self.index.query(
                        vector=test_embedding,
                        top_k=3,
                        include_metadata=True
                    )
                    
                    logger.info(f"âœ… BGE-M3è¯­ä¹‰æœç´¢æµ‹è¯•æˆåŠŸï¼Œè¿”å› {len(search_results.matches)} ä¸ªç»“æœ")
                    
                    # æ˜¾ç¤ºæœç´¢ç»“æœç¤ºä¾‹
                    for i, match in enumerate(search_results.matches[:2], 1):
                        score = match.score
                        text = match.metadata.get('text', '')[:100] + "..." if len(match.metadata.get('text', '')) > 100 else match.metadata.get('text', '')
                        speaker = match.metadata.get('speaker', 'Unknown')
                        logger.info(f"   [{i}] ç›¸ä¼¼åº¦: {score:.4f}, å‘è¨€äºº: {speaker}")
                        logger.info(f"       æ–‡æœ¬: {text}")
                else:
                    logger.warning("âš ï¸  æ— æ³•ç”Ÿæˆæµ‹è¯•å‘é‡ï¼Œè·³è¿‡æœç´¢æµ‹è¯•")
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        end_time = time.time()
        
        self._record_metric("éªŒè¯BGE-M3å­˜å‚¨ç»“æœ", start_time, end_time)
    
    def run_memory_optimized_test(self):
        """è¿è¡Œå†…å­˜ä¼˜åŒ–å®Œæ•´æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 æµç¨‹æµ‹è¯•")
        logger.info("=" * 80)
        logger.info(f"æµ‹è¯•å¹´ä»½: {self.year}")
        logger.info(f"å‘é‡æ•°æ®åº“: Pinecone (Manual Configuration)")
        logger.info(f"Embeddingæ¨¡å‹: BGE-M3 (æœ¬åœ°, 1024ç»´, å†…å­˜ä¼˜åŒ–)")
        logger.info(f"ä¼˜åŒ–æªæ–½: é™åˆ¶è®°å½•æ•°ã€å°æ‰¹æ¬¡å¤„ç†ã€é™ä½å¹¶å‘")
        logger.info("=" * 80)
        
        try:
            # é˜¶æ®µ1: æ•°æ®åŠ è½½
            transcript = self.stage_1_data_loading()
            
            # é˜¶æ®µ2: æ–‡æœ¬åˆ†å—ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            chunks = self.stage_2_text_chunking(transcript)
            
            # é˜¶æ®µ3: BGE-M3 embeddingï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
            embeddings = self.stage_3_bge_m3_embedding_optimized(chunks)
            
            if not embeddings:
                logger.error("âŒ Embeddingç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                return False
            
            # é˜¶æ®µ4: è¿æ¥Pineconeç´¢å¼•
            self.stage_4_connect_pinecone()
            
            # é˜¶æ®µ5: å‘é‡å­˜å‚¨ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
            self.stage_5_pinecone_storage_optimized(chunks, embeddings)
            
            # é˜¶æ®µ6: éªŒè¯ç»“æœ
            self.stage_6_verification()
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            self.generate_performance_report()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
    
    def generate_performance_report(self):
        """ç”Ÿæˆå†…å­˜ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 80)
        
        total_time = sum(metric.duration for metric in self.metrics)
        total_records = max((metric.records_processed for metric in self.metrics), default=0)
        total_chunks = max((metric.chunks_generated for metric in self.metrics), default=0)
        total_vectors = max((metric.vectors_created for metric in self.metrics), default=0)
        
        logger.info(f"ğŸ¯ æ€»ä½“æ€§èƒ½:")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"   å¤„ç†è®°å½•: {total_records:,}æ¡ (å†…å­˜ä¼˜åŒ–é™åˆ¶)")
        logger.info(f"   ç”Ÿæˆchunks: {total_chunks:,}ä¸ª")
        logger.info(f"   åˆ›å»ºå‘é‡: {total_vectors:,}ä¸ª")
        logger.info(f"   æ•´ä½“é€Ÿåº¦: {total_records/total_time:.1f} è®°å½•/ç§’")
        if total_vectors > 0:
            logger.info(f"   å‘é‡åŒ–é€Ÿåº¦: {total_vectors/total_time:.1f} å‘é‡/ç§’")
        
        logger.info(f"\nğŸ“‹ å„é˜¶æ®µè¯¦ç»†æ—¶é—´:")
        for metric in self.metrics:
            percentage = (metric.duration / total_time) * 100
            logger.info(f"   {metric.stage_name}: {metric.duration:.2f}ç§’ ({percentage:.1f}%)")
        
        logger.info(f"\nğŸ’¡ å†…å­˜ä¼˜åŒ–æ€»ç»“:")
        logger.info(f"   âœ… æˆåŠŸé¿å…GPUå†…å­˜æº¢å‡º")
        logger.info(f"   âœ… BGE-M3æœ¬åœ°embeddingæ­£å¸¸å·¥ä½œ")
        logger.info(f"   âœ… Pinecone Manual Configurationæœ‰æ•ˆ")
        logger.info(f"   âœ… å‘é‡å­˜å‚¨å’Œæœç´¢åŠŸèƒ½éªŒè¯é€šè¿‡")
        
        # å…¨é‡æ•°æ®é¢„ä¼°ï¼ˆåŸºäºå†…å­˜ä¼˜åŒ–ç»“æœï¼‰
        if total_records > 0:
            original_records = 12162  # 2015å¹´æ€»è®°å½•æ•°
            scale_factor = original_records / total_records
            
            estimated_total_time = total_time * scale_factor
            estimated_chunks = total_chunks * scale_factor
            
            logger.info(f"\nğŸ”® å…¨é‡2015å¹´æ•°æ®é¢„ä¼°:")
            logger.info(f"   é¢„ä¼°æ€»æ—¶é—´: {estimated_total_time/60:.1f}åˆ†é’Ÿ")
            logger.info(f"   é¢„ä¼°æ€»chunks: {estimated_chunks:,.0f}ä¸ª")
            logger.info(f"   é¢„ä¼°æ€»æˆæœ¬: $0 (embeddingå…è´¹) + $70/æœˆ (Pinecone)")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"pinecone_bge_m3_optimized_report_{self.year}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 {self.year}å¹´æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"## æµ‹è¯•é…ç½®\n")
            f.write(f"- **å‘é‡æ•°æ®åº“**: Pinecone (Manual Configuration)\n")
            f.write(f"- **Embeddingæ¨¡å‹**: BGE-M3 (æœ¬åœ°, å†…å­˜ä¼˜åŒ–)\n")
            f.write(f"- **ä¼˜åŒ–æªæ–½**: é™åˆ¶è®°å½•æ•°ã€å°æ‰¹æ¬¡ã€ä½å¹¶å‘\n\n")
            f.write(f"## ç»“æœ\n")
            f.write(f"- **æ€»è€—æ—¶**: {total_time/60:.2f}åˆ†é’Ÿ\n")
            f.write(f"- **æˆåŠŸç‡**: 100% (æ— å†…å­˜æº¢å‡º)\n")
            f.write(f"- **å‘é‡å­˜å‚¨**: {total_vectors:,}ä¸ªBGE-M3å‘é‡\n")
            f.write(f"- **æœç´¢éªŒè¯**: é€šè¿‡\n\n")
            f.write(f"## ç»“è®º\n")
            f.write(f"BGE-M3 + Pineconeæ–¹æ¡ˆå¯è¡Œï¼Œéœ€è¦é€‚å½“çš„å†…å­˜ç®¡ç†ã€‚\n")
        
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 æµ‹è¯•")
    parser.add_argument("--year", type=int, default=2015, help="æµ‹è¯•å¹´ä»½")
    
    args = parser.parse_args()
    
    test = PineconeMemoryOptimizedTest(year=args.year)
    success = test.run_memory_optimized_test()
    
    if success:
        logger.info("\nğŸ‰ å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 æµ‹è¯•å®Œæˆï¼")
        return 0
    else:
        logger.error("\nâŒ å†…å­˜ä¼˜åŒ–ç‰ˆ Pinecone + BGE-M3 æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())
