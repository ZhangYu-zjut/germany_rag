#!/usr/bin/env python3
"""
å¿«é€ŸéªŒè¯Pineconeæ•°æ®
åªåšåŸºæœ¬æ£€æŸ¥ï¼Œé¿å…å¤§é‡æŸ¥è¯¢å¯¼è‡´è¶…æ—¶
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import json

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent
sys.path.append(str(project_root))
load_dotenv(project_root / ".env", override=True)

from pinecone import Pinecone
from src.utils.logger import setup_logger

logger = setup_logger()


def quick_verify():
    """å¿«é€ŸéªŒè¯æ¯å¹´æ•°æ®"""
    logger.info("=" * 80)
    logger.info("ğŸ” å¿«é€ŸéªŒè¯Pineconeæ•°æ®")
    logger.info("=" * 80)

    # è¿æ¥Pinecone
    pc = Pinecone(api_key=os.getenv("PINECONE_VECTOR_DATABASE_API_KEY"))
    index = pc.Index("german-bge")

    # 1. æ€»ä½“ç»Ÿè®¡
    stats = index.describe_index_stats()
    total_vectors = stats.get("total_vector_count", 0)
    logger.info(f"\nğŸ“Š æ€»å‘é‡æ•°: {total_vectors:,}\n")

    # 2. æŒ‰å¹´ä»½å¿«é€Ÿæ£€æŸ¥ï¼ˆæ¯å¹´åªæŸ¥1ä¸ªæ ·æœ¬ï¼‰
    logger.info("ğŸ“… æŒ‰å¹´ä»½æ£€æŸ¥:")
    logger.info("-" * 80)

    results = {}

    for year in range(2015, 2026):
        year_str = str(year)

        try:
            # åªæŸ¥è¯¢1ä¸ªæ ·æœ¬æ¥éªŒè¯æ•°æ®å­˜åœ¨
            result = index.query(
                vector=[0.01] * 1024,
                top_k=1,
                filter={"year": {"$eq": year_str}},
                include_metadata=True
            )

            if result.matches:
                match = result.matches[0]
                metadata = match.metadata

                # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
                speaker = metadata.get("speaker", "")
                date = metadata.get("date", "")
                group = metadata.get("group", "")

                # æ ‡è®°ç¼ºå¤±å­—æ®µ
                missing = []
                if not speaker or speaker in ["æœªçŸ¥", "Unknown"]:
                    missing.append("speaker")
                if not date or date in["æœªçŸ¥", "Unknown"]:
                    missing.append("date")
                if not group or group in ["æœªçŸ¥", "Unknown"]:
                    missing.append("group")

                status = "âš ï¸" if missing else "âœ…"
                missing_str = f" (ç¼ºå¤±: {', '.join(missing)})" if missing else ""

                logger.info(f"  {status} {year_str}: æœ‰æ•°æ® - {speaker or 'æœªçŸ¥'}, {date or 'æœªçŸ¥'}{missing_str}")

                results[year_str] = {
                    "has_data": True,
                    "sample_speaker": speaker or "æœªçŸ¥",
                    "sample_date": date or "æœªçŸ¥",
                    "sample_group": group or "æœªçŸ¥",
                    "missing_fields": missing
                }
            else:
                logger.info(f"  âŒ {year_str}: æ— æ•°æ®")
                results[year_str] = {
                    "has_data": False
                }

        except Exception as e:
            logger.error(f"  âŒ {year_str}: æŸ¥è¯¢å¤±è´¥ - {str(e)}")
            results[year_str] = {
                "has_data": False,
                "error": str(e)
            }

    # 3. ä¸“é—¨æ£€æŸ¥2025å¹´
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ” 2025å¹´å…ƒæ•°æ®è¯¦ç»†æ£€æŸ¥")
    logger.info("=" * 80)

    try:
        result = index.query(
            vector=[0.01] * 1024,
            top_k=10,  # æŸ¥è¯¢10ä¸ªæ ·æœ¬
            filter={"year": {"$eq": "2025"}},
            include_metadata=True
        )

        if result.matches:
            logger.info(f"\nâœ… æ‰¾åˆ° {len(result.matches)} ä¸ª2025å¹´æ ·æœ¬\n")

            missing_speaker_count = 0
            missing_date_count = 0

            for i, match in enumerate(result.matches, 1):
                metadata = match.metadata
                speaker = metadata.get("speaker", "")
                date = metadata.get("date", "")
                group = metadata.get("group", "")
                text_preview = metadata.get("text", "")[:50]

                if not speaker or speaker in ["æœªçŸ¥", "Unknown"]:
                    missing_speaker_count += 1
                if not date or date in ["æœªçŸ¥", "Unknown"]:
                    missing_date_count += 1

                logger.info(f"æ ·æœ¬ {i}:")
                logger.info(f"  ID: {match.id}")
                logger.info(f"  speaker: {speaker or 'âŒ ç¼ºå¤±'}")
                logger.info(f"  date: {date or 'âŒ ç¼ºå¤±'}")
                logger.info(f"  group: {group or 'âŒ ç¼ºå¤±'}")
                logger.info(f"  text: {text_preview}...")
                logger.info("")

            logger.info(f"ç»Ÿè®¡:")
            logger.info(f"  ç¼ºå¤±speaker: {missing_speaker_count}/10")
            logger.info(f"  ç¼ºå¤±date: {missing_date_count}/10")

            results["2025_detail"] = {
                "sample_count": len(result.matches),
                "missing_speaker_ratio": f"{missing_speaker_count}/10",
                "missing_date_ratio": f"{missing_date_count}/10"
            }

        else:
            logger.error("âŒ 2025å¹´æ— æ•°æ®")

    except Exception as e:
        logger.error(f"âŒ 2025å¹´æŸ¥è¯¢å¤±è´¥: {str(e)}")

    # 4. æ£€æŸ¥åŸå§‹2025æ•°æ®æ–‡ä»¶
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ æ£€æŸ¥2025å¹´åŸå§‹æ•°æ®æ–‡ä»¶")
    logger.info("=" * 80)

    data_file = project_root / "data" / "pp_json_49-21" / "pp_2025.json"
    if data_file.exists():
        logger.info(f"\nâœ… æ‰¾åˆ°æ–‡ä»¶: {data_file}")

        try:
            with open(data_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            logger.info(f"åŸå§‹è®°å½•æ•°: {len(data)}")

            if data:
                logger.info("\nå‰3æ¡è®°å½•çš„å…ƒæ•°æ®:")
                for i, record in enumerate(data[:3], 1):
                    logger.info(f"\nè®°å½• {i}:")
                    logger.info(f"  speaker: {record.get('speaker', 'âŒ ç¼ºå¤±')}")
                    logger.info(f"  date: {record.get('date', 'âŒ ç¼ºå¤±')}")
                    logger.info(f"  group: {record.get('group', 'âŒ ç¼ºå¤±')}")
                    logger.info(f"  texté•¿åº¦: {len(record.get('text', ''))} å­—ç¬¦")

                # ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
                missing_speaker = sum(1 for r in data if not r.get('speaker'))
                missing_date = sum(1 for r in data if not r.get('date'))

                logger.info(f"\nåŸå§‹æ•°æ®ç¼ºå¤±ç»Ÿè®¡:")
                logger.info(f"  ç¼ºå¤±speaker: {missing_speaker}/{len(data)}")
                logger.info(f"  ç¼ºå¤±date: {missing_date}/{len(data)}")

                results["2025_source_file"] = {
                    "total_records": len(data),
                    "missing_speaker": missing_speaker,
                    "missing_date": missing_date
                }

        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    else:
        logger.error(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {data_file}")

    # 5. ä¿å­˜ç»“æœ
    output_file = project_root / "quick_verification_report.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "total_vectors": total_vectors,
            "year_check": results
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

    # 6. æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š éªŒè¯æ€»ç»“")
    logger.info("=" * 80)

    years_with_data = sum(1 for r in results.values() if isinstance(r, dict) and r.get("has_data"))
    years_with_issues = sum(1 for r in results.values() if isinstance(r, dict) and r.get("missing_fields"))

    logger.info(f"\næœ‰æ•°æ®çš„å¹´ä»½: {years_with_data}/11")
    if years_with_issues > 0:
        logger.warning(f"âš ï¸  æœ‰å…ƒæ•°æ®ç¼ºå¤±çš„å¹´ä»½: {years_with_issues}")
    else:
        logger.info(f"âœ… æ‰€æœ‰å¹´ä»½å…ƒæ•°æ®å®Œæ•´")


if __name__ == "__main__":
    try:
        quick_verify()
        logger.info("\nâœ… å¿«é€ŸéªŒè¯å®Œæˆ\n")
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)
