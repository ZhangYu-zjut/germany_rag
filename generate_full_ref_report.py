#!/usr/bin/env python3
"""
ç”Ÿæˆå®Œæ•´çš„å¼•ç”¨æ–‡æœ¬æŠ¥å‘Šï¼ˆ--full_refæ¨¡å¼ï¼‰

åŠŸèƒ½ï¼š
1. ä»stateä¸­æå–å®Œæ•´çš„æ£€ç´¢é“¾è·¯æ•°æ®
2. ç”Ÿæˆå¸¦æœ‰åŸå§‹æ–‡æœ¬ã€åˆ†æ•°çš„MarkdownæŠ¥å‘Š
3. ç”ŸæˆJSONæ ¼å¼çš„åŸå§‹æ•°æ®
4. ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå¬å›ç‡ã€ReRankæ•ˆæœï¼‰
"""

import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class FullRefReportGenerator:
    """å®Œæ•´å¼•ç”¨æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str = "outputs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_report(self, final_state: Dict, question_id: str = "Q1"):
        """
        ç”Ÿæˆå®Œæ•´æŠ¥å‘Š

        Args:
            final_state: LangGraphæœ€ç»ˆçŠ¶æ€
            question_id: é—®é¢˜IDï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = self.output_dir / f"{question_id}_{timestamp}"
        report_dir.mkdir(parents=True, exist_ok=True)

        print(f"[FullRefReport] ç”ŸæˆæŠ¥å‘Šåˆ°: {report_dir}")

        # 1. ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Š
        self._generate_markdown_report(final_state, report_dir, question_id)

        # 2. ç”ŸæˆåŸå§‹JSONæ•°æ®
        self._generate_raw_json(final_state, report_dir, question_id)

        # 3. ç”Ÿæˆç®€åŒ–ç‰ˆJSONï¼ˆåªä¿ç•™metadataå’Œåˆ†æ•°ï¼‰
        self._generate_summary_json(final_state, report_dir, question_id)

        # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis_dir = report_dir / "analysis"
        analysis_dir.mkdir(exist_ok=True)
        self._generate_retrieval_analysis(final_state, analysis_dir)
        self._generate_rerank_analysis(final_state, analysis_dir)
        self._generate_citation_mapping(final_state, analysis_dir)

        print(f"[FullRefReport] âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print(f"[FullRefReport] ğŸ“ æŠ¥å‘Šç›®å½•: {report_dir}")

        return report_dir

    def _generate_markdown_report(self, state: Dict, report_dir: Path, qid: str):
        """ç”Ÿæˆäººç±»å¯è¯»çš„Markdownå®Œæ•´æŠ¥å‘Š"""

        md_content = []

        # æ ‡é¢˜
        md_content.append(f"# {qid} å®Œæ•´æ£€ç´¢é“¾è·¯æŠ¥å‘Š\n")
        md_content.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        md_content.append("---\n")

        # åŸå§‹é—®é¢˜
        md_content.append("## åŸå§‹é—®é¢˜\n")
        md_content.append(f"{state.get('question', 'N/A')}\n")

        # æå–çš„å‚æ•°
        md_content.append("## æå–çš„å‚æ•°\n")
        parameters = state.get('parameters', {})
        md_content.append(f"```json\n{json.dumps(parameters, ensure_ascii=False, indent=2)}\n```\n")

        # å­é—®é¢˜åˆ—è¡¨
        sub_questions = state.get('sub_questions', [])
        if sub_questions:
            md_content.append(f"## å­é—®é¢˜åˆ—è¡¨ ({len(sub_questions)}ä¸ª)\n")
            for i, sq in enumerate(sub_questions, 1):
                if isinstance(sq, dict):
                    q_text = sq.get('question', sq)
                    target_year = sq.get('target_year')
                    strategy = sq.get('retrieval_strategy')
                    md_content.append(f"{i}. **{q_text}**\n")
                    if target_year:
                        md_content.append(f"   - ç›®æ ‡å¹´ä»½: `{target_year}`\n")
                    if strategy:
                        md_content.append(f"   - æ£€ç´¢ç­–ç•¥: `{strategy}`\n")
                else:
                    md_content.append(f"{i}. {sq}\n")
            md_content.append("\n")

        # æ£€ç´¢å’ŒReRankç»“æœ
        md_content.append("## æ£€ç´¢å’ŒReRankè¯¦æƒ…\n")

        retrieval_results = state.get('retrieval_results', [])
        reranked_results = state.get('reranked_results', [])

        for i, (retrieve_item, rerank_item) in enumerate(zip(retrieval_results, reranked_results), 1):
            question = retrieve_item.get('question', f'å­é—®é¢˜{i}')

            md_content.append(f"### å­é—®é¢˜ {i}: {question}\n")

            # Retrieveé˜¶æ®µ
            retrieve_chunks = retrieve_item.get('chunks', [])
            year_dist = retrieve_item.get('year_distribution', {})
            method = retrieve_item.get('retrieval_method', 'N/A')

            md_content.append(f"#### ğŸ“¥ Retrieveé˜¶æ®µ\n")
            md_content.append(f"- **æ£€ç´¢æ–¹æ³•**: `{method}`\n")
            md_content.append(f"- **æ£€ç´¢æ–‡æ¡£æ•°**: {len(retrieve_chunks)}\n")
            md_content.append(f"- **å¹´ä»½åˆ†å¸ƒ**: {year_dist}\n")

            if retrieve_chunks:
                md_content.append(f"\n**æ£€ç´¢åˆ°çš„æ–‡æ¡£ (Top 10)**:\n")
                for j, chunk in enumerate(retrieve_chunks[:10], 1):
                    score = chunk.get('score', 0.0)
                    metadata = chunk.get('metadata', {})
                    year = metadata.get('year', 'N/A')
                    speaker = metadata.get('speaker', 'N/A')
                    party = metadata.get('party', 'N/A')
                    date = metadata.get('date', 'N/A')

                    md_content.append(f"\n{j}. **{speaker} ({party}), {date}** | ç›¸ä¼¼åº¦: `{score:.4f}`\n")

                    # æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬å†…å®¹
                    text = chunk.get('text', '')
                    md_content.append(f"   > {text}\n")

            md_content.append("\n")

            # ReRanké˜¶æ®µ
            rerank_chunks = rerank_item.get('chunks', [])
            original_count = rerank_item.get('original_count', len(retrieve_chunks))

            md_content.append(f"#### ğŸ¯ ReRanké˜¶æ®µ\n")
            md_content.append(f"- **è¾“å…¥æ–‡æ¡£æ•°**: {original_count}\n")
            md_content.append(f"- **ä¿ç•™æ–‡æ¡£æ•°**: {len(rerank_chunks)}\n")
            md_content.append(f"- **ç²¾ç®€æ¯”ä¾‹**: {len(rerank_chunks)/original_count*100:.1f}%\n")

            if rerank_chunks:
                md_content.append(f"\n**ReRankåçš„Top 10æ–‡æ¡£**:\n")
                for j, chunk in enumerate(rerank_chunks[:10], 1):
                    retrieval_score = chunk.get('score', chunk.get('retrieval_score', 0.0))
                    rerank_score = chunk.get('rerank_score', 0.0)
                    metadata = chunk.get('metadata', {})
                    speaker = metadata.get('speaker', 'N/A')
                    party = metadata.get('party', 'N/A')
                    date = metadata.get('date', 'N/A')

                    md_content.append(
                        f"\n{j}. **{speaker} ({party}), {date}** | "
                        f"æ£€ç´¢: `{retrieval_score:.4f}` â†’ ReRank: `{rerank_score:.4f}`\n"
                    )

                    # å®Œæ•´æ–‡æœ¬ï¼ˆç”¨äºéªŒè¯å¼•ç”¨ï¼‰
                    text = chunk.get('text', '')
                    md_content.append(f"\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹å®Œæ•´æ–‡æœ¬</summary>\n\n{text}\n\n</details>\n")

            md_content.append("\n---\n")

        # æœ€ç»ˆç­”æ¡ˆ
        md_content.append("## æœ€ç»ˆç­”æ¡ˆ\n")
        final_answer = state.get('final_answer', 'N/A')
        md_content.append(f"{final_answer}\n")

        # Quellenå¼•ç”¨
        md_content.append("## Quellenå¼•ç”¨æ˜ å°„\n")
        md_content.append("ä»¥ä¸‹æ˜¯ç­”æ¡ˆä¸­å¼•ç”¨çš„Quellenä¸å®é™…æ–‡æœ¬å—çš„å¯¹åº”å…³ç³»ï¼š\n\n")

        # ä»ç­”æ¡ˆä¸­æå–Quellen
        quellen = self._extract_quellen_from_answer(final_answer)
        md_content.append(f"**å…±æ‰¾åˆ° {len(quellen)} ä¸ªå¼•ç”¨**\n\n")

        for i, q in enumerate(quellen, 1):
            md_content.append(f"{i}. `{q['citation']}`\n")

            # å°è¯•åŒ¹é…åˆ°reranked_resultsä¸­çš„chunk
            matched_chunks = self._match_citation_to_chunks(q, reranked_results)

            if matched_chunks:
                md_content.append(f"   **åŒ¹é…åˆ° {len(matched_chunks)} ä¸ªæ–‡æœ¬å—**:\n")
                for j, chunk in enumerate(matched_chunks, 1):
                    text = chunk.get('text', '')
                    # æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬å†…å®¹
                    md_content.append(f"   {j}) {text}\n\n")
            else:
                md_content.append(f"   âš ï¸ **æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬å—**\n")

            md_content.append("\n")

        # å†™å…¥æ–‡ä»¶
        md_file = report_dir / f"{qid}_full_report.md"
        md_file.write_text("".join(md_content), encoding='utf-8')
        print(f"[FullRefReport] âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_file}")

    def _generate_raw_json(self, state: Dict, report_dir: Path, qid: str):
        """ç”Ÿæˆå®Œæ•´åŸå§‹JSONæ•°æ®"""

        raw_data = {
            "question": state.get('question'),
            "parameters": state.get('parameters'),
            "sub_questions": state.get('sub_questions'),
            "retrieval_results": state.get('retrieval_results'),
            "reranked_results": state.get('reranked_results'),
            "final_answer": state.get('final_answer'),
            "overall_year_distribution": state.get('overall_year_distribution'),
            "intent": state.get('intent'),
            "question_type": state.get('question_type'),
        }

        json_file = report_dir / f"{qid}_raw_data.json"
        json_file.write_text(
            json.dumps(raw_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
        print(f"[FullRefReport] âœ… åŸå§‹JSONå·²ç”Ÿæˆ: {json_file}")

    def _generate_summary_json(self, state: Dict, report_dir: Path, qid: str):
        """ç”Ÿæˆç®€åŒ–ç‰ˆJSONï¼ˆåªä¿ç•™metadataå’Œåˆ†æ•°ï¼‰"""

        summary_data = {
            "question": state.get('question'),
            "parameters": state.get('parameters'),
            "sub_questions_summary": [],
            "overall_stats": {
                "total_retrieved": 0,
                "total_reranked": 0,
                "year_distribution": state.get('overall_year_distribution', {})
            }
        }

        retrieval_results = state.get('retrieval_results', [])
        reranked_results = state.get('reranked_results', [])

        for retrieve_item, rerank_item in zip(retrieval_results, reranked_results):
            question = retrieve_item.get('question')

            # ç»Ÿè®¡
            retrieve_count = len(retrieve_item.get('chunks', []))
            rerank_count = len(rerank_item.get('chunks', []))

            summary_data['overall_stats']['total_retrieved'] += retrieve_count
            summary_data['overall_stats']['total_reranked'] += rerank_count

            # ç®€åŒ–çš„å­é—®é¢˜æ•°æ®ï¼ˆåªä¿ç•™metadataå’Œåˆ†æ•°ï¼‰
            sub_summary = {
                "question": question,
                "retrieval_method": retrieve_item.get('retrieval_method'),
                "year_distribution": retrieve_item.get('year_distribution'),
                "retrieve_count": retrieve_count,
                "rerank_count": rerank_count,
                "chunks_metadata": []
            }

            # åªä¿ç•™metadataå’Œåˆ†æ•°
            for chunk in rerank_item.get('chunks', []):
                chunk_meta = {
                    "metadata": chunk.get('metadata'),
                    "retrieval_score": chunk.get('score', chunk.get('retrieval_score')),
                    "rerank_score": chunk.get('rerank_score'),
                    "text_length": len(chunk.get('text', ''))
                }
                sub_summary['chunks_metadata'].append(chunk_meta)

            summary_data['sub_questions_summary'].append(sub_summary)

        json_file = report_dir / f"{qid}_summary.json"
        json_file.write_text(
            json.dumps(summary_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
        print(f"[FullRefReport] âœ… ç®€åŒ–JSONå·²ç”Ÿæˆ: {json_file}")

    def _generate_retrieval_analysis(self, state: Dict, analysis_dir: Path):
        """ç”Ÿæˆæ£€ç´¢è´¨é‡åˆ†ææŠ¥å‘Š"""

        md_content = []
        md_content.append("# æ£€ç´¢è´¨é‡åˆ†æ\n\n")

        retrieval_results = state.get('retrieval_results', [])

        # ç»Ÿè®¡ä¿¡æ¯
        total_retrieved = sum(len(r.get('chunks', [])) for r in retrieval_results)
        avg_per_question = total_retrieved / len(retrieval_results) if retrieval_results else 0

        md_content.append("## æ•´ä½“ç»Ÿè®¡\n")
        md_content.append(f"- å­é—®é¢˜æ•°: {len(retrieval_results)}\n")
        md_content.append(f"- æ€»æ£€ç´¢æ–‡æ¡£æ•°: {total_retrieved}\n")
        md_content.append(f"- å¹³å‡æ¯é¢˜æ–‡æ¡£æ•°: {avg_per_question:.1f}\n\n")

        # å¹´ä»½åˆ†å¸ƒ
        md_content.append("## å¹´ä»½åˆ†å¸ƒ\n")
        year_dist = state.get('overall_year_distribution', {})
        for year, count in sorted(year_dist.items()):
            md_content.append(f"- {year}: {count} ä¸ªæ–‡æ¡£\n")

        md_content.append("\n")

        # ç›¸ä¼¼åº¦åˆ†å¸ƒ
        md_content.append("## ç›¸ä¼¼åº¦åˆ†æ•°åˆ†å¸ƒ\n")
        all_scores = []
        for r in retrieval_results:
            for chunk in r.get('chunks', []):
                score = chunk.get('score', 0.0)
                all_scores.append(score)

        if all_scores:
            md_content.append(f"- æœ€é«˜åˆ†: {max(all_scores):.4f}\n")
            md_content.append(f"- æœ€ä½åˆ†: {min(all_scores):.4f}\n")
            md_content.append(f"- å¹³å‡åˆ†: {sum(all_scores)/len(all_scores):.4f}\n")

            # åˆ†æ•°åŒºé—´åˆ†å¸ƒ
            bins = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.0]
            md_content.append("\n**åˆ†æ•°åŒºé—´åˆ†å¸ƒ**:\n")
            for i in range(len(bins) - 1):
                count = sum(1 for s in all_scores if bins[i] >= s > bins[i+1])
                percentage = count / len(all_scores) * 100
                md_content.append(f"- {bins[i]:.1f} ~ {bins[i+1]:.1f}: {count} ({percentage:.1f}%)\n")

        # å†™å…¥æ–‡ä»¶
        md_file = analysis_dir / "retrieval_analysis.md"
        md_file.write_text("".join(md_content), encoding='utf-8')
        print(f"[FullRefReport] âœ… æ£€ç´¢åˆ†æå·²ç”Ÿæˆ: {md_file}")

    def _generate_rerank_analysis(self, state: Dict, analysis_dir: Path):
        """ç”ŸæˆReRankæ•ˆæœåˆ†ææŠ¥å‘Š"""

        md_content = []
        md_content.append("# ReRankæ•ˆæœåˆ†æ\n\n")

        reranked_results = state.get('reranked_results', [])

        # ç»Ÿè®¡ä¿¡æ¯
        total_before = sum(r.get('original_count', 0) for r in reranked_results)
        total_after = sum(len(r.get('chunks', [])) for r in reranked_results)

        md_content.append("## æ•´ä½“ç»Ÿè®¡\n")
        md_content.append(f"- ReRankå‰æ–‡æ¡£æ•°: {total_before}\n")
        md_content.append(f"- ReRankåæ–‡æ¡£æ•°: {total_after}\n")
        md_content.append(f"- ç²¾ç®€æ¯”ä¾‹: {total_after/total_before*100:.1f}%\n\n")

        # ReRankåˆ†æ•°åˆ†å¸ƒ
        md_content.append("## ReRankåˆ†æ•°åˆ†å¸ƒ\n")
        all_rerank_scores = []
        for r in reranked_results:
            for chunk in r.get('chunks', []):
                score = chunk.get('rerank_score', 0.0)
                if score > 0:
                    all_rerank_scores.append(score)

        if all_rerank_scores:
            md_content.append(f"- æœ€é«˜åˆ†: {max(all_rerank_scores):.4f}\n")
            md_content.append(f"- æœ€ä½åˆ†: {min(all_rerank_scores):.4f}\n")
            md_content.append(f"- å¹³å‡åˆ†: {sum(all_rerank_scores)/len(all_rerank_scores):.4f}\n\n")

        # æ’åå˜åŒ–åˆ†æ
        md_content.append("## æ’åå˜åŒ–åˆ†æ\n")
        md_content.append("TODO: åˆ†ææ£€ç´¢æ’åvs ReRankæ’åçš„å˜åŒ–\n\n")

        # å†™å…¥æ–‡ä»¶
        md_file = analysis_dir / "rerank_analysis.md"
        md_file.write_text("".join(md_content), encoding='utf-8')
        print(f"[FullRefReport] âœ… ReRankåˆ†æå·²ç”Ÿæˆ: {md_file}")

    def _generate_citation_mapping(self, state: Dict, analysis_dir: Path):
        """ç”ŸæˆQuellenå¼•ç”¨æ˜ å°„è¡¨"""

        md_content = []
        md_content.append("# Quellenå¼•ç”¨æ˜ å°„è¡¨\n\n")

        final_answer = state.get('final_answer', '')
        quellen = self._extract_quellen_from_answer(final_answer)
        reranked_results = state.get('reranked_results', [])

        md_content.append(f"## å¼•ç”¨ç»Ÿè®¡\n")
        md_content.append(f"- æ€»å¼•ç”¨æ•°: {len(quellen)}\n\n")

        md_content.append("## å¼•ç”¨è¯¦æƒ…\n\n")

        for i, q in enumerate(quellen, 1):
            citation = q['citation']
            speaker = q.get('speaker', 'N/A')
            date = q.get('date', 'N/A')

            md_content.append(f"### {i}. {citation}\n")
            md_content.append(f"- å‘è¨€äºº: {speaker}\n")
            md_content.append(f"- æ—¥æœŸ: {date}\n")

            # åŒ¹é…åˆ°çš„chunks
            matched_chunks = self._match_citation_to_chunks(q, reranked_results)
            md_content.append(f"- åŒ¹é…æ–‡æœ¬å—æ•°: {len(matched_chunks)}\n\n")

            if matched_chunks:
                md_content.append("**åŒ¹é…çš„æ–‡æœ¬å—**:\n\n")
                for j, chunk in enumerate(matched_chunks, 1):
                    metadata = chunk.get('metadata', {})
                    text = chunk.get('text', '')

                    md_content.append(f"#### åŒ¹é… {j}\n")
                    md_content.append(f"- å¹´ä»½: {metadata.get('year', 'N/A')}\n")
                    md_content.append(f"- å…šæ´¾: {metadata.get('party', 'N/A')}\n")
                    md_content.append(f"- ä¼šè®®: {metadata.get('session', 'N/A')}\n")
                    md_content.append(f"- ReRankåˆ†æ•°: {chunk.get('rerank_score', 0.0):.4f}\n\n")
                    md_content.append(f"**æ–‡æœ¬å†…å®¹**:\n\n{text}\n\n")
                    md_content.append("---\n\n")
            else:
                md_content.append("âš ï¸ **æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬å—**\n\n")

        # å†™å…¥æ–‡ä»¶
        md_file = analysis_dir / "citation_mapping.md"
        md_file.write_text("".join(md_content), encoding='utf-8')
        print(f"[FullRefReport] âœ… å¼•ç”¨æ˜ å°„å·²ç”Ÿæˆ: {md_file}")

    def _extract_quellen_from_answer(self, answer: str) -> List[Dict]:
        """
        ä»ç­”æ¡ˆä¸­æå–Quellenå¼•ç”¨ï¼ˆé²æ£’æ–¹æ¡ˆï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œfallbackï¼‰

        æ”¯æŒçš„æ ¼å¼ï¼š
        1. "*   Name (Party), Date"
        2. "- Material X: Name (Party), Date"
        3. "- Name (Party), Date"
        4. "*   Redner: Name (Party), Date" (åµŒå¥—æ ¼å¼ï¼Œå…è®¸ç¼©è¿›)
        5. "- Material X: Redner: Name (Party), Date" (æ··åˆæ ¼å¼)
        6. "*   Material X: Name (Party), Date" (æ˜Ÿå· + Material)

        Fallbackæœºåˆ¶ï¼š
        1. ä¼˜å…ˆåŒ¹é… **Quellen** section
        2. å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œåœ¨ç­”æ¡ˆç»“å°¾åŒ¹é…
        """
        quellen = []

        # å°è¯•1: å®šä½åˆ°Quellen section
        # ä¿®å¤ï¼šä½¿ç”¨è´ªå©ªåŒ¹é…åˆ°ç­”æ¡ˆç»“å°¾ï¼Œè€Œä¸æ˜¯é‡åˆ°\n\nå°±åœæ­¢
        quellen_match = re.search(r'\*\*Quellen\*\*(.*)', answer, re.DOTALL)

        if quellen_match:
            quellen_text = quellen_match.group(1)
        else:
            # Fallback 1: å¦‚æœæ²¡æœ‰**Quellen**æ ‡é¢˜ï¼Œå°è¯•åœ¨ç­”æ¡ˆç»“å°¾æå–
            # å–ç­”æ¡ˆæœ€å2000å­—ç¬¦ï¼ˆé€šå¸¸å¼•ç”¨åœ¨ç»“å°¾ï¼‰
            quellen_text = answer[-2000:]

        # æ”¯æŒ6ç§å¼•ç”¨æ ¼å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½å°è¯•åŒ¹é…
        # æ³¨æ„ï¼šSpeakeråå­—å¯èƒ½åŒ…å«æ‹¬å·ï¼Œå¦‚"Thomas Strobl (Heilbronn)"
        # å› æ­¤éœ€è¦ä»å³å¾€å·¦åŒ¹é…ï¼šæ—¥æœŸ -> æœ€åä¸€ä¸ªæ‹¬å·(Party) -> Speaker

        # Pattern 6: "*   Material X: Name (Party), Date" (æ˜Ÿå· + Materialï¼Œéœ€è¦æœ€é«˜ä¼˜å…ˆçº§)
        pattern6 = r'^\*\s+Material\s+\d+:\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches6 = re.findall(pattern6, quellen_text, re.MULTILINE)

        # Pattern 5: "- Material X: Redner: Name (Party), Date" (æ¨ªçº¿ + Material + Redner)
        pattern5 = r'^-\s+Material\s+\d+:\s+Redner:\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches5 = re.findall(pattern5, quellen_text, re.MULTILINE)

        # Pattern 4: "    *   Redner: Name (Party), Date" (åµŒå¥—æ ¼å¼ï¼Œå¸¦ç¼©è¿›å’ŒRednerå‰ç¼€)
        pattern4 = r'^\s*\*\s+Redner:\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches4 = re.findall(pattern4, quellen_text, re.MULTILINE)

        # Pattern 2: "- Material X: Name (Party), Date" (æ¨ªçº¿ + Material)
        pattern2 = r'^-\s+Material\s+\d+:\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches2 = re.findall(pattern2, quellen_text, re.MULTILINE)

        # Pattern 3: "- Name (Party), Date" (çº¯æ¨ªçº¿ï¼Œæ²¡æœ‰Materialå‰ç¼€)
        pattern3 = r'^-\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches3 = re.findall(pattern3, quellen_text, re.MULTILINE)

        # Pattern 1: "*   Name (Party), Date" (çº¯æ˜Ÿå·ï¼Œæ²¡æœ‰Material/Rednerå‰ç¼€)
        pattern1 = r'^\*\s+(.+)\s+\(([^)]+)\),\s*(\d{4}-\d{2}-\d{2})$'
        matches1 = re.findall(pattern1, quellen_text, re.MULTILINE)

        # åˆå¹¶æ‰€æœ‰åŒ¹é…ç»“æœï¼ˆä¼˜å…ˆçº§: Pattern6 > Pattern5 > Pattern4 > Pattern2 > Pattern3 > Pattern1ï¼‰
        # ä¼˜å…ˆé€‰æ‹©æœ€ç‰¹æ®Šçš„æ ¼å¼ï¼Œé¿å…è¯¯åŒ¹é…
        all_matches = matches6 if matches6 else (matches5 if matches5 else (matches4 if matches4 else (matches2 if matches2 else (matches3 if matches3 else matches1))))

        for match in all_matches:
            speaker = match[0].strip()
            party = match[1].strip()
            date = match[2].strip()

            quellen.append({
                "citation": f"{speaker} ({party}), {date}",
                "speaker": speaker,
                "party": party,
                "date": date
            })

        return quellen

    def _normalize_date(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD (è¡¥é›¶)"""
        if not date_str:
            return date_str

        try:
            from datetime import datetime
            # å°è¯•è§£æå¤šç§æ ¼å¼
            for fmt in ['%Y-%m-%d', '%Y-%m-%d']:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime('%Y-%m-%d')  # ç»Ÿä¸€ä¸ºè¡¥é›¶æ ¼å¼
                except ValueError:
                    continue

            # å¦‚æœæ— æ³•è§£æï¼Œå°è¯•æ‰‹åŠ¨è¡¥é›¶
            parts = date_str.strip().split('-')
            if len(parts) == 3:
                year, month, day = parts
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

            return date_str
        except Exception:
            return date_str

    def _match_citation_to_chunks(self, citation: Dict, reranked_results: List) -> List[Dict]:
        """å°†å¼•ç”¨åŒ¹é…åˆ°å…·ä½“çš„æ–‡æœ¬å—"""
        matched_chunks = []

        citation_speaker = citation.get('speaker', '').strip().lower()
        citation_date = self._normalize_date(citation.get('date', ''))

        # åœ¨æ‰€æœ‰reranked_resultsä¸­æŸ¥æ‰¾åŒ¹é…
        for result in reranked_results:
            for chunk in result.get('chunks', []):
                metadata = chunk.get('metadata', {})

                chunk_speaker = metadata.get('speaker', '').strip().lower()
                chunk_date = self._normalize_date(metadata.get('date', ''))

                # åŒ¹é…: speakeråå­—åŒ…å« + æ—¥æœŸæ ‡å‡†åŒ–åç›¸ç­‰
                if citation_speaker in chunk_speaker and citation_date == chunk_date:
                    matched_chunks.append(chunk)

        return matched_chunks


def main():
    """ä¸»å‡½æ•°ï¼šä»test_langgraph_complete.pyçš„ç»“æœç”ŸæˆæŠ¥å‘Š"""
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python generate_full_ref_report.py <final_state.json>")
        print("æˆ–è€…: åœ¨test_langgraph_complete.pyä¸­è°ƒç”¨generate_report()")
        return

    # è¯»å–final_state
    json_file = sys.argv[1]
    with open(json_file, 'r', encoding='utf-8') as f:
        final_state = json.load(f)

    # ç”ŸæˆæŠ¥å‘Š
    generator = FullRefReportGenerator(output_dir="outputs")
    report_dir = generator.generate_report(final_state, question_id="Q1")

    print(f"\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {report_dir}")
    print(f"\nè¯·æŸ¥çœ‹:")
    print(f"  - Q1_full_report.md: å®Œæ•´çš„äººç±»å¯è¯»æŠ¥å‘Š")
    print(f"  - Q1_raw_data.json: åŸå§‹æ•°æ®")
    print(f"  - Q1_summary.json: ç®€åŒ–ç‰ˆæ•°æ®")
    print(f"  - analysis/: åˆ†ææŠ¥å‘Š")


if __name__ == "__main__":
    main()
