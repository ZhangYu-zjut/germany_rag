"""
å¾·å›½è®®ä¼šRAGç³»ç»Ÿ - Streamlitäº¤äº’ç•Œé¢ (Pineconeç‰ˆæœ¬)
æ”¯æŒæµå¼è¾“å‡ºã€å®æ—¶è¿›åº¦æ˜¾ç¤ºã€å®Œæ•´workflowæµ‹è¯•
"""

import streamlit as st
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¾·å›½è®®ä¼šRAGç³»ç»Ÿ",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stProgress > div > div > div > div {
        background-color: #1f77b4;
    }
    .status-box {
        padding: 10px;
        border-radius: 5px;
        margin: 5px 0;
    }
    .status-running {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    .status-done {
        background-color: #e8f5e9;
        border-left: 4px solid #4caf50;
    }
    .status-error {
        background-color: #ffebee;
        border-left: 4px solid #f44336;
    }
</style>
""", unsafe_allow_html=True)


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    issues = []

    # æ£€æŸ¥Pinecone APIå¯†é’¥
    pinecone_key = os.getenv('PINECONE_VECTOR_DATABASE_API_KEY')
    if not pinecone_key:
        issues.append("âŒ PINECONE_VECTOR_DATABASE_API_KEY æœªè®¾ç½®")
    else:
        st.sidebar.success("âœ… Pinecone API Key å·²é…ç½®")

    # æ£€æŸ¥LLM APIå¯†é’¥
    llm_key = os.getenv('GEMINI_API_KEY') or os.getenv('OPENAI_API_KEY')
    if not llm_key:
        issues.append("âŒ LLM API Key æœªè®¾ç½®")
    else:
        st.sidebar.success("âœ… LLM API Key å·²é…ç½®")

    # æ£€æŸ¥Cohere APIå¯†é’¥
    cohere_key = os.getenv('COHERE_API_KEY')
    if not cohere_key:
        issues.append("âš ï¸ COHERE_API_KEY æœªè®¾ç½® (ReRankåŠŸèƒ½å°†ä¸å¯ç”¨)")
    else:
        st.sidebar.success("âœ… Cohere API Key å·²é…ç½®")

    return issues


def create_progress_placeholder():
    """åˆ›å»ºè¿›åº¦æ˜¾ç¤ºå®¹å™¨"""
    return st.container()


def update_progress(container, stage: str, status: str, message: str = ""):
    """æ›´æ–°è¿›åº¦æ˜¾ç¤º

    Args:
        container: streamlitå®¹å™¨
        stage: é˜¶æ®µåç§°
        status: çŠ¶æ€ (running/done/error)
        message: é™„åŠ æ¶ˆæ¯
    """
    status_icon = {
        "running": "ğŸ”„",
        "done": "âœ…",
        "error": "âŒ"
    }

    status_class = {
        "running": "status-running",
        "done": "status-done",
        "error": "status-error"
    }

    icon = status_icon.get(status, "â¸ï¸")
    css_class = status_class.get(status, "")

    with container:
        st.markdown(
            f'<div class="status-box {css_class}">'
            f'{icon} <strong>{stage}</strong>'
            f'{f": {message}" if message else ""}'
            f'</div>',
            unsafe_allow_html=True
        )


def run_complete_workflow_with_progress(question: str, enable_rerank: bool = True):
    """è¿è¡Œå®Œæ•´workflowå¹¶æ˜¾ç¤ºè¿›åº¦"""

    # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºåŒºåŸŸ
    progress_container = st.empty()
    result_container = st.empty()

    stages = {
        "init": "åˆå§‹åŒ–ç³»ç»Ÿ",
        "intent": "æ„å›¾åˆ†æ",
        "classify": "é—®é¢˜åˆ†ç±»",
        "extract": "å‚æ•°æå–",
        "decompose": "é—®é¢˜åˆ†è§£",
        "retrieve": "æ–‡æ¡£æ£€ç´¢",
        "rerank": "æ–‡æ¡£é‡æ’",
        "summarize": "ç”Ÿæˆç­”æ¡ˆ"
    }

    try:
        # 1. åˆå§‹åŒ–
        with progress_container.container():
            update_progress(st, "åˆå§‹åŒ–ç³»ç»Ÿ", "running", "åŠ è½½æ¨¡å‹å’Œé…ç½®...")

        from src.llm.embeddings import GeminiEmbeddingClient
        from src.llm.client import GeminiLLMClient
        from pinecone import Pinecone
        import requests

        # åˆå§‹åŒ–ç»„ä»¶
        embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )

        api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        pc = Pinecone(api_key=api_key)
        index = pc.Index("german-bge")

        llm = GeminiLLMClient(temperature=0.0)

        cohere_api_key = os.getenv("COHERE_API_KEY")

        with progress_container.container():
            update_progress(st, "åˆå§‹åŒ–ç³»ç»Ÿ", "done", "âœ“ å®Œæˆ")

        # 2. å‚æ•°æå–
        with progress_container.container():
            update_progress(st, "å‚æ•°æå–", "running", "åˆ†æé—®é¢˜å…³é”®ä¿¡æ¯...")

        import re
        params = {}
        year_match = re.search(r'20\d{2}', question)
        if year_match:
            params['year'] = year_match.group()

        parties = []
        if 'CDU/CSU' in question or 'CDU' in question:
            parties.append('CDU/CSU')
        if 'SPD' in question:
            parties.append('SPD')
        if parties:
            params['parties'] = parties

        if 'éš¾æ°‘' in question or 'ç§»æ°‘' in question:
            params['topic'] = 'refugee'
        elif 'æ¬§ç›Ÿ' in question:
            params['topic'] = 'EU'

        with progress_container.container():
            update_progress(st, "å‚æ•°æå–", "done", f"âœ“ {params}")

        # 3. æ–‡æ¡£æ£€ç´¢
        with progress_container.container():
            update_progress(st, "æ–‡æ¡£æ£€ç´¢", "running", "ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£...")

        query_vector = embedding_client.embed_text(question)

        query_params = {
            "vector": query_vector,
            "top_k": 20,
            "include_metadata": True
        }

        filters = []
        if 'year' in params:
            filters.append({"year": {"$eq": params['year']}})
        if 'parties' in params and len(params['parties']) > 0:
            filters.append({"group": {"$in": params['parties']}})

        if filters:
            if len(filters) == 1:
                query_params["filter"] = filters[0]
            else:
                query_params["filter"] = {"$and": filters}

        results = index.query(**query_params)

        chunks = []
        for match in results.matches:
            chunk = {
                "id": match.id,
                "score": match.score,
                "text": match.metadata.get("text", ""),
                "metadata": match.metadata
            }
            chunks.append(chunk)

        with progress_container.container():
            update_progress(st, "æ–‡æ¡£æ£€ç´¢", "done", f"âœ“ æ£€ç´¢åˆ°{len(chunks)}ä¸ªæ–‡æ¡£")

        # 4. æ–‡æ¡£é‡æ’
        reranked_chunks = chunks
        if enable_rerank and cohere_api_key and len(chunks) > 0:
            with progress_container.container():
                update_progress(st, "æ–‡æ¡£é‡æ’", "running", "ä½¿ç”¨Cohere APIé‡æ–°æ’åº...")

            try:
                documents = [chunk['text'] for chunk in chunks]

                url = "https://api.cohere.com/v2/rerank"
                headers = {
                    "Authorization": f"Bearer {cohere_api_key}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "model": "rerank-v3.5",
                    "query": question,
                    "documents": documents,
                    "top_n": 10
                }

                response = requests.post(url, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                result = response.json()

                reranked_chunks = []
                for item in result.get("results", []):
                    index_num = item["index"]
                    relevance_score = item["relevance_score"]

                    reranked_chunk = chunks[index_num].copy()
                    reranked_chunk["rerank_score"] = relevance_score
                    reranked_chunks.append(reranked_chunk)

                with progress_container.container():
                    update_progress(st, "æ–‡æ¡£é‡æ’", "done", f"âœ“ é‡æ’åˆ°{len(reranked_chunks)}ä¸ªæœ€ç›¸å…³æ–‡æ¡£")

            except Exception as e:
                with progress_container.container():
                    update_progress(st, "æ–‡æ¡£é‡æ’", "error", f"å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {str(e)[:50]}")
                reranked_chunks = chunks[:10]
        else:
            reranked_chunks = chunks[:10]
            with progress_container.container():
                update_progress(st, "æ–‡æ¡£é‡æ’", "done", "âœ“ è·³è¿‡ï¼ˆæœªå¯ç”¨æˆ–æ— APIå¯†é’¥ï¼‰")

        # 5. ç”Ÿæˆç­”æ¡ˆ
        with progress_container.container():
            update_progress(st, "ç”Ÿæˆç­”æ¡ˆ", "running", "LLMæ­£åœ¨ç”Ÿæˆç­”æ¡ˆï¼Œè¯·ç¨å€™...")

        # æ„å»ºcontext
        context_parts = []
        for i, chunk in enumerate(reranked_chunks, 1):
            metadata = chunk.get('metadata', {})
            speaker = metadata.get('speaker', 'æœªçŸ¥')
            date = metadata.get('date', 'æœªçŸ¥')
            group = metadata.get('group', 'æœªçŸ¥')
            text = chunk.get('text', '')

            context_parts.append(
                f"[æ–‡æ¡£{i}] å‘è¨€äºº: {speaker}, å…šæ´¾: {group}, æ—¥æœŸ: {date}\n{text}"
            )

        context = "\n\n".join(context_parts)

        prompt = f"""è¯·åŸºäºä»¥ä¸‹å¾·å›½è®®ä¼šå‘è¨€è®°å½•å›ç­”é—®é¢˜ã€‚

ã€é—®é¢˜ã€‘
{question}

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºæä¾›çš„èµ„æ–™è¿›è¡Œæ€»ç»“å’Œåˆ†æ
2. å¦‚æœèµ„æ–™ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“å‘è¨€äººã€æ—¥æœŸå’Œå…šæ´¾
4. ä¿æŒå®¢è§‚å’Œå‡†ç¡®

è¯·å›ç­”ï¼š"""

        answer = llm.invoke(prompt)

        with progress_container.container():
            update_progress(st, "ç”Ÿæˆç­”æ¡ˆ", "done", f"âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼ˆ{len(answer)}å­—ç¬¦ï¼‰")

        return {
            "success": True,
            "answer": answer,
            "params": params,
            "chunks_retrieved": len(chunks),
            "chunks_reranked": len(reranked_chunks),
            "reranked_chunks": reranked_chunks
        }

    except Exception as e:
        with progress_container.container():
            update_progress(st, "ç³»ç»Ÿé”™è¯¯", "error", str(e))
        return {
            "success": False,
            "error": str(e)
        }


def main():
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ›ï¸ å¾·å›½è”é‚¦è®®é™¢æ¼”è®²æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.markdown("*åŸºäºPineconeå‘é‡æ•°æ®åº“ + BGE-M3 Embedding + Cohere ReRank + Gemini 2.5 Pro*")
    st.markdown("---")

    # ä¾§è¾¹æ ï¼šç³»ç»ŸçŠ¶æ€
    st.sidebar.header("ğŸ”§ ç³»ç»ŸçŠ¶æ€")

    # ç¯å¢ƒæ£€æŸ¥
    issues = check_environment()
    if issues:
        st.sidebar.error("âš ï¸ é…ç½®é—®é¢˜:")
        for issue in issues:
            st.sidebar.write(issue)

    # ä¾§è¾¹æ ï¼šæ•°æ®èŒƒå›´ä¿¡æ¯
    st.sidebar.header("ğŸ“Š å½“å‰æ•°æ®èŒƒå›´")
    st.sidebar.info("""
    **æ—¶é—´èŒƒå›´**: 2015-2016å¹´ï¼ˆå·²ä¸Šä¼ ï¼‰

    **å…šæ´¾è¦†ç›–**:
    - CDU/CSU (è”ç›Ÿå…š)
    - SPD (ç¤¾ä¼šæ°‘ä¸»å…š)
    - FDP (è‡ªç”±æ°‘ä¸»å…š)
    - BÃœNDNIS 90/DIE GRÃœNEN (ç»¿å…š)
    - DIE LINKE (å·¦ç¿¼å…š)
    - AfD (å¾·å›½é€‰æ‹©å…š)

    **å‘é‡æ•°æ®åº“**: Pinecone (german-bgeç´¢å¼•)
    **æ€»å‘é‡æ•°**: ~50,000+
    """)

    # ä¸»ç•Œé¢ï¼šé—®é¢˜è¾“å…¥åŒºåŸŸ
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    # 2015å¹´æµ‹è¯•é—®é¢˜
    st.subheader("ğŸ“‹ 2015å¹´æµ‹è¯•é—®é¢˜")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**ğŸ” æ€»ç»“ç±» & å¯¹æ¯”ç±»**")
        test_questions_1 = [
            "è¯·æ€»ç»“2015å¹´å¾·å›½è®®ä¼šå…³äºéš¾æ°‘æ”¿ç­–çš„ä¸»è¦è®¨è®ºå†…å®¹",
            "CDU/CSUå’ŒSPDåœ¨2015å¹´å¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ",
        ]
        for q in test_questions_1:
            st.markdown(f"- {q}")

    with col2:
        st.markdown("**ğŸ“Š è§‚ç‚¹ç±» & äº‹å®æŸ¥è¯¢**")
        test_questions_2 = [
            "2015å¹´å¾·å›½è®®ä¼šè®®å‘˜å¯¹æ¬§ç›Ÿä¸€ä½“åŒ–çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "2015å¹´å¾·å›½è®®ä¼šæœ‰å“ªäº›é‡è¦æ³•æ¡ˆè¢«è®¨è®ºï¼Ÿ"
        ]
        for q in test_questions_2:
            st.markdown(f"- {q}")

    # é—®é¢˜è¾“å…¥
    st.subheader("âœï¸ è¾“å…¥æ‚¨çš„é—®é¢˜")

    # å¿«é€Ÿé€‰æ‹©
    all_test_questions = [
        "è¯·æ€»ç»“2015å¹´å¾·å›½è®®ä¼šå…³äºéš¾æ°‘æ”¿ç­–çš„ä¸»è¦è®¨è®ºå†…å®¹",
        "CDU/CSUå’ŒSPDåœ¨2015å¹´å¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ",
        "2015å¹´å¾·å›½è®®ä¼šè®®å‘˜å¯¹æ¬§ç›Ÿä¸€ä½“åŒ–çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "2015å¹´å¾·å›½è®®ä¼šæœ‰å“ªäº›é‡è¦æ³•æ¡ˆè¢«è®¨è®ºï¼Ÿ"
    ]

    selected_question = st.selectbox("é€‰æ‹©æµ‹è¯•é—®é¢˜", ["è¯·é€‰æ‹©..."] + all_test_questions)

    # é—®é¢˜è¾“å…¥æ¡†
    if selected_question != "è¯·é€‰æ‹©...":
        user_question = st.text_area("é—®é¢˜å†…å®¹", value=selected_question, height=100)
    else:
        user_question = st.text_area(
            "é—®é¢˜å†…å®¹",
            placeholder="ä¾‹å¦‚ï¼šè¯·æ€»ç»“2015å¹´å¾·å›½è®®ä¼šå…³äºéš¾æ°‘æ”¿ç­–çš„ä¸»è¦è®¨è®ºå†…å®¹",
            height=100
        )

    # é«˜çº§é€‰é¡¹
    with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹"):
        col1, col2 = st.columns(2)

        with col1:
            enable_rerank = st.checkbox(
                "å¯ç”¨Cohere ReRank",
                value=True,
                help="ä½¿ç”¨Cohere APIè¿›è¡Œæ–‡æ¡£é‡æ’ï¼Œæå‡ç­”æ¡ˆè´¨é‡"
            )

        with col2:
            show_retrieved_docs = st.checkbox(
                "æ˜¾ç¤ºæ£€ç´¢æ–‡æ¡£",
                value=False,
                help="æ˜¾ç¤ºæ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£å†…å®¹"
            )

    # æäº¤æŒ‰é’®
    col1, col2, col3 = st.columns([2, 1, 2])
    with col2:
        submit_button = st.button(
            "ğŸš€ å¼€å§‹é—®ç­”",
            type="primary",
            disabled=not user_question.strip(),
            use_container_width=True
        )

    if submit_button:
        st.markdown("---")
        st.header("ğŸ“Š å¤„ç†è¿›åº¦")

        # è¿è¡Œworkflow
        result = run_complete_workflow_with_progress(user_question, enable_rerank)

        st.markdown("---")

        if result.get("success"):
            # æ˜¾ç¤ºç­”æ¡ˆ
            st.header("ğŸ’¡ ç”Ÿæˆçš„ç­”æ¡ˆ")
            st.markdown(result['answer'])

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            st.markdown("---")
            st.subheader("ğŸ“ˆ å¤„ç†ç»Ÿè®¡")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("æå–å‚æ•°", len(result.get('params', {})))

            with col2:
                st.metric("æ£€ç´¢æ–‡æ¡£", result.get('chunks_retrieved', 0))

            with col3:
                st.metric("é‡æ’åæ–‡æ¡£", result.get('chunks_reranked', 0))

            with col4:
                st.metric("ç­”æ¡ˆé•¿åº¦", f"{len(result['answer'])} å­—ç¬¦")

            # æ˜¾ç¤ºæ£€ç´¢æ–‡æ¡£
            if show_retrieved_docs and result.get('reranked_chunks'):
                st.markdown("---")
                st.subheader("ğŸ“„ æ£€ç´¢åˆ°çš„æ–‡æ¡£")

                for i, chunk in enumerate(result['reranked_chunks'][:5], 1):
                    with st.expander(f"æ–‡æ¡£ {i} - ç›¸å…³æ€§è¯„åˆ†: {chunk.get('rerank_score', chunk.get('score', 0)):.4f}"):
                        metadata = chunk.get('metadata', {})

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.write(f"**å‘è¨€äºº**: {metadata.get('speaker', 'æœªçŸ¥')}")
                        with col2:
                            st.write(f"**å…šæ´¾**: {metadata.get('group', 'æœªçŸ¥')}")
                        with col3:
                            st.write(f"**æ—¥æœŸ**: {metadata.get('date', 'æœªçŸ¥')}")

                        st.markdown("**æ–‡æ¡£å†…å®¹**:")
                        st.text(chunk.get('text', '')[:500] + "...")
        else:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # é¡µé¢åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    st.markdown("""
    ### ğŸ’¡ ä½¿ç”¨æç¤º

    1. **å®Œæ•´æµç¨‹**: å‚æ•°æå– â†’ æ–‡æ¡£æ£€ç´¢(20ä¸ª) â†’ Cohereé‡æ’(10ä¸ª) â†’ LLMç”Ÿæˆç­”æ¡ˆ
    2. **å®æ—¶è¿›åº¦**: ç³»ç»Ÿä¼šæ˜¾ç¤ºæ¯ä¸ªå¤„ç†é˜¶æ®µçš„å®æ—¶çŠ¶æ€
    3. **ReRankä¼˜åŠ¿**: å¯ç”¨åå¯æ˜¾è‘—æå‡ç­”æ¡ˆè´¨é‡ï¼Œä½†ä¼šå¢åŠ 1-2ç§’å¤„ç†æ—¶é—´
    4. **æµ‹è¯•é—®é¢˜**: å»ºè®®ä»ä¸Šæ–¹4ä¸ªæµ‹è¯•é—®é¢˜å¼€å§‹ä½“éªŒ

    ### ğŸ”§ æŠ€æœ¯æ¶æ„
    - **Embedding**: BGE-M3 (æœ¬åœ°, 1024ç»´, GPUåŠ é€Ÿ)
    - **å‘é‡æ•°æ®åº“**: Pinecone (german-bgeç´¢å¼•)
    - **é‡æ’**: Cohere rerank-v3.5 API
    - **ç”Ÿæˆ**: Gemini 2.5 Pro
    """)


if __name__ == "__main__":
    main()
