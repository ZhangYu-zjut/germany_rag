#!/usr/bin/env python3
"""
2015-2020å¹´æ•°æ®æ‰¹é‡è¿ç§»åˆ°Pinecone
ä½¿ç”¨å·²éªŒè¯çš„BGE-M3 + Pinecone Manual Configurationæ–¹æ¡ˆ
"""

import os
import sys
import json
import time
import gc
import random
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv
import signal

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter
from src.llm.embeddings import GeminiEmbeddingClient

logger = setup_logger()

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…é€€å‡º
should_stop = False

def signal_handler(signum, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·"""
    global should_stop
    should_stop = True
    logger.info("ğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

@dataclass
class MigrationStats:
    """è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
    year: int
    records_processed: int = 0
    chunks_generated: int = 0
    vectors_created: int = 0
    vectors_stored: int = 0
    start_time: float = 0
    end_time: float = 0
    
    @property
    def duration_minutes(self) -> float:
        return (self.end_time - self.start_time) / 60 if self.end_time > 0 else 0

class Migrate2015To2020Pinecone:
    """2015-2020å¹´æ•°æ®è¿ç§»åˆ°Pinecone"""
    
    def __init__(self):
        # Pineconeé…ç½®
        self.pinecone_api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pinecone_region = os.getenv("PINECONE_REGION", "us-east-1")
        
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_VECTOR_DATABASE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
        # ä½¿ç”¨å·²éªŒè¯æˆåŠŸçš„BGE-M3å‚æ•°
        self.embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )
        
        # åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯
        self.pc = self._init_pinecone_client()
        self.index = None
        self.index_name = "german-parliament-2015-2020"
        
        # è¿ç§»ç»Ÿè®¡
        self.stats: List[MigrationStats] = []
        
        logger.info("ğŸš€ åˆå§‹åŒ–2015-2020å¹´Pineconeè¿ç§»ç³»ç»Ÿ")
        logger.info("âœ… ä½¿ç”¨å·²éªŒè¯çš„BGE-M3æœ¬åœ°embedding (1024ç»´)")
    
    def _init_pinecone_client(self):
        """åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯"""
        try:
            from pinecone import Pinecone, ServerlessSpec
            
            pc = Pinecone(api_key=self.pinecone_api_key)
            logger.info("âœ… Pineconeå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return pc
            
        except Exception as e:
            logger.error(f"âŒ Pineconeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def create_or_connect_index(self):
        """åˆ›å»ºæˆ–è¿æ¥åˆ°Pineconeç´¢å¼•"""
        logger.info(f"ğŸ“¦ è®¾ç½®Pineconeç´¢å¼•: {self.index_name}")
        
        try:
            # æ£€æŸ¥ç°æœ‰ç´¢å¼•
            existing_indexes = self.pc.list_indexes()
            index_names = [idx.name for idx in existing_indexes]
            
            if self.index_name in index_names:
                logger.info(f"ğŸ”— è¿æ¥åˆ°ç°æœ‰ç´¢å¼•: {self.index_name}")
                self.index = self.pc.Index(self.index_name)
                
                # éªŒè¯ç´¢å¼•é…ç½®
                stats = self.index.describe_index_stats()
                logger.info(f"ğŸ“Š ç´¢å¼•çŠ¶æ€: {stats}")
                
            else:
                logger.info(f"ğŸ”¨ åˆ›å»ºæ–°ç´¢å¼•: {self.index_name}")
                
                # åˆ›å»ºç´¢å¼• - Manual Configurationï¼Œæ”¯æŒBGE-M3
                from pinecone import ServerlessSpec
                
                self.pc.create_index(
                    name=self.index_name,
                    dimension=1024,  # BGE-M3ç»´åº¦
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud="aws",
                        region=self.pinecone_region
                    )
                )
                
                logger.info("â³ ç­‰å¾…ç´¢å¼•åˆå§‹åŒ–...")
                time.sleep(30)  # ç­‰å¾…ç´¢å¼•å‡†å¤‡å°±ç»ª
                
                self.index = self.pc.Index(self.index_name)
                logger.info(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ: {self.index_name}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•è®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    def load_year_data(self, year: int) -> List[Dict]:
        """åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®"""
        logger.info(f"ğŸ“– åŠ è½½{year}å¹´æ•°æ®...")
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼š2021å¹´æ•°æ®è¢«åˆ†æˆä¸¤ä¸ªæ–‡ä»¶
        if year == 2021:
            logger.info("ğŸ”„ å¤„ç†2021å¹´åˆå¹¶æ•°æ®...")
            data_file = project_root / "data/pp_json_49-21/pp_2021_merged.json"
            if not data_file.exists():
                logger.error(f"âŒ 2021å¹´åˆå¹¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œmerge_2021_data.pyåˆå¹¶2021å¹´æ•°æ®")
                return []
        else:
            data_file = project_root / f"data/pp_json_49-21/pp_{year}.json"
        
        if not data_file.exists():
            logger.error(f"âŒ {year}å¹´æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return []
        
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            transcript = data.get('transcript', [])
            file_size_mb = data_file.stat().st_size / (1024*1024)
            
            logger.info(f"âœ… {year}å¹´æ•°æ®åŠ è½½æˆåŠŸ: {len(transcript):,}æ¡è®°å½•, {file_size_mb:.1f}MB")
            return transcript
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return []
    
    def process_year_chunks(self, year: int, transcript: List[Dict]) -> List[Dict]:
        """å¤„ç†å¹´ä»½æ•°æ®åˆ†å—"""
        logger.info(f"âœ‚ï¸  å¤„ç†{year}å¹´æ–‡æœ¬åˆ†å—...")
        
        all_chunks = []
        valid_records = 0
        
        for i, record in enumerate(transcript):
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢åˆ†å—å¤„ç†")
                break
            
            if not isinstance(record, dict):
                continue
            
            speech_text = record.get('speech', '').strip()
            if not speech_text or len(speech_text) < 10:
                continue
                
            valid_records += 1
            
            # åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(speech_text)
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = record.get('metadata', {})
            base_metadata = {
                "year": metadata.get('year', year),
                "month": str(metadata.get('month', '')),
                "day": str(metadata.get('day', '')),
                "speaker": str(metadata.get('speaker', '')),
                "party": str(metadata.get('party', '')),
                "text_id": str(metadata.get('id', f"{year}_{i}")),
                "record_index": i
            }
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": j,
                    "total_chunks": len(chunks),
                    "chunk_id": f"{year}_{i}_{j}"
                })
                
                all_chunks.append({
                    "id": f"{year}_{i}_{j}",
                    "text": chunk,
                    "metadata": chunk_metadata
                })
            
            # æ¯å¤„ç†1000æ¡è®°å½•æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 1000 == 0:
                logger.info(f"   ğŸ“ˆ å·²å¤„ç† {i + 1:,}/{len(transcript):,} æ¡è®°å½•...")
        
        logger.info(f"âœ… {year}å¹´åˆ†å—å®Œæˆ: {valid_records:,}æ¡æœ‰æ•ˆè®°å½• â†’ {len(all_chunks):,}ä¸ªchunks")
        return all_chunks
    
    def generate_embeddings(self, year: int, chunks: List[Dict]) -> List[List[float]]:
        """ç”ŸæˆBGE-M3 embeddings"""
        logger.info(f"ğŸ§  ç”Ÿæˆ{year}å¹´BGE-M3 embeddings...")
        
        texts = [chunk["text"] for chunk in chunks]
        
        # ä½¿ç”¨å·²éªŒè¯æˆåŠŸçš„ä¿å®ˆå‚æ•°
        try:
            embeddings = self.embedding_client.embed_batch(
                texts,
                batch_size=64,     # å·²éªŒè¯çš„ç¨³å®šæ‰¹æ¬¡å¤§å°
                max_workers=4,     # å·²éªŒè¯çš„ç¨³å®šå¹¶å‘æ•°
                request_delay=1.0  # ä¿å®ˆå»¶è¿Ÿ
            )
            
            logger.info(f"âœ… {year}å¹´embeddingsç”ŸæˆæˆåŠŸ: {len(embeddings):,}ä¸ª1024ç»´å‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´embeddingsç”Ÿæˆå¤±è´¥: {str(e)}")
            return []
    
    def store_to_pinecone(self, year: int, chunks: List[Dict], embeddings: List[List[float]]) -> int:
        """å­˜å‚¨åˆ°Pinecone"""
        logger.info(f"ğŸ’¾ å­˜å‚¨{year}å¹´æ•°æ®åˆ°Pinecone...")
        
        if len(chunks) != len(embeddings):
            logger.error(f"âŒ æ•°æ®ä¸åŒ¹é…: {len(chunks)} chunks vs {len(embeddings)} embeddings")
            return 0
        
        batch_size = 100  # Pineconeæ¨èæ‰¹æ¬¡å¤§å°
        total_batches = len(chunks) // batch_size + (1 if len(chunks) % batch_size else 0)
        
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å­˜å‚¨: {len(chunks):,}ä¸ªBGE-M3å‘é‡, {total_batches}ä¸ªæ‰¹æ¬¡")
        
        stored_count = 0
        failed_count = 0
        
        for i in range(0, len(chunks), batch_size):
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢å­˜å‚¨")
                break
            
            batch_chunks = chunks[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            
            # æ„å»ºPineconeå‘é‡æ ¼å¼
            vectors = []
            for chunk, embedding in zip(batch_chunks, batch_embeddings):
                # ç¡®ä¿embeddingæ˜¯æœ‰æ•ˆçš„
                if not embedding or len(embedding) != 1024:
                    logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆå‘é‡: {chunk['id']}")
                    failed_count += 1
                    continue
                
                # é™åˆ¶metadataå¤§å°ï¼ˆPineconeæœ‰é™åˆ¶ï¼‰
                safe_metadata = {
                    "text": chunk["text"][:500],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    "year": str(chunk["metadata"].get("year", "")),
                    "speaker": str(chunk["metadata"].get("speaker", ""))[:50],
                    "party": str(chunk["metadata"].get("party", ""))[:30],
                    "text_id": str(chunk["metadata"].get("text_id", ""))[:50],
                    "chunk_index": chunk["metadata"].get("chunk_index", 0)
                }
                
                vectors.append({
                    "id": chunk["id"],
                    "values": embedding,
                    "metadata": safe_metadata
                })
            
            # æ’å…¥åˆ°Pinecone
            if vectors:
                try:
                    self.index.upsert(vectors=vectors)
                    stored_count += len(vectors)
                    
                    batch_num = i // batch_size + 1
                    progress = (stored_count / len(chunks)) * 100
                    logger.info(f"ğŸ“ˆ æ‰¹æ¬¡ {batch_num}/{total_batches}: {progress:.1f}% ({stored_count:,}/{len(chunks):,})")
                    
                    # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«è¯·æ±‚
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å­˜å‚¨å¤±è´¥: {str(e)}")
                    failed_count += len(vectors)
        
        logger.info(f"âœ… {year}å¹´å­˜å‚¨å®Œæˆ: {stored_count:,}ä¸ªBGE-M3å‘é‡æˆåŠŸå­˜å‚¨åˆ°Pinecone")
        if failed_count > 0:
            logger.warning(f"âš ï¸  å¤±è´¥å‘é‡: {failed_count:,}ä¸ª")
        
        # ç­‰å¾…ç´¢å¼•æ›´æ–°
        logger.info("â³ ç­‰å¾…Pineconeç´¢å¼•æ›´æ–°...")
        time.sleep(10)
        
        return stored_count
    
    def verify_storage(self, year: int):
        """éªŒè¯å­˜å‚¨ç»“æœ"""
        logger.info(f"ğŸ” éªŒè¯{year}å¹´å­˜å‚¨ç»“æœ...")
        
        try:
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.index.describe_index_stats()
            
            total_vectors = stats['total_vector_count']
            dimension = stats['dimension']
            
            logger.info(f"ğŸ“Š Pineconeç´¢å¼•ç»Ÿè®¡:")
            logger.info(f"   æ€»å‘é‡æ•°: {total_vectors:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {dimension}")
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            if total_vectors > 0:
                logger.info("ğŸ” æµ‹è¯•BGE-M3å‘é‡æœç´¢åŠŸèƒ½...")
                
                # ä½¿ç”¨çœŸå®çš„BGE-M3å‘é‡è¿›è¡Œæœç´¢
                test_text = f"{year}å¹´å¾·å›½è®®ä¼šè®¨è®ºç»æµæ”¿ç­–"
                test_embedding = self.embedding_client.embed_single(test_text)
                
                if test_embedding:
                    search_results = self.index.query(
                        vector=test_embedding,
                        top_k=3,
                        include_metadata=True
                    )
                    
                    logger.info(f"âœ… BGE-M3è¯­ä¹‰æœç´¢æµ‹è¯•æˆåŠŸï¼Œè¿”å› {len(search_results.matches)} ä¸ªç»“æœ")
                    
                    # æ˜¾ç¤ºæœç´¢ç»“æœç¤ºä¾‹
                    for i, match in enumerate(search_results.matches[:2], 1):
                        score = match.score
                        metadata = match.metadata
                        text = metadata.get('text', '')[:100] + "..." if len(metadata.get('text', '')) > 100 else metadata.get('text', '')
                        speaker = metadata.get('speaker', 'Unknown')
                        year_meta = metadata.get('year', 'Unknown')
                        logger.info(f"   [{i}] ç›¸ä¼¼åº¦: {score:.4f}, {year_meta}å¹´, å‘è¨€äºº: {speaker}")
                        logger.info(f"       æ–‡æœ¬: {text}")
                else:
                    logger.warning("âš ï¸  æ— æ³•ç”Ÿæˆæµ‹è¯•å‘é‡ï¼Œè·³è¿‡æœç´¢æµ‹è¯•")
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    def migrate_single_year(self, year: int) -> MigrationStats:
        """è¿ç§»å•ä¸ªå¹´ä»½çš„æ•°æ®"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¼€å§‹è¿ç§»{year}å¹´æ•°æ®åˆ°Pinecone")
        logger.info("=" * 60)
        
        stats = MigrationStats(year=year, start_time=time.time())
        
        try:
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            # 1. åŠ è½½æ•°æ®
            transcript = self.load_year_data(year)
            if not transcript:
                logger.warning(f"âš ï¸  {year}å¹´æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            stats.records_processed = len(transcript)
            
            # 2. æ–‡æœ¬åˆ†å—
            if should_stop:
                return stats
            
            chunks = self.process_year_chunks(year, transcript)
            if not chunks:
                logger.warning(f"âš ï¸  {year}å¹´åˆ†å—ç»“æœä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            stats.chunks_generated = len(chunks)
            
            # 3. ç”Ÿæˆembeddings
            if should_stop:
                return stats
            
            embeddings = self.generate_embeddings(year, chunks)
            if not embeddings:
                logger.error(f"âŒ {year}å¹´embeddingsç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡å­˜å‚¨")
                return stats
            
            stats.vectors_created = len(embeddings)
            
            # 4. å­˜å‚¨åˆ°Pinecone
            if should_stop:
                return stats
            
            stored_count = self.store_to_pinecone(year, chunks, embeddings)
            stats.vectors_stored = stored_count
            
            # 5. éªŒè¯å­˜å‚¨ç»“æœ
            if stored_count > 0:
                self.verify_storage(year)
            
            # 6. æ¸…ç†å†…å­˜
            del transcript, chunks, embeddings
            gc.collect()
            
            stats.end_time = time.time()
            
            logger.info(f"ğŸ‰ {year}å¹´è¿ç§»å®Œæˆ!")
            logger.info(f"   ğŸ“Š å¤„ç†è®°å½•: {stats.records_processed:,}æ¡")
            logger.info(f"   ğŸ“Š ç”Ÿæˆchunks: {stats.chunks_generated:,}ä¸ª")
            logger.info(f"   ğŸ“Š åˆ›å»ºå‘é‡: {stats.vectors_created:,}ä¸ª")
            logger.info(f"   ğŸ“Š å­˜å‚¨å‘é‡: {stats.vectors_stored:,}ä¸ª")
            logger.info(f"   â±ï¸  è€—æ—¶: {stats.duration_minutes:.1f}åˆ†é’Ÿ")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´è¿ç§»å¤±è´¥: {str(e)}")
            stats.end_time = time.time()
            return stats
    
    def migrate_all_years(self, start_year: int = 2015, end_year: int = 2020):
        """è¿ç§»æ‰€æœ‰å¹´ä»½çš„æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹2015-2020å¹´æ‰¹é‡æ•°æ®è¿ç§»åˆ°Pinecone")
        logger.info("=" * 80)
        
        # é¦–å…ˆè®¾ç½®ç´¢å¼•
        if not self.create_or_connect_index():
            logger.error("âŒ ç´¢å¼•è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¿ç§»")
            return
        
        years = list(range(start_year, end_year + 1))
        total_start_time = time.time()
        
        for year in years:
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢æ‰¹é‡è¿ç§»")
                break
            
            logger.info(f"ğŸ“… å¼€å§‹å¤„ç†{year}å¹´ ({years.index(year) + 1}/{len(years)})")
            stats = self.migrate_single_year(year)
            self.stats.append(stats)
            
            # å¹´ä»½é—´æš‚åœï¼Œè®©ç³»ç»Ÿä¼‘æ¯
            if not should_stop:
                logger.info("â³ å¹´ä»½é—´æš‚åœ5ç§’...")
                time.sleep(5)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        total_time = time.time() - total_start_time
        self.generate_final_report(total_time)
    
    def generate_final_report(self, total_time: float):
        """ç”Ÿæˆæœ€ç»ˆè¿ç§»æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š 2015-2020å¹´Pineconeè¿ç§»æœ€ç»ˆæŠ¥å‘Š")
        logger.info("=" * 80)
        
        # ç»Ÿè®¡æ€»æ•°
        total_records = sum(s.records_processed for s in self.stats)
        total_chunks = sum(s.chunks_generated for s in self.stats)
        total_vectors_created = sum(s.vectors_created for s in self.stats)
        total_vectors_stored = sum(s.vectors_stored for s in self.stats)
        successful_years = len([s for s in self.stats if s.vectors_stored > 0])
        
        logger.info(f"ğŸ¯ æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"   æˆåŠŸå¹´ä»½: {successful_years}/{len(self.stats)}")
        logger.info(f"   å¤„ç†è®°å½•: {total_records:,}æ¡")
        logger.info(f"   ç”Ÿæˆchunks: {total_chunks:,}ä¸ª")
        logger.info(f"   åˆ›å»ºå‘é‡: {total_vectors_created:,}ä¸ª")
        logger.info(f"   å­˜å‚¨å‘é‡: {total_vectors_stored:,}ä¸ª")
        logger.info(f"   æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
        
        if total_time > 0:
            logger.info(f"   å¹³å‡é€Ÿåº¦: {total_vectors_created/total_time:.1f} å‘é‡/ç§’")
        
        logger.info(f"\nğŸ“‹ å„å¹´ä»½è¯¦æƒ…:")
        for stats in self.stats:
            status = "âœ…" if stats.vectors_stored > 0 else "âŒ"
            logger.info(f"   {status} {stats.year}å¹´: {stats.records_processed:,}è®°å½• â†’ {stats.chunks_generated:,}chunks â†’ {stats.vectors_stored:,}å‘é‡ ({stats.duration_minutes:.1f}åˆ†é’Ÿ)")
        
        # æœ€ç»ˆç´¢å¼•ç»Ÿè®¡
        try:
            final_stats = self.index.describe_index_stats()
            logger.info(f"\nğŸ“Š æœ€ç»ˆPineconeç´¢å¼•ç»Ÿè®¡:")
            logger.info(f"   ç´¢å¼•åç§°: {self.index_name}")
            logger.info(f"   æ€»å‘é‡æ•°: {final_stats['total_vector_count']:,}")
            logger.info(f"   å‘é‡ç»´åº¦: {final_stats['dimension']} (BGE-M3)")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•è·å–æœ€ç»ˆç»Ÿè®¡: {e}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"migration_2015_2020_pinecone_report.md"
        self.save_detailed_report(report_file, total_time)
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        logger.info(f"\nğŸ‰ 2015-2020å¹´æ•°æ®è¿ç§»å®Œæˆ!")
        logger.info(f"   Vector Database: Pinecone (Manual Configuration)")  
        logger.info(f"   Embedding Model: BGE-M3 (æœ¬åœ°1024ç»´)")
        logger.info(f"   Total Cost: $0 (embeddingå…è´¹) + $70/æœˆ (Pinecone)")
    
    def save_detailed_report(self, filename: str, total_time: float):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        total_records = sum(s.records_processed for s in self.stats)
        total_chunks = sum(s.chunks_generated for s in self.stats)
        total_vectors_created = sum(s.vectors_created for s in self.stats)
        total_vectors_stored = sum(s.vectors_stored for s in self.stats)
        successful_years = len([s for s in self.stats if s.vectors_stored > 0])
        
        content = f"""# 2015-2020å¹´æ•°æ®Pineconeè¿ç§»æŠ¥å‘Š

## è¿ç§»é…ç½®
- **å‘é‡æ•°æ®åº“**: Pinecone (Manual Configuration)
- **Embeddingæ¨¡å‹**: BGE-M3 (æœ¬åœ°, 1024ç»´)
- **æ‰¹æ¬¡å‚æ•°**: batch_size=64, max_workers=4 (å†…å­˜ä¼˜åŒ–)
- **è¿ç§»æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **ç´¢å¼•åç§°**: {self.index_name}

## æ€»ä½“ç»“æœ
- **æˆåŠŸå¹´ä»½**: {successful_years}/{len(self.stats)}
- **å¤„ç†è®°å½•**: {total_records:,}æ¡
- **ç”Ÿæˆchunks**: {total_chunks:,}ä¸ª
- **åˆ›å»ºå‘é‡**: {total_vectors_created:,}ä¸ª
- **å­˜å‚¨å‘é‡**: {total_vectors_stored:,}ä¸ª
- **æ€»è€—æ—¶**: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)
- **å¹³å‡é€Ÿåº¦**: {total_vectors_created/total_time:.1f} å‘é‡/ç§’

## å„å¹´ä»½è¯¦æƒ…

| å¹´ä»½ | çŠ¶æ€ | è®°å½•æ•° | Chunks | å‘é‡æ•° | è€—æ—¶(åˆ†) |
|------|------|--------|--------|--------|----------|
"""
        
        for stats in self.stats:
            status = "âœ…" if stats.vectors_stored > 0 else "âŒ"
            content += f"| {stats.year} | {status} | {stats.records_processed:,} | {stats.chunks_generated:,} | {stats.vectors_stored:,} | {stats.duration_minutes:.1f} |\n"
        
        content += f"""

## æˆæœ¬åˆ†æ
- **BGE-M3 Embedding**: $0 (æœ¬åœ°å…è´¹)
- **Pineconeå­˜å‚¨**: $70/æœˆ
- **æ€»ä½“æˆæœ¬**: è¿œä½äºOpenAIæ–¹æ¡ˆ

## æŠ€æœ¯éªŒè¯
âœ… BGE-M3æœ¬åœ°embeddingç¨³å®šè¿è¡Œ
âœ… å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆæœ‰æ•ˆ
âœ… Pinecone Manual ConfigurationæˆåŠŸ
âœ… å¤§è§„æ¨¡æ•°æ®å¤„ç†éªŒè¯é€šè¿‡

## ç»“è®º
2015-2020å¹´å¾·å›½è®®ä¼šæ•°æ®æˆåŠŸè¿ç§»åˆ°Pineconeï¼Œ
ä½¿ç”¨BGE-M3æœ¬åœ°embedding + Pinecone Manual Configuration
å®ç°äº†é«˜æ€§èƒ½ã€ä½æˆæœ¬çš„å‘é‡å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="2015-2020å¹´æ•°æ®è¿ç§»åˆ°Pinecone")
    parser.add_argument("--start-year", type=int, default=2015, help="å¼€å§‹å¹´ä»½")
    parser.add_argument("--end-year", type=int, default=2020, help="ç»“æŸå¹´ä»½")
    parser.add_argument("--single-year", type=int, help="åªå¤„ç†å•ä¸ªå¹´ä»½")
    
    args = parser.parse_args()
    
    migrator = Migrate2015To2020Pinecone()
    
    try:
        if args.single_year:
            logger.info(f"ğŸ¯ å•å¹´ä»½æ¨¡å¼: {args.single_year}")
            # é¦–å…ˆè®¾ç½®ç´¢å¼•
            if migrator.create_or_connect_index():
                stats = migrator.migrate_single_year(args.single_year)
                logger.info(f"ğŸ‰ {args.single_year}å¹´è¿ç§»å®Œæˆ")
            else:
                logger.error("âŒ ç´¢å¼•è®¾ç½®å¤±è´¥")
                return 1
        else:
            logger.info(f"ğŸš€ æ‰¹é‡æ¨¡å¼: {args.start_year}-{args.end_year}")
            migrator.migrate_all_years(args.start_year, args.end_year)
            
        return 0
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­è¿ç§»")
        return 1
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å¤±è´¥: {str(e)}")
        return 1

if __name__ == "__main__":
    exit(main())
