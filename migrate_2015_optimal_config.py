#!/usr/bin/env python3
"""
ä½¿ç”¨æœ€ä½³é…ç½®è¿ç§»2015å¹´æ•°æ®åˆ°german-bgeç´¢å¼•
æœ€ä½³é…ç½®ï¼š4000å­—ç¬¦åˆ†å—ï¼Œ800é‡å ï¼Œæ‰¹æ¬¡100ï¼Œæ— ä»£ç†
"""

import os
import sys
import time
import json
import gc
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.llm.embeddings import GeminiEmbeddingClient
from src.data_loader.splitter import ParliamentTextSplitter

logger = setup_logger()

def check_proxy():
    """æ£€æŸ¥ç½‘ç»œä»£ç†è®¾ç½®"""
    logger.info("ğŸ” æ£€æŸ¥ç½‘ç»œä»£ç†è®¾ç½®")
    
    proxy_vars = ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']
    proxy_status = {}
    
    for var in proxy_vars:
        value = os.environ.get(var)
        if value:
            proxy_status[var] = value
            logger.info(f"   å‘ç°ä»£ç†: {var}={value}")
    
    if proxy_status:
        logger.info("âœ… ä»£ç†è®¾ç½®æ­£å¸¸ï¼Œå°†ä½¿ç”¨ä»£ç†è¿æ¥")
    else:
        logger.info("âœ… æ— ä»£ç†è®¾ç½®ï¼Œç›´æ¥è¿æ¥")

def load_2015_data():
    """åŠ è½½2015å¹´æ•°æ®"""
    logger.info("ğŸ“‚ åŠ è½½2015å¹´å¾·å›½è®®ä¼šæ•°æ®")
    
    data_file = project_root / "data" / "pp_json_49-21" / "pp_2015.json"
    
    if not data_file.exists():
        logger.error(f"âŒ 2015å¹´æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return None
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # æå–transcriptæ•°ç»„
        if 'transcript' not in raw_data:
            logger.error("âŒ æ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°transcriptå­—æ®µ")
            return None
            
        data = raw_data['transcript']
        
        logger.info(f"âœ… 2015å¹´æ•°æ®åŠ è½½æˆåŠŸ")
        logger.info(f"   æ–‡ä»¶å¤§å°: {data_file.stat().st_size / (1024*1024):.1f} MB")
        logger.info(f"   æ•°æ®æ¡æ•°: {len(data)}")
        
        # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
        if data:
            sample = data[0]
            logger.info(f"   æ•°æ®ç¤ºä¾‹:")
            logger.info(f"     ID: {sample.get('text_id', 'N/A')}")
            logger.info(f"     å‘è¨€äºº: {sample.get('metadata', {}).get('speaker', 'N/A')}")
            logger.info(f"     æ–‡æœ¬é•¿åº¦: {len(sample.get('speech', ''))}")
            
        return data
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½2015å¹´æ•°æ®å¤±è´¥: {str(e)}")
        return None

def chunk_2015_data(data):
    """ä½¿ç”¨æœ€ä½³é…ç½®åˆ†å—2015å¹´æ•°æ®"""
    logger.info("ğŸ”„ ä½¿ç”¨æœ€ä½³é…ç½®åˆ†å—2015å¹´æ•°æ®")
    
    # æœ€ä½³åˆ†å—é…ç½®
    chunk_size = 4000      # 4000å­—ç¬¦åˆ†å—
    chunk_overlap = 800    # 800å­—ç¬¦é‡å 
    
    logger.info(f"ğŸ“Š åˆ†å—é…ç½®:")
    logger.info(f"   å—å¤§å°: {chunk_size} å­—ç¬¦")
    logger.info(f"   é‡å å¤§å°: {chunk_overlap} å­—ç¬¦")
    logger.info(f"   æœ‰æ•ˆå—å¤§å°: {chunk_size - chunk_overlap} å­—ç¬¦")
    
    # åˆå§‹åŒ–åˆ†å—å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®
    text_splitter = ParliamentTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    all_chunks = []
    total_chars = 0
    
    start_time = time.time()
    
    for i, item in enumerate(data):
        text_id = item.get('text_id', f'2015_unknown_{i}')
        speech = item.get('speech', '')
        metadata = item.get('metadata', {})
        
        if not speech.strip():
            continue
            
        total_chars += len(speech)
        
        # ä½¿ç”¨åˆ†å—å™¨çš„text_splitterè¿›è¡Œåˆ†å—
        chunk_texts = text_splitter.text_splitter.split_text(speech)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        for j, chunk_text in enumerate(chunk_texts):
            chunk = {
                'text_id': f"{text_id}_chunk_{j}",
                'original_text_id': text_id,
                'chunk_index': j,
                'text': chunk_text.strip(),
                'metadata': {
                    # Core time fields (separate for precise filtering)
                    'year': metadata.get('year', '2015'),
                    'month': metadata.get('month', '01'),
                    'day': metadata.get('day', '01'),

                    # Document ID (crucial for citation)
                    'id': metadata.get('id', text_id),

                    # Speaker information
                    'speaker': metadata.get('speaker', ''),
                    'group': metadata.get('group', ''),
                    'lp': metadata.get('lp', ''),

                    # Session information
                    'session': metadata.get('session', ''),

                    # User-friendly source reference: "id + speaker + time"
                    'source_reference': f"{metadata.get('id', text_id)} | {metadata.get('speaker', 'Unknown')} | {metadata.get('year', '2015')}-{metadata.get('month', '01')}-{metadata.get('day', '01')}",

                    # Technical fields
                    'chunk_size': len(chunk_text.strip()),
                    'total_chunks': len(chunk_texts),
                    'source': 'german_parliament'
                }
            }
            all_chunks.append(chunk)
        
        # æ¯å¤„ç†100æ¡è®°å½•æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 100 == 0:
            logger.info(f"   å·²å¤„ç†: {i + 1}/{len(data)} æ¡è®°å½•")
    
    processing_time = time.time() - start_time
    
    logger.info(f"âœ… 2015å¹´æ•°æ®åˆ†å—å®Œæˆ")
    logger.info(f"   åŸå§‹è®°å½•: {len(data)} æ¡")
    logger.info(f"   ç”Ÿæˆå—æ•°: {len(all_chunks)} ä¸ª")
    logger.info(f"   æ€»å­—ç¬¦æ•°: {total_chars:,}")
    logger.info(f"   å¹³å‡å—å¤§å°: {sum(len(c['text']) for c in all_chunks) / len(all_chunks):.0f} å­—ç¬¦")
    logger.info(f"   åˆ†å—è€—æ—¶: {processing_time:.2f} ç§’")
    
    # ä¼°ç®—ç›¸æ¯”1000å­—ç¬¦åˆ†å—çš„æ”¹è¿›
    estimated_1000_chunks = total_chars // 900  # 1000-100é‡å 
    reduction_ratio = len(all_chunks) / estimated_1000_chunks if estimated_1000_chunks > 0 else 0
    
    logger.info(f"ğŸ“Š åˆ†å—ä¼˜åŒ–æ•ˆæœ:")
    logger.info(f"   1000å­—ç¬¦åˆ†å—é¢„ä¼°: {estimated_1000_chunks:,} ä¸ª")
    logger.info(f"   4000å­—ç¬¦åˆ†å—å®é™…: {len(all_chunks):,} ä¸ª")
    logger.info(f"   å—æ•°å‡å°‘: {(1-reduction_ratio)*100:.1f}%")
    
    return all_chunks

def generate_embeddings_optimized(chunks):
    """ä½¿ç”¨æœ€ä½³é…ç½®ç”Ÿæˆembeddings"""
    logger.info("ğŸ§  ä½¿ç”¨æœ€ä½³é…ç½®ç”ŸæˆBGE-M3 embeddings")
    
    # åˆå§‹åŒ–BGE-M3å®¢æˆ·ç«¯
    embedding_client = GeminiEmbeddingClient(
        embedding_mode="local",
        model_name="BAAI/bge-m3",
        dimensions=1024
    )
    
    # æå–æ–‡æœ¬
    texts = [chunk['text'] for chunk in chunks]
    
    logger.info(f"ğŸ“Š Embeddingé…ç½®:")
    logger.info(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
    logger.info(f"   æ¨¡å‹: BGE-M3")
    logger.info(f"   ç»´åº¦: 1024")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: 128 (embeddingä¼˜åŒ–)")
    
    start_time = time.time()
    
    # ä½¿ç”¨ä¼˜åŒ–çš„embeddingé…ç½®
    vectors = embedding_client.embed_batch(
        texts,
        batch_size=128,  # embeddingæ‰¹æ¬¡ä¼˜åŒ–
        max_workers=6    # é€‚ä¸­å¹¶å‘
    )
    
    embedding_time = time.time() - start_time
    
    logger.info(f"âœ… Embeddingç”Ÿæˆå®Œæˆ")
    logger.info(f"   ç”Ÿæˆæ—¶é—´: {embedding_time:.2f} ç§’")
    logger.info(f"   å¤„ç†é€Ÿåº¦: {len(texts)/embedding_time:.1f} æ¡/ç§’")
    logger.info(f"   å‘é‡ç»´åº¦éªŒè¯: {len(vectors[0]) if vectors else 0}")
    
    # æ£€æŸ¥å¹¶æ¸…ç†NaNå€¼
    import math
    cleaned_vectors = []
    nan_count = 0
    
    for i, vector in enumerate(vectors):
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        has_nan = any(math.isnan(v) or math.isinf(v) for v in vector)
        
        if has_nan:
            nan_count += 1
            logger.warning(f"   å‘ç°NaN/Infå‘é‡: ç´¢å¼•{i}, å°†ä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
            # ç”¨é›¶å‘é‡æ›¿ä»£
            cleaned_vector = [0.0] * len(vector)
        else:
            cleaned_vector = vector
        
        cleaned_vectors.append(cleaned_vector)
    
    if nan_count > 0:
        logger.warning(f"âš ï¸  æ¸…ç†äº†{nan_count}ä¸ªåŒ…å«NaN/Infçš„å‘é‡")
    
    return cleaned_vectors

def upload_to_pinecone_optimized(chunks, vectors):
    """ä½¿ç”¨æœ€ä½³é…ç½®ä¸Šä¼ åˆ°Pinecone"""
    logger.info("ğŸ“¤ ä½¿ç”¨æœ€ä½³é…ç½®ä¸Šä¼ åˆ°Pinecone")
    
    try:
        from pinecone import Pinecone
        
        # åˆå§‹åŒ–Pinecone (ç¡®ä¿æ— ä»£ç†)
        api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        pc = Pinecone(api_key=api_key)
        index = pc.Index("german-bge")
        
        # æ£€æŸ¥ä¸Šä¼ å‰çŠ¶æ€
        stats_before = index.describe_index_stats()
        initial_count = stats_before['total_vector_count']
        
        logger.info(f"ğŸ“Š Pineconeä¸Šä¼ é…ç½®:")
        logger.info(f"   ç´¢å¼•: german-bge")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: 100 (å­˜å‚¨ä¼˜åŒ–)")
        logger.info(f"   ä¸Šä¼ å‰å‘é‡æ•°: {initial_count}")
        logger.info(f"   å¾…ä¸Šä¼ å‘é‡æ•°: {len(vectors)}")
        
        # å‡†å¤‡å‘é‡æ•°æ®
        vector_data = []
        timestamp = int(time.time())
        
        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
            vector_item = {
                "id": f"2015_{timestamp}_{i}",
                "values": vector,
                "metadata": {
                    **chunk['metadata'],
                    "text": chunk['text'][:1000] + "..." if len(chunk['text']) > 1000 else chunk['text'],
                    "original_text_id": chunk['original_text_id'],
                    "chunk_index": chunk['chunk_index'],
                    "upload_timestamp": timestamp
                }
            }
            vector_data.append(vector_item)
        
        # æ‰¹é‡ä¸Šä¼  (ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°100)
        batch_size = 100
        total_batches = (len(vector_data) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸Šä¼ : {total_batches} ä¸ªæ‰¹æ¬¡")
        
        start_time = time.time()
        uploaded_count = 0
        
        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, len(vector_data))
            batch_vectors = vector_data[batch_start:batch_end]
            
            batch_start_time = time.time()
            
            try:
                upsert_response = index.upsert(vectors=batch_vectors)
                batch_time = time.time() - batch_start_time
                batch_speed = len(batch_vectors) / batch_time if batch_time > 0 else 0
                
                uploaded_count += len(batch_vectors)
                
                logger.info(f"   æ‰¹æ¬¡ {batch_idx+1}/{total_batches}: "
                          f"{len(batch_vectors)} å‘é‡, "
                          f"{batch_time:.2f}ç§’, "
                          f"{batch_speed:.1f} å‘é‡/ç§’")
                
            except Exception as e:
                logger.error(f"   âŒ æ‰¹æ¬¡ {batch_idx+1} ä¸Šä¼ å¤±è´¥: {str(e)}")
                continue
        
        total_upload_time = time.time() - start_time
        avg_speed = uploaded_count / total_upload_time if total_upload_time > 0 else 0
        
        logger.info(f"âœ… Pineconeä¸Šä¼ å®Œæˆ")
        logger.info(f"   ä¸Šä¼ å‘é‡æ•°: {uploaded_count}")
        logger.info(f"   æ€»è€—æ—¶: {total_upload_time:.2f} ç§’")
        logger.info(f"   å¹³å‡é€Ÿåº¦: {avg_speed:.1f} å‘é‡/ç§’")
        
        # ç­‰å¾…ç´¢å¼•æ›´æ–°
        logger.info("â³ ç­‰å¾…Pineconeç´¢å¼•æ›´æ–°...")
        time.sleep(5)
        
        # éªŒè¯ä¸Šä¼ ç»“æœ
        stats_after = index.describe_index_stats()
        final_count = stats_after['total_vector_count']
        
        logger.info(f"ğŸ“Š ä¸Šä¼ éªŒè¯:")
        logger.info(f"   ä¸Šä¼ å‰: {initial_count} å‘é‡")
        logger.info(f"   ä¸Šä¼ å: {final_count} å‘é‡")
        logger.info(f"   å‡€å¢åŠ : {final_count - initial_count} å‘é‡")
        
        if final_count > initial_count:
            logger.info("ğŸ‰ 2015å¹´æ•°æ®æˆåŠŸä¸Šä¼ åˆ°Pinecone!")
            return True
        else:
            logger.error("âŒ å‘é‡æ•°æœªå¢åŠ ï¼Œä¸Šä¼ å¯èƒ½å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Pineconeä¸Šä¼ å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹2015å¹´æ•°æ®æœ€ä½³é…ç½®è¿ç§»")
    logger.info("=" * 60)
    
    total_start_time = time.time()
    
    # æ­¥éª¤1: æ£€æŸ¥ä»£ç†è®¾ç½®ï¼ˆä¸å†ç¦ç”¨ï¼‰
    check_proxy()
    
    # æ­¥éª¤2: åŠ è½½2015å¹´æ•°æ®
    data = load_2015_data()
    if not data:
        return 1
    
    # æ­¥éª¤3: åˆ†å—å¤„ç†
    chunks = chunk_2015_data(data)
    if not chunks:
        return 1
    
    # æ­¥éª¤4: ç”Ÿæˆembeddings
    vectors = generate_embeddings_optimized(chunks)
    if not vectors:
        return 1
    
    # æ­¥éª¤5: ä¸Šä¼ åˆ°Pinecone
    success = upload_to_pinecone_optimized(chunks, vectors)
    if not success:
        return 1
    
    # æ€»ç»“
    total_time = time.time() - total_start_time
    
    logger.info("=" * 60)
    logger.info("ğŸ‰ 2015å¹´æ•°æ®è¿ç§»å®Œæˆ!")
    logger.info(f"ğŸ“Š è¿ç§»ç»Ÿè®¡:")
    logger.info(f"   æºæ•°æ®: {len(data)} æ¡è®°å½•")
    logger.info(f"   ç”Ÿæˆå—æ•°: {len(chunks)} ä¸ª")
    logger.info(f"   ç”Ÿæˆå‘é‡: {len(vectors)} ä¸ª")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    
    # æ€§èƒ½å¯¹æ¯”
    logger.info(f"ğŸš€ æ€§èƒ½å¯¹æ¯” (vs åŸé¢„æœŸ4-6å°æ—¶):")
    original_estimate_hours = 4.5  # åŸä¼°è®¡ä¸­å€¼
    actual_hours = total_time / 3600
    improvement = (original_estimate_hours / actual_hours - 1) * 100
    logger.info(f"   åŸé¢„æœŸ: {original_estimate_hours} å°æ—¶")
    logger.info(f"   å®é™…è€—æ—¶: {actual_hours:.2f} å°æ—¶")
    logger.info(f"   æ€§èƒ½æå‡: {improvement:.0f}x å€é€Ÿ")
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    
    logger.info("âœ… å‡†å¤‡è¿›è¡Œé—®ç­”æµ‹è¯•éªŒè¯")
    
    return 0

if __name__ == "__main__":
    exit(main())
