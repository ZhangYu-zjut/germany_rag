#!/usr/bin/env python3
"""
2015-2020å¹´æ•°æ®æ‰¹é‡è¿ç§»åˆ°Qdrant Cloud
ä½¿ç”¨å·²éªŒè¯æˆåŠŸçš„BGE-M3 + Qdrant Cloudæ–¹æ¡ˆ
"""

import os
import sys
import json
import time
import gc
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv
import signal

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.data_loader.splitter import ParliamentTextSplitter
from src.llm.embeddings import GeminiEmbeddingClient

logger = setup_logger()

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…é€€å‡º
should_stop = False

def signal_handler(signum, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·"""
    global should_stop
    should_stop = True
    logger.info("ğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

@dataclass
class MigrationStats:
    """è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
    year: int
    records_processed: int = 0
    chunks_generated: int = 0
    vectors_created: int = 0
    vectors_stored: int = 0
    start_time: float = 0
    end_time: float = 0
    
    @property
    def duration_minutes(self) -> float:
        return (self.end_time - self.start_time) / 60 if self.end_time > 0 else 0

class Migrate2015To2020:
    """2015-2020å¹´æ•°æ®è¿ç§»åˆ°Qdrant Cloud"""
    
    def __init__(self):
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
        # ä½¿ç”¨å·²éªŒè¯æˆåŠŸçš„BGE-M3å‚æ•°
        self.embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )
        
        # åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯
        self.qdrant_client = self._init_qdrant_client()
        
        # è¿ç§»ç»Ÿè®¡
        self.stats: List[MigrationStats] = []
        
        logger.info("ğŸš€ åˆå§‹åŒ–2015-2020å¹´Qdrant Cloudè¿ç§»ç³»ç»Ÿ")
        logger.info("âœ… ä½¿ç”¨å·²éªŒè¯çš„BGE-M3æœ¬åœ°embedding (batch_size=64, workers=4)")
    
    def _init_qdrant_client(self):
        """åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯"""
        try:
            from src.vectordb.qdrant_client import QdrantClient
            
            qdrant_client = QdrantClient(
                mode="cloud",
                embedding_client=self.embedding_client
            )
            
            logger.info("âœ… Qdrant Cloudå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return qdrant_client
            
        except Exception as e:
            logger.error(f"âŒ Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def load_year_data(self, year: int) -> List[Dict]:
        """åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®"""
        logger.info(f"ğŸ“– åŠ è½½{year}å¹´æ•°æ®...")
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼š2021å¹´æ•°æ®è¢«åˆ†æˆä¸¤ä¸ªæ–‡ä»¶
        if year == 2021:
            logger.info("ğŸ”„ å¤„ç†2021å¹´åˆå¹¶æ•°æ®...")
            data_file = project_root / "data/pp_json_49-21/pp_2021_merged.json"
            if not data_file.exists():
                logger.error(f"âŒ 2021å¹´åˆå¹¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œmerge_2021_data.pyåˆå¹¶2021å¹´æ•°æ®")
                return []
        else:
            data_file = project_root / f"data/pp_json_49-21/pp_{year}.json"
        
        if not data_file.exists():
            logger.error(f"âŒ {year}å¹´æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return []
        
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            transcript = data.get('transcript', [])
            file_size_mb = data_file.stat().st_size / (1024*1024)
            
            logger.info(f"âœ… {year}å¹´æ•°æ®åŠ è½½æˆåŠŸ: {len(transcript):,}æ¡è®°å½•, {file_size_mb:.1f}MB")
            return transcript
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return []
    
    def process_year_chunks(self, year: int, transcript: List[Dict]) -> List[Dict]:
        """å¤„ç†å¹´ä»½æ•°æ®åˆ†å—"""
        logger.info(f"âœ‚ï¸  å¤„ç†{year}å¹´æ–‡æœ¬åˆ†å—...")
        
        all_chunks = []
        valid_records = 0
        
        for i, record in enumerate(transcript):
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢åˆ†å—å¤„ç†")
                break
            
            if not isinstance(record, dict):
                continue
            
            speech_text = record.get('speech', '').strip()
            if not speech_text or len(speech_text) < 10:
                continue
                
            valid_records += 1
            
            # åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(speech_text)
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = record.get('metadata', {})
            base_metadata = {
                "year": metadata.get('year', year),
                "month": str(metadata.get('month', '')),
                "day": str(metadata.get('day', '')),
                "speaker": str(metadata.get('speaker', '')),
                "party": str(metadata.get('party', '')),
                "text_id": str(metadata.get('id', f"{year}_{i}")),
                "record_index": i
            }
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": j,
                    "total_chunks": len(chunks),
                    "chunk_id": f"{year}_{i}_{j}"
                })
                
                all_chunks.append({
                    "id": f"{year}_{i}_{j}",
                    "text": chunk,
                    "metadata": chunk_metadata
                })
            
            # æ¯å¤„ç†1000æ¡è®°å½•æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 1000 == 0:
                logger.info(f"   ğŸ“ˆ å·²å¤„ç† {i + 1:,}/{len(transcript):,} æ¡è®°å½•...")
        
        logger.info(f"âœ… {year}å¹´åˆ†å—å®Œæˆ: {valid_records:,}æ¡æœ‰æ•ˆè®°å½• â†’ {len(all_chunks):,}ä¸ªchunks")
        return all_chunks
    
    def generate_embeddings(self, year: int, chunks: List[Dict]) -> List[List[float]]:
        """ç”ŸæˆBGE-M3 embeddings"""
        logger.info(f"ğŸ§  ç”Ÿæˆ{year}å¹´BGE-M3 embeddings...")
        
        texts = [chunk["text"] for chunk in chunks]
        
        # ä½¿ç”¨å·²éªŒè¯æˆåŠŸçš„ä¿å®ˆå‚æ•°
        try:
            embeddings = self.embedding_client.embed_batch(
                texts,
                batch_size=64,     # å·²éªŒè¯çš„ç¨³å®šæ‰¹æ¬¡å¤§å°
                max_workers=4,     # å·²éªŒè¯çš„ç¨³å®šå¹¶å‘æ•°
                request_delay=1.0  # ä¿å®ˆå»¶è¿Ÿ
            )
            
            logger.info(f"âœ… {year}å¹´embeddingsç”ŸæˆæˆåŠŸ: {len(embeddings):,}ä¸ª1024ç»´å‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´embeddingsç”Ÿæˆå¤±è´¥: {str(e)}")
            return []
    
    def store_to_qdrant(self, year: int, chunks: List[Dict], embeddings: List[List[float]]) -> int:
        """å­˜å‚¨åˆ°Qdrant Cloud"""
        logger.info(f"ğŸ’¾ å­˜å‚¨{year}å¹´æ•°æ®åˆ°Qdrant Cloud...")
        
        if len(chunks) != len(embeddings):
            logger.error(f"âŒ æ•°æ®ä¸åŒ¹é…: {len(chunks)} chunks vs {len(embeddings)} embeddings")
            return 0
        
        try:
            # ä½¿ç”¨Qdrantå®¢æˆ·ç«¯çš„æ‰¹é‡æ’å…¥åŠŸèƒ½
            stored_count = self.qdrant_client.upsert_german_parliament_data(
                chunks, embeddings
            )
            
            logger.info(f"âœ… {year}å¹´æ•°æ®å­˜å‚¨æˆåŠŸ: {stored_count:,}ä¸ªå‘é‡")
            return stored_count
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´æ•°æ®å­˜å‚¨å¤±è´¥: {str(e)}")
            return 0
    
    def migrate_single_year(self, year: int) -> MigrationStats:
        """è¿ç§»å•ä¸ªå¹´ä»½çš„æ•°æ®"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¼€å§‹è¿ç§»{year}å¹´æ•°æ®")
        logger.info("=" * 60)
        
        stats = MigrationStats(year=year, start_time=time.time())
        
        try:
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            # 1. åŠ è½½æ•°æ®
            transcript = self.load_year_data(year)
            if not transcript:
                logger.warning(f"âš ï¸  {year}å¹´æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            stats.records_processed = len(transcript)
            
            # 2. æ–‡æœ¬åˆ†å—
            if should_stop:
                return stats
            
            chunks = self.process_year_chunks(year, transcript)
            if not chunks:
                logger.warning(f"âš ï¸  {year}å¹´åˆ†å—ç»“æœä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return stats
            
            stats.chunks_generated = len(chunks)
            
            # 3. ç”Ÿæˆembeddings
            if should_stop:
                return stats
            
            embeddings = self.generate_embeddings(year, chunks)
            if not embeddings:
                logger.error(f"âŒ {year}å¹´embeddingsç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡å­˜å‚¨")
                return stats
            
            stats.vectors_created = len(embeddings)
            
            # 4. å­˜å‚¨åˆ°Qdrant
            if should_stop:
                return stats
            
            stored_count = self.store_to_qdrant(year, chunks, embeddings)
            stats.vectors_stored = stored_count
            
            # 5. æ¸…ç†å†…å­˜
            del transcript, chunks, embeddings
            gc.collect()
            
            stats.end_time = time.time()
            
            logger.info(f"ğŸ‰ {year}å¹´è¿ç§»å®Œæˆ!")
            logger.info(f"   ğŸ“Š å¤„ç†è®°å½•: {stats.records_processed:,}æ¡")
            logger.info(f"   ğŸ“Š ç”Ÿæˆchunks: {stats.chunks_generated:,}ä¸ª")
            logger.info(f"   ğŸ“Š åˆ›å»ºå‘é‡: {stats.vectors_created:,}ä¸ª")
            logger.info(f"   ğŸ“Š å­˜å‚¨å‘é‡: {stats.vectors_stored:,}ä¸ª")
            logger.info(f"   â±ï¸  è€—æ—¶: {stats.duration_minutes:.1f}åˆ†é’Ÿ")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ {year}å¹´è¿ç§»å¤±è´¥: {str(e)}")
            stats.end_time = time.time()
            return stats
    
    def migrate_all_years(self, start_year: int = 2015, end_year: int = 2020):
        """è¿ç§»æ‰€æœ‰å¹´ä»½çš„æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹2015-2020å¹´æ‰¹é‡æ•°æ®è¿ç§»åˆ°Qdrant Cloud")
        logger.info("=" * 80)
        
        years = list(range(start_year, end_year + 1))
        total_start_time = time.time()
        
        for year in years:
            if should_stop:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢æ‰¹é‡è¿ç§»")
                break
            
            logger.info(f"ğŸ“… å¼€å§‹å¤„ç†{year}å¹´ ({years.index(year) + 1}/{len(years)})")
            stats = self.migrate_single_year(year)
            self.stats.append(stats)
            
            # å¹´ä»½é—´æš‚åœï¼Œè®©ç³»ç»Ÿä¼‘æ¯
            if not should_stop:
                logger.info("â³ å¹´ä»½é—´æš‚åœ5ç§’...")
                time.sleep(5)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        total_time = time.time() - total_start_time
        self.generate_final_report(total_time)
    
    def generate_final_report(self, total_time: float):
        """ç”Ÿæˆæœ€ç»ˆè¿ç§»æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š 2015-2020å¹´Qdrant Cloudè¿ç§»æœ€ç»ˆæŠ¥å‘Š")
        logger.info("=" * 80)
        
        # ç»Ÿè®¡æ€»æ•°
        total_records = sum(s.records_processed for s in self.stats)
        total_chunks = sum(s.chunks_generated for s in self.stats)
        total_vectors_created = sum(s.vectors_created for s in self.stats)
        total_vectors_stored = sum(s.vectors_stored for s in self.stats)
        successful_years = len([s for s in self.stats if s.vectors_stored > 0])
        
        logger.info(f"ğŸ¯ æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"   æˆåŠŸå¹´ä»½: {successful_years}/{len(self.stats)}")
        logger.info(f"   å¤„ç†è®°å½•: {total_records:,}æ¡")
        logger.info(f"   ç”Ÿæˆchunks: {total_chunks:,}ä¸ª")
        logger.info(f"   åˆ›å»ºå‘é‡: {total_vectors_created:,}ä¸ª")
        logger.info(f"   å­˜å‚¨å‘é‡: {total_vectors_stored:,}ä¸ª")
        logger.info(f"   æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
        
        if total_time > 0:
            logger.info(f"   å¹³å‡é€Ÿåº¦: {total_vectors_created/total_time:.1f} å‘é‡/ç§’")
        
        logger.info(f"\nğŸ“‹ å„å¹´ä»½è¯¦æƒ…:")
        for stats in self.stats:
            status = "âœ…" if stats.vectors_stored > 0 else "âŒ"
            logger.info(f"   {status} {stats.year}å¹´: {stats.records_processed:,}è®°å½• â†’ {stats.chunks_generated:,}chunks â†’ {stats.vectors_stored:,}å‘é‡ ({stats.duration_minutes:.1f}åˆ†é’Ÿ)")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"migration_2015_2020_qdrant_report.md"
        self.save_detailed_report(report_file, total_time)
        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        logger.info(f"\nğŸ‰ 2015-2020å¹´æ•°æ®è¿ç§»å®Œæˆ!")
        logger.info(f"   Vector Database: Qdrant Cloud")  
        logger.info(f"   Embedding Model: BGE-M3 (æœ¬åœ°1024ç»´)")
        logger.info(f"   Total Cost: $0 (embeddingå…è´¹) + Qdrant Cloudè´¹ç”¨")
    
    def save_detailed_report(self, filename: str, total_time: float):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        total_records = sum(s.records_processed for s in self.stats)
        total_chunks = sum(s.chunks_generated for s in self.stats)
        total_vectors_created = sum(s.vectors_created for s in self.stats)
        total_vectors_stored = sum(s.vectors_stored for s in self.stats)
        successful_years = len([s for s in self.stats if s.vectors_stored > 0])
        
        content = f"""# 2015-2020å¹´æ•°æ®Qdrant Cloudè¿ç§»æŠ¥å‘Š

## è¿ç§»é…ç½®
- **å‘é‡æ•°æ®åº“**: Qdrant Cloud
- **Embeddingæ¨¡å‹**: BGE-M3 (æœ¬åœ°, 1024ç»´)
- **æ‰¹æ¬¡å‚æ•°**: batch_size=64, max_workers=4 (å†…å­˜ä¼˜åŒ–)
- **è¿ç§»æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## æ€»ä½“ç»“æœ
- **æˆåŠŸå¹´ä»½**: {successful_years}/{len(self.stats)}
- **å¤„ç†è®°å½•**: {total_records:,}æ¡
- **ç”Ÿæˆchunks**: {total_chunks:,}ä¸ª
- **åˆ›å»ºå‘é‡**: {total_vectors_created:,}ä¸ª
- **å­˜å‚¨å‘é‡**: {total_vectors_stored:,}ä¸ª
- **æ€»è€—æ—¶**: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)
- **å¹³å‡é€Ÿåº¦**: {total_vectors_created/total_time:.1f} å‘é‡/ç§’

## å„å¹´ä»½è¯¦æƒ…

| å¹´ä»½ | çŠ¶æ€ | è®°å½•æ•° | Chunks | å‘é‡æ•° | è€—æ—¶(åˆ†) |
|------|------|--------|--------|--------|----------|
"""
        
        for stats in self.stats:
            status = "âœ…" if stats.vectors_stored > 0 else "âŒ"
            content += f"| {stats.year} | {status} | {stats.records_processed:,} | {stats.chunks_generated:,} | {stats.vectors_stored:,} | {stats.duration_minutes:.1f} |\n"
        
        content += f"""

## æˆæœ¬åˆ†æ
- **BGE-M3 Embedding**: $0 (æœ¬åœ°å…è´¹)
- **Qdrant Cloud**: æŒ‰å®é™…ä½¿ç”¨é‡è®¡è´¹
- **æ€»ä½“æˆæœ¬**: è¿œä½äºOpenAIæ–¹æ¡ˆ

## æŠ€æœ¯éªŒè¯
âœ… BGE-M3æœ¬åœ°embeddingç¨³å®šè¿è¡Œ
âœ… å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆæœ‰æ•ˆ
âœ… Qdrant Cloudå­˜å‚¨æˆåŠŸ
âœ… å¤§è§„æ¨¡æ•°æ®å¤„ç†éªŒè¯é€šè¿‡

## ç»“è®º
2015-2020å¹´å¾·å›½è®®ä¼šæ•°æ®æˆåŠŸè¿ç§»åˆ°Qdrant Cloudï¼Œ
ä½¿ç”¨BGE-M3æœ¬åœ°embeddingå®ç°äº†é«˜æ€§èƒ½ã€ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="2015-2020å¹´æ•°æ®è¿ç§»åˆ°Qdrant Cloud")
    parser.add_argument("--start-year", type=int, default=2015, help="å¼€å§‹å¹´ä»½")
    parser.add_argument("--end-year", type=int, default=2020, help="ç»“æŸå¹´ä»½")
    parser.add_argument("--single-year", type=int, help="åªå¤„ç†å•ä¸ªå¹´ä»½")
    
    args = parser.parse_args()
    
    migrator = Migrate2015To2020()
    
    try:
        if args.single_year:
            logger.info(f"ğŸ¯ å•å¹´ä»½æ¨¡å¼: {args.single_year}")
            stats = migrator.migrate_single_year(args.single_year)
            logger.info(f"ğŸ‰ {args.single_year}å¹´è¿ç§»å®Œæˆ")
        else:
            logger.info(f"ğŸš€ æ‰¹é‡æ¨¡å¼: {args.start_year}-{args.end_year}")
            migrator.migrate_all_years(args.start_year, args.end_year)
            
        return 0
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­è¿ç§»")
        return 1
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å¤±è´¥: {str(e)}")
        return 1

if __name__ == "__main__":
    exit(main())
