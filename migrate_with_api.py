#!/usr/bin/env python3
"""
ä½¿ç”¨äº‘ç«¯APIçš„å¾·å›½è®®ä¼šæ•°æ®è¿ç§»è„šæœ¬
ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œç»•è¿‡GPUæ¨¡å‹åŠ è½½é—®é¢˜
"""

import sys
import os
import json
import time
from pathlib import Path
from typing import List, Dict, Any
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

from src.utils.logger import setup_logger
from src.vectordb.qdrant_client import create_qdrant_client
from src.llm.embeddings import GeminiEmbeddingClient
from src.data_loader.splitter import ParliamentTextSplitter

logger = setup_logger()

class CloudAPIMigrator:
    """ä½¿ç”¨äº‘ç«¯APIçš„è¿ç§»å™¨"""
    
    def __init__(self, collection_name: str = "german_parliament"):
        self.collection_name = collection_name
        
        # è®¾ç½®ä¿å®ˆå‚æ•°
        self.embedding_batch_size = 50
        self.qdrant_batch_size = 25
        
        logger.info("ğŸ”— åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯...")
        os.environ["QDRANT_MODE"] = "local"
        os.environ["QDRANT_LOCAL_PATH"] = "./data/qdrant"
        self.qdrant_client = create_qdrant_client()
        
        logger.info("ğŸŒ åˆå§‹åŒ–äº‘ç«¯embeddingå®¢æˆ·ç«¯...")
        # ä½¿ç”¨äº‘ç«¯APIè€Œä¸æ˜¯æœ¬åœ°GPUæ¨¡å‹
        self.embedding_client = GeminiEmbeddingClient(
            api_key=os.getenv("GEMINI_API_KEY"),
            embedding_mode="api"  # ä½¿ç”¨APIæ¨¡å¼
        )
        
        logger.info("ğŸ§  åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨...")
        self.text_splitter = ParliamentTextSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
    
    def migrate_single_year(self, year: int, data_file: Path) -> bool:
        """è¿ç§»å•ä¸ªå¹´ä»½çš„æ•°æ®ï¼ˆäº‘ç«¯APIç‰ˆï¼‰"""
        
        logger.info(f"ğŸš€ å¼€å§‹è¿ç§»{year}å¹´æ•°æ®: {data_file}")
        start_time = time.time()
        
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            logger.info(f"[{year}] ğŸ“‹ ç¡®ä¿Qdranté›†åˆå­˜åœ¨...")
            self.qdrant_client.create_collection_for_german_parliament(self.collection_name)
            
            # è¯»å–JSONæ•°æ®
            logger.info(f"[{year}] ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶...")
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            transcript = data.get('transcript', [])
            logger.info(f"[{year}] ğŸ“Š åŸå§‹è®°å½•æ•°: {len(transcript)}")
            
            if not transcript:
                logger.warning(f"[{year}] âš ï¸  æ— æ•°æ®ï¼Œè·³è¿‡")
                return True
            
            # æ–‡æœ¬åˆ†å—
            logger.info(f"[{year}] âœ‚ï¸  æ–‡æœ¬åˆ†å—å¤„ç†...")
            all_chunks = []
            valid_records = 0
            
            for i, record in enumerate(transcript):
                if not isinstance(record, dict):
                    continue
                
                speech_text = record.get('speech', '').strip()
                if not speech_text or len(speech_text) < 10:
                    continue
                
                valid_records += 1
                
                # åˆ†å—
                chunks = self.text_splitter.split_text(speech_text)
                
                # æ„å»ºå…ƒæ•°æ®
                metadata = record.get('metadata', {})
                base_metadata = {
                    "year": metadata.get('year', year),
                    "month": metadata.get('month', ''),
                    "day": metadata.get('day', ''),
                    "speaker": metadata.get('speaker', ''),
                    "party": metadata.get('party', ''),
                    "text_id": metadata.get('id', f"{year}_{i}")
                }
                
                # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ¡ç›®
                for j, chunk in enumerate(chunks):
                    chunk_metadata = base_metadata.copy()
                    chunk_metadata.update({
                        "chunk_index": j,
                        "total_chunks": len(chunks)
                    })
                    
                    all_chunks.append({
                        "text": chunk,
                        "metadata": chunk_metadata
                    })
            
            logger.info(f"[{year}] ğŸ“Š æœ‰æ•ˆè®°å½•: {valid_records}, æ€»chunks: {len(all_chunks)}")
            
            # åˆ†æ‰¹å¤„ç†
            total_chunks = len(all_chunks)
            processed = 0
            
            for i in range(0, total_chunks, self.embedding_batch_size):
                batch_chunks = all_chunks[i:i + self.embedding_batch_size]
                batch_num = i // self.embedding_batch_size + 1
                
                logger.info(f"[{year}] ğŸŒ å¤„ç†æ‰¹æ¬¡ {batch_num}, å¤§å°: {len(batch_chunks)}")
                
                # ç”Ÿæˆembedding (ä½¿ç”¨äº‘ç«¯API)
                try:
                    chunks_with_embeddings = self.embedding_client.embed_chunks(
                        batch_chunks,
                        text_key="text",
                        batch_size=self.embedding_batch_size,
                        max_workers=3,  # é™ä½å¹¶å‘æ•°
                        request_delay=2.0  # å¢åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    )
                    logger.info(f"[{year}] âœ… æ‰¹æ¬¡ {batch_num} embeddingå®Œæˆ")
                except Exception as e:
                    logger.error(f"[{year}] âŒ æ‰¹æ¬¡ {batch_num} embeddingå¤±è´¥: {str(e)}")
                    continue
                
                # å‡†å¤‡Qdrantæ•°æ®ç‚¹
                points_to_upsert = []
                point_id = int(time.time() * 1000000) + i
                
                for chunk_data in chunks_with_embeddings:
                    vector = chunk_data.get("vector")
                    if vector is None or len(vector) == 0:
                        continue
                    
                    # éªŒè¯å‘é‡
                    if any(not isinstance(x, (int, float)) or x != x for x in vector):
                        continue
                    
                    payload = chunk_data.get("metadata", {})
                    payload["text"] = chunk_data.get("text", "")
                    
                    points_to_upsert.append({
                        "id": point_id,
                        "vector": vector,
                        "payload": payload
                    })
                    point_id += 1
                
                # æ‰¹é‡æ’å…¥åˆ°Qdrant
                if points_to_upsert:
                    try:
                        self.qdrant_client.upsert_german_parliament_data(
                            collection_name=self.collection_name,
                            data_points=points_to_upsert
                        )
                        logger.info(f"[{year}] âœ… æ‰¹æ¬¡ {batch_num} Qdrantæ’å…¥å®Œæˆ")
                    except Exception as e:
                        logger.error(f"[{year}] âŒ æ‰¹æ¬¡ {batch_num} Qdrantæ’å…¥å¤±è´¥: {str(e)}")
                        continue
                
                processed += len(batch_chunks)
                progress = (processed / total_chunks) * 100
                logger.info(f"[{year}] ğŸ“Š è¿›åº¦: {progress:.1f}% ({processed}/{total_chunks})")
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
            
            duration = time.time() - start_time
            logger.info(f"[{year}] âœ… è¿ç§»å®Œæˆ: {processed}ä¸ªchunks, è€—æ—¶: {duration:.1f}ç§’")
            return True
            
        except Exception as e:
            logger.error(f"[{year}] âŒ è¿ç§»å¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="äº‘ç«¯APIå¾·å›½è®®ä¼šæ•°æ®è¿ç§»è„šæœ¬")
    parser.add_argument("--year", type=int, required=True, help="è¦è¿ç§»çš„å¹´ä»½")
    parser.add_argument("--data-dir", type=str, default="./data/pp_json_49-21", help="æ•°æ®ç›®å½•")
    
    args = parser.parse_args()
    
    logger.info("ğŸŒ å¯åŠ¨äº‘ç«¯APIè¿ç§»è„šæœ¬")
    logger.info(f"   ç›®æ ‡å¹´ä»½: {args.year}")
    logger.info(f"   æ•°æ®ç›®å½•: {args.data_dir}")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("GEMINI_API_KEY"):
        logger.error("âŒ GEMINI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return False
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_dir = Path(args.data_dir)
    
    if args.year == 2021:
        data_file = data_dir / "pp_2021_merged.json"
        if not data_file.exists():
            data_file = data_dir / "pp_2021.json"
    else:
        data_file = data_dir / f"pp_{args.year}.json"
    
    if not data_file.exists():
        logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    # å¼€å§‹è¿ç§»
    migrator = CloudAPIMigrator()
    success = migrator.migrate_single_year(args.year, data_file)
    
    if success:
        logger.info(f"ğŸ‰ {args.year}å¹´æ•°æ®è¿ç§»æˆåŠŸå®Œæˆï¼")
        return True
    else:
        logger.error(f"âŒ {args.year}å¹´æ•°æ®è¿ç§»å¤±è´¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
