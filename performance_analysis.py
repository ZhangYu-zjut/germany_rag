#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†æ€§èƒ½åˆ†æè„šæœ¬
è¯¦ç»†åˆ†æè¿ç§»è¿‡ç¨‹ä¸­æ¯ä¸ªç¯èŠ‚çš„è€—æ—¶
"""

import json
import time
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.data_loader.splitter import ParliamentTextSplitter
from src.vectordb.qdrant_client import create_qdrant_client
from src.llm.embeddings import GeminiEmbeddingClient

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, sample_size: int = 1000):
        self.sample_size = sample_size
        self.timings = {}
        
        print(f"ğŸ” åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨ï¼ˆæ ·æœ¬å¤§å°: {sample_size}ï¼‰")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        self.embedding_client = GeminiEmbeddingClient(embedding_mode="local")
        self.qdrant_client = create_qdrant_client(location="./performance_test_qdrant")
        
    def start_timer(self, name: str):
        """å¼€å§‹è®¡æ—¶"""
        self.timings[name] = {"start": time.time()}
        
    def end_timer(self, name: str):
        """ç»“æŸè®¡æ—¶"""
        if name in self.timings:
            self.timings[name]["end"] = time.time()
            self.timings[name]["duration"] = self.timings[name]["end"] - self.timings[name]["start"]
            
    def analyze_json_parsing(self, file_path: str):
        """åˆ†æJSONè§£ææ€§èƒ½"""
        print("\nğŸ“ æ­¥éª¤1ï¼šJSONè§£æå’Œæ–‡æœ¬é¢„å¤„ç†")
        print("-" * 40)
        
        self.start_timer("json_parsing")
        
        # JSONæ–‡ä»¶è¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        transcript = data.get('transcript', [])
        sample_records = transcript[:self.sample_size]
        
        self.end_timer("json_parsing")
        
        print(f"âœ… JSONè§£æå®Œæˆ")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / (1024*1024):.1f} MB")
        print(f"   - æ€»è®°å½•æ•°: {len(transcript):,}")
        print(f"   - æ ·æœ¬è®°å½•æ•°: {len(sample_records):,}")
        print(f"   - è§£æè€—æ—¶: {self.timings['json_parsing']['duration']:.3f}ç§’")
        
        return sample_records
        
    def analyze_text_chunking(self, records: List[Dict]):
        """åˆ†ææ–‡æœ¬åˆ†å—æ€§èƒ½"""
        print("\nâœ‚ï¸ æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—å¤„ç†")
        print("-" * 40)
        
        self.start_timer("text_chunking")
        
        all_chunks = []
        total_chars = 0
        valid_records = 0
        
        for record in records:
            if not isinstance(record, dict):
                continue
                
            text_content = record.get('speech', '')
            if not text_content or len(text_content.strip()) < 50:
                continue
                
            # æ–‡æœ¬åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(text_content)
            valid_chunks = [chunk for chunk in chunks if len(chunk.strip()) >= 30]
            
            for chunk in valid_chunks:
                chunk_data = {
                    "text": chunk,
                    "metadata": record.get("metadata", {})
                }
                all_chunks.append(chunk_data)
            
            total_chars += len(text_content)
            valid_records += 1
            
        self.end_timer("text_chunking")
        
        print(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆ")
        print(f"   - æœ‰æ•ˆè®°å½•æ•°: {valid_records:,}")
        print(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"   - ç”Ÿæˆchunksæ•°: {len(all_chunks):,}")
        print(f"   - å¹³å‡æ¯æ¡è®°å½•chunksæ•°: {len(all_chunks)/valid_records:.1f}")
        print(f"   - åˆ†å—è€—æ—¶: {self.timings['text_chunking']['duration']:.3f}ç§’")
        print(f"   - åˆ†å—é€Ÿåº¦: {len(all_chunks)/self.timings['text_chunking']['duration']:.1f} chunks/ç§’")
        
        return all_chunks
        
    def analyze_data_validation(self, chunks: List[Dict]):
        """åˆ†ææ•°æ®éªŒè¯å’Œè¿‡æ»¤æ€§èƒ½"""
        print("\nğŸ” æ­¥éª¤3ï¼šæ•°æ®éªŒè¯å’Œè¿‡æ»¤")
        print("-" * 40)
        
        self.start_timer("data_validation")
        
        valid_chunks = []
        filtered_count = 0
        
        for chunk_data in chunks:
            # éªŒè¯æ–‡æœ¬å†…å®¹
            text = chunk_data["text"]
            if len(text.strip()) < 30:
                filtered_count += 1
                continue
                
            # éªŒè¯å…ƒæ•°æ®
            metadata = chunk_data["metadata"]
            if not metadata.get("year") or not metadata.get("speaker"):
                filtered_count += 1
                continue
                
            # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
            cleaned_metadata = {
                "year": int(metadata.get("year", 0)) if metadata.get("year") else None,
                "month": int(metadata.get("month", 0)) if metadata.get("month") else None,
                "day": int(metadata.get("day", 0)) if metadata.get("day") else None,
                "speaker": metadata.get("speaker", "").strip(),
                "party": metadata.get("group", "").strip(),
                "session": metadata.get("session", "").strip(),
                "text": text.strip()
            }
            
            valid_chunks.append(cleaned_metadata)
            
        self.end_timer("data_validation")
        
        print(f"âœ… æ•°æ®éªŒè¯å®Œæˆ")
        print(f"   - è¾“å…¥chunksæ•°: {len(chunks):,}")
        print(f"   - æœ‰æ•ˆchunksæ•°: {len(valid_chunks):,}")
        print(f"   - è¿‡æ»¤chunksæ•°: {filtered_count:,}")
        print(f"   - æœ‰æ•ˆç‡: {len(valid_chunks)/len(chunks)*100:.1f}%")
        print(f"   - éªŒè¯è€—æ—¶: {self.timings['data_validation']['duration']:.3f}ç§’")
        print(f"   - éªŒè¯é€Ÿåº¦: {len(chunks)/self.timings['data_validation']['duration']:.1f} chunks/ç§’")
        
        return valid_chunks
        
    def analyze_embedding_generation(self, chunks: List[Dict], batch_size: int = 150):
        """åˆ†æembeddingç”Ÿæˆæ€§èƒ½"""
        print("\nğŸ§  æ­¥éª¤4ï¼šEmbeddingç”Ÿæˆ")
        print("-" * 40)
        
        self.start_timer("embedding_generation")
        
        texts_to_embed = [chunk["text"] for chunk in chunks]
        
        # æ‰¹é‡ç”Ÿæˆembedding
        vectors = self.embedding_client.embed_batch(
            texts_to_embed,
            batch_size=batch_size
        )
        
        self.end_timer("embedding_generation")
        
        print(f"âœ… Embeddingç”Ÿæˆå®Œæˆ")
        print(f"   - æ–‡æœ¬æ•°é‡: {len(texts_to_embed):,}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - ç”Ÿæˆè€—æ—¶: {self.timings['embedding_generation']['duration']:.3f}ç§’")
        print(f"   - ç”Ÿæˆé€Ÿåº¦: {len(texts_to_embed)/self.timings['embedding_generation']['duration']:.1f} embeddings/ç§’")
        
        return vectors
        
    def analyze_database_insertion(self, chunks: List[Dict], vectors: List[List[float]], batch_size: int = 100):
        """åˆ†ææ•°æ®åº“æ’å…¥æ€§èƒ½"""
        print("\nğŸ’¾ æ­¥éª¤5ï¼šQdrantæ•°æ®åº“æ‰¹é‡æ’å…¥")
        print("-" * 40)
        
        # æ¸…ç†æµ‹è¯•é›†åˆ
        collection_name = "performance_test"
        try:
            self.qdrant_client.delete_collection(collection_name)
        except:
            pass
            
        # åˆ›å»ºæµ‹è¯•é›†åˆ
        self.qdrant_client.create_collection_for_german_parliament(
            collection_name=collection_name,
            force_recreate=True
        )
        
        self.start_timer("db_insertion")
        
        # å‡†å¤‡æ•°æ®ç‚¹
        points_to_insert = []
        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
            point = {
                "id": i,
                "vector": vector,
                "payload": chunk
            }
            points_to_insert.append(point)
            
        # æ‰¹é‡æ’å…¥
        self.qdrant_client.upsert_german_parliament_data(
            collection_name=collection_name,
            data_points=points_to_insert,
            batch_size=batch_size
        )
        
        self.end_timer("db_insertion")
        
        print(f"âœ… æ•°æ®åº“æ’å…¥å®Œæˆ")
        print(f"   - æ•°æ®ç‚¹æ•°: {len(points_to_insert):,}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - æ’å…¥è€—æ—¶: {self.timings['db_insertion']['duration']:.3f}ç§’")
        print(f"   - æ’å…¥é€Ÿåº¦: {len(points_to_insert)/self.timings['db_insertion']['duration']:.1f} points/ç§’")
        
        # éªŒè¯æ’å…¥ç»“æœ
        try:
            collection_info = self.qdrant_client.get_collection_info(collection_name)
            print(f"   - éªŒè¯: é›†åˆä¸­æœ‰ {collection_info['points_count']} ä¸ªæ•°æ®ç‚¹")
        except Exception as e:
            print(f"   - éªŒè¯å¤±è´¥: {str(e)}")
            
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        total_time = sum(timing.get("duration", 0) for timing in self.timings.values())
        
        print(f"\nâ±ï¸  å„ç¯èŠ‚è€—æ—¶ç»Ÿè®¡:")
        print("-" * 40)
        
        step_names = {
            "json_parsing": "JSONè§£æå’Œé¢„å¤„ç†",
            "text_chunking": "æ–‡æœ¬åˆ†å—å¤„ç†", 
            "data_validation": "æ•°æ®éªŒè¯å’Œè¿‡æ»¤",
            "embedding_generation": "Embeddingç”Ÿæˆ",
            "db_insertion": "æ•°æ®åº“æ‰¹é‡æ’å…¥"
        }
        
        for step, timing in self.timings.items():
            duration = timing.get("duration", 0)
            percentage = (duration / total_time) * 100 if total_time > 0 else 0
            
            step_name = step_names.get(step, step)
            print(f"{step_name:<20}: {duration:>8.3f}ç§’ ({percentage:>5.1f}%)")
            
        print("-" * 40)
        print(f"{'æ€»è®¡':<20}: {total_time:>8.3f}ç§’ (100.0%)")
        
        # æ¨ç®—å…¨é‡æ•°æ®æ€§èƒ½
        sample_ratio = self.sample_size / 20000  # å‡è®¾å…¨é‡çº¦2ä¸‡æ¡è®°å½•
        estimated_full_time = total_time / sample_ratio
        
        print(f"\nğŸ”® å…¨é‡æ•°æ®æ€§èƒ½æ¨ç®—:")
        print(f"   - æ ·æœ¬æ¯”ä¾‹: {sample_ratio*100:.1f}% ({self.sample_size:,}/20,000)")
        print(f"   - æ¨ç®—å…¨é‡å¤„ç†æ—¶é—´: {estimated_full_time/60:.1f}åˆ†é’Ÿ")
        
        # è¯†åˆ«ä¸»è¦ç“¶é¢ˆ
        max_duration = 0
        bottleneck = ""
        for step, timing in self.timings.items():
            if timing.get("duration", 0) > max_duration:
                max_duration = timing.get("duration", 0)
                bottleneck = step_names.get(step, step)
                
        print(f"   - ğŸ¯ ä¸»è¦ç“¶é¢ˆ: {bottleneck}")
        
        return self.timings

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ•°æ®å¤„ç†æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    # ä½¿ç”¨è¾ƒå°æ ·æœ¬è¿›è¡Œå¿«é€Ÿåˆ†æ
    analyzer = PerformanceAnalyzer(sample_size=500)  # å‡å°‘æ ·æœ¬æ•°ä»¥å¿«é€Ÿåˆ†æ
    
    try:
        # åˆ†æ2019å¹´æ•°æ®
        data_file = "./data/pp_json_49-21/pp_2019.json"
        
        # æ­¥éª¤1: JSONè§£æ
        records = analyzer.analyze_json_parsing(data_file)
        
        # æ­¥éª¤2: æ–‡æœ¬åˆ†å—
        chunks = analyzer.analyze_text_chunking(records)
        
        # æ­¥éª¤3: æ•°æ®éªŒè¯
        validated_chunks = analyzer.analyze_data_validation(chunks)
        
        # æ­¥éª¤4: Embeddingç”Ÿæˆ
        vectors = analyzer.analyze_embedding_generation(validated_chunks[:100])  # åªæµ‹è¯•å‰100ä¸ª
        
        # æ­¥éª¤5: æ•°æ®åº“æ’å…¥  
        analyzer.analyze_database_insertion(validated_chunks[:100], vectors)
        
        # ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_performance_report()
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            analyzer.qdrant_client.delete_collection("performance_test")
            print(f"\nğŸ§¹ æµ‹è¯•æ•°æ®å·²æ¸…ç†")
        except:
            pass

if __name__ == "__main__":
    main()
