"""
ä¼˜åŒ–ç‰ˆPinecone Metadataæ‰¹é‡æ›´æ–°å™¨
ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åŠ é€Ÿï¼Œé¢„è®¡5-10åˆ†é’Ÿå®Œæˆ17ä¸‡å‘é‡çš„metadataæ›´æ–°
"""

import json
import os
import time
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from pinecone import Pinecone
from dotenv import load_dotenv
from loguru import logger

# é…ç½®logger
logger.remove()
logger.add(
    lambda msg: print(msg, end=''),
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level:8}</level> | <level>{message}</level>",
    level="INFO"
)

load_dotenv()


class OptimizedMetadataUpdater:
    """ä¼˜åŒ–ç‰ˆPinecone Metadataæ‰¹é‡æ›´æ–°å™¨ï¼ˆå¤šçº¿ç¨‹ï¼‰"""

    def __init__(self, max_workers: int = 20):
        """
        åˆå§‹åŒ–Pineconeè¿æ¥

        Args:
            max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤20
        """
        api_key = os.getenv('PINECONE_VECTOR_DATABASE_API_KEY')
        self.pc = Pinecone(api_key=api_key)
        self.index = self.pc.Index('german-bge')
        self.max_workers = max_workers

        # åŠ è½½åŸå§‹JSONæ•°æ®ç¼“å­˜
        self.data_cache = {}  # {year: {text_id: metadata}}

        logger.info(f"âœ… Pineconeè¿æ¥åˆå§‹åŒ–æˆåŠŸ (å¹¶å‘çº¿ç¨‹: {max_workers})")

    def load_year_data(self, year: int) -> Dict:
        """
        åŠ è½½æŸä¸€å¹´çš„åŸå§‹JSONæ•°æ®

        Returns:
            {text_id: metadata} çš„æ˜ å°„
        """
        if year in self.data_cache:
            return self.data_cache[year]

        json_file = f'data/pp_json_49-21/pp_{year}.json'

        if not os.path.exists(json_file):
            logger.warning(f"âš ï¸ {year}å¹´æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            return {}

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ„å»º text_id -> metadata æ˜ å°„
            text_id_map = {}
            if 'transcript' in data:
                for item in data['transcript']:
                    if item.get('type') == 'text_block':
                        text_id = item.get('text_id')
                        metadata = item.get('metadata', {})
                        if text_id and metadata:
                            text_id_map[text_id] = metadata

            self.data_cache[year] = text_id_map
            logger.info(f"ğŸ“š åŠ è½½{year}å¹´æ•°æ®: {len(text_id_map)}æ¡è®°å½•")

            return text_id_map

        except Exception as e:
            logger.error(f"âŒ åŠ è½½{year}å¹´æ•°æ®å¤±è´¥: {str(e)}")
            return {}

    def format_source_reference(self, metadata: Dict) -> str:
        """
        æ ¼å¼åŒ–source_reference: "id | speaker | year-month-day"
        """
        doc_id = metadata.get('id', 'unknown')
        speaker = metadata.get('speaker', 'Unknown')
        year = metadata.get('year', '0000')
        month = metadata.get('month', '01')
        day = metadata.get('day', '01')

        return f"{doc_id} | {speaker} | {year}-{month}-{day}"

    def update_single_vector(
        self,
        vector_id: str,
        original_text_id: str,
        year: int
    ) -> tuple[bool, str]:
        """
        æ›´æ–°å•ä¸ªå‘é‡çš„metadataï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        Args:
            vector_id: Pineconeå‘é‡ID
            original_text_id: åŸå§‹æ–‡æœ¬ID
            year: å¹´ä»½

        Returns:
            (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æŸ¥æ‰¾åŸå§‹metadata
            year_data = self.data_cache.get(year, {})

            if not year_data:
                return False, f"å¹´ä»½{year}æ•°æ®æœªåŠ è½½"

            if original_text_id not in year_data:
                return False, f"æœªæ‰¾åˆ°åŸå§‹metadata: {original_text_id}"

            original_meta = year_data[original_text_id]

            # æ„å»ºæ–°çš„metadataå­—æ®µï¼ˆåªæ›´æ–°4ä¸ªæ–°å­—æ®µï¼‰
            new_fields = {
                'month': original_meta.get('month', '01'),
                'day': original_meta.get('day', '01'),
                'id': original_meta.get('id', original_text_id),
                'source_reference': self.format_source_reference(original_meta)
            }

            # ä½¿ç”¨Pineconeçš„updateæ–¹æ³•æ›´æ–°metadata
            self.index.update(
                id=vector_id,
                set_metadata=new_fields
            )

            return True, ""

        except Exception as e:
            return False, str(e)

    def get_all_vectors_for_year(self, year: int) -> List[tuple]:
        """
        è·å–æŸå¹´æ‰€æœ‰å‘é‡çš„IDå’Œoriginal_text_id (ä½¿ç”¨list APIåˆ†é¡µè·å–)

        Returns:
            [(vector_id, original_text_id), ...]
        """
        logger.info(f"ğŸ“Š æŸ¥è¯¢{year}å¹´çš„æ‰€æœ‰å‘é‡...")

        all_vectors = []

        try:
            # ä½¿ç”¨list()æ–¹æ³•åˆ†é¡µè·å–æ‰€æœ‰å‘é‡ID
            # list()è¿”å›ä¸€ä¸ªgeneratorï¼Œæ¯æ¬¡yieldä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆ100ä¸ªIDï¼‰
            page_count = 0

            for id_batch in self.index.list(prefix=f'{year}_'):
                page_count += 1

                # id_batchæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚: ['2016_xxx_0', '2016_xxx_1', ...]
                if not id_batch:
                    break

                # Fetchè¿™æ‰¹å‘é‡çš„metadata
                fetch_result = self.index.fetch(ids=id_batch)

                # æå–original_text_id
                for vec_id in id_batch:
                    if vec_id in fetch_result.vectors:
                        vec = fetch_result.vectors[vec_id]
                        original_text_id = vec.metadata.get('original_text_id')

                        if original_text_id:
                            all_vectors.append((vec_id, original_text_id))

                # æ¯10é¡µæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if page_count % 10 == 0:
                    logger.info(f"  å·²è·å– {page_count} é¡µï¼Œå…± {len(all_vectors)} ä¸ªå‘é‡...")

            logger.info(f"âœ… {year}å¹´å…±æ‰¾åˆ° {len(all_vectors)} ä¸ªå‘é‡ (åˆ†{page_count}é¡µ)")
            return all_vectors

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢{year}å¹´å‘é‡å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def update_year_metadata_parallel(self, year: int) -> Dict:
        """
        ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ›´æ–°æŸä¸€å¹´æ‰€æœ‰å‘é‡çš„metadata

        Args:
            year: å¹´ä»½

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”§ å¼€å§‹æ›´æ–°{year}å¹´metadata (å¹¶å‘æ¨¡å¼)")
        logger.info(f"{'='*60}")

        start_time = time.time()

        # 1. é¢„åŠ è½½è¯¥å¹´ä»½çš„åŸå§‹æ•°æ®
        year_data = self.load_year_data(year)
        if not year_data:
            logger.warning(f"âš ï¸ {year}å¹´æ— æ•°æ®ï¼Œè·³è¿‡")
            return {"year": year, "updated": 0, "failed": 0, "total": 0}

        # 2. è·å–æ‰€æœ‰å‘é‡
        vectors = self.get_all_vectors_for_year(year)
        total_vectors = len(vectors)

        if total_vectors == 0:
            logger.warning(f"âš ï¸ {year}å¹´æ— å‘é‡æ•°æ®")
            return {"year": year, "updated": 0, "failed": 0, "total": 0}

        # 3. å¤šçº¿ç¨‹å¹¶å‘æ›´æ–°
        logger.info(f"ğŸš€ å¯åŠ¨{self.max_workers}ä¸ªå¹¶å‘çº¿ç¨‹è¿›è¡Œæ›´æ–°...")

        updated_count = 0
        failed_count = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_vector = {
                executor.submit(
                    self.update_single_vector,
                    vector_id,
                    original_text_id,
                    year
                ): (vector_id, original_text_id)
                for vector_id, original_text_id in vectors
            }

            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            for i, future in enumerate(as_completed(future_to_vector), 1):
                success, error_msg = future.result()

                if success:
                    updated_count += 1
                else:
                    failed_count += 1
                    if failed_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                        vector_id, _ = future_to_vector[future]
                        logger.debug(f"  æ›´æ–°å¤±è´¥ {vector_id}: {error_msg}")

                # æ¯100ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 100 == 0 or i == total_vectors:
                    progress = i / total_vectors * 100
                    logger.info(
                        f"  è¿›åº¦: {i}/{total_vectors} ({progress:.1f}%) - "
                        f"æˆåŠŸ: {updated_count}, å¤±è´¥: {failed_count}"
                    )

        elapsed_time = time.time() - start_time

        logger.info(f"\nâœ… {year}å¹´metadataæ›´æ–°å®Œæˆ")
        logger.info(f"   æ€»å‘é‡æ•°: {total_vectors}")
        logger.info(f"   æˆåŠŸæ›´æ–°: {updated_count}")
        logger.info(f"   å¤±è´¥: {failed_count}")
        logger.info(f"   è€—æ—¶: {elapsed_time:.1f}ç§’ ({elapsed_time/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"   é€Ÿåº¦: {total_vectors/elapsed_time:.1f} å‘é‡/ç§’")

        return {
            "year": year,
            "total": total_vectors,
            "updated": updated_count,
            "failed": failed_count,
            "time": elapsed_time
        }

    def verify_update(self, year: int, sample_size: int = 5):
        """
        éªŒè¯æ›´æ–°ç»“æœ

        Args:
            year: å¹´ä»½
            sample_size: æŠ½æ ·æ•°é‡
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ” éªŒè¯{year}å¹´metadataæ›´æ–°ç»“æœ")
        logger.info(f"{'='*60}")

        # æŸ¥è¯¢å‡ ä¸ªæ ·æœ¬
        dummy_vector = [0.0] * 1024
        results = self.index.query(
            vector=dummy_vector,
            top_k=sample_size,
            filter={'year': {'$eq': str(year)}},
            include_metadata=True
        )

        logger.info(f"\næŠ½æ ·æ£€æŸ¥ {len(results.matches)} ä¸ªå‘é‡:\n")

        success_count = 0
        for i, match in enumerate(results.matches, 1):
            meta = match.metadata

            month = meta.get('month')
            day = meta.get('day')
            doc_id = meta.get('id')
            source_ref = meta.get('source_reference')

            logger.info(f"æ ·æœ¬ {i}:")
            logger.info(f"  ID: {match.id[:40]}...")
            logger.info(f"  month: {month} (type: {type(month).__name__})")
            logger.info(f"  day: {day} (type: {type(day).__name__})")
            logger.info(f"  id: {doc_id}")
            logger.info(f"  source_reference: {source_ref[:60] if source_ref else 'None'}...")

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸæ›´æ–°
            if month and day and doc_id and source_ref:
                logger.info(f"  âœ… æ›´æ–°æˆåŠŸ\n")
                success_count += 1
            else:
                logger.warning(f"  âš ï¸ æ›´æ–°ä¸å®Œæ•´\n")

        logger.info(f"éªŒè¯æˆåŠŸç‡: {success_count}/{len(results.matches)} ({success_count/len(results.matches)*100:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*60)
    logger.info("Pinecone Metadataæ‰¹é‡æ›´æ–°å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info("="*60)
    logger.info("\nåŠŸèƒ½: ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ›´æ–°metadata")
    logger.info("ä¼˜åŠ¿: 5-10åˆ†é’Ÿå®Œæˆ17ä¸‡å‘é‡æ›´æ–°\n")

    # åˆ›å»ºæ›´æ–°å™¨ï¼ˆ20ä¸ªå¹¶å‘çº¿ç¨‹ï¼‰
    updater = OptimizedMetadataUpdater(max_workers=20)

    # è¦æ›´æ–°çš„å¹´ä»½
    years_to_update = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]

    total_start = time.time()
    all_stats = []

    for year in years_to_update:
        stats = updater.update_year_metadata_parallel(year)
        all_stats.append(stats)

        # éªŒè¯å‰3å¹´çš„æ›´æ–°ç»“æœ
        if year <= 2017:
            # ç­‰å¾…3ç§’è®©PineconeåŒæ­¥
            logger.info(f"\nâ³ ç­‰å¾…3ç§’è®©PineconeåŒæ­¥æ•°æ®...")
            time.sleep(3)
            updater.verify_update(year, sample_size=3)

    total_time = time.time() - total_start

    # æ€»ç»“
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š æ‰€æœ‰å¹´ä»½æ›´æ–°å®Œæˆ")
    logger.info(f"{'='*60}\n")

    total_updated = sum(s['updated'] for s in all_stats)
    total_failed = sum(s['failed'] for s in all_stats)
    total_vectors = sum(s['total'] for s in all_stats)

    logger.info(f"æ€»å‘é‡æ•°: {total_vectors:,}")
    logger.info(f"æˆåŠŸæ›´æ–°: {total_updated:,}")
    logger.info(f"å¤±è´¥: {total_failed:,}")
    logger.info(f"æˆåŠŸç‡: {total_updated/total_vectors*100:.2f}%")
    logger.info(f"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    logger.info(f"å¹³å‡é€Ÿåº¦: {total_vectors/total_time:.1f} å‘é‡/ç§’")
    logger.info(f"\nå¹´ä»½è¯¦æƒ…:")

    for stats in all_stats:
        logger.info(f"  {stats['year']}: {stats['updated']}/{stats['total']} æˆåŠŸ ({stats['time']:.1f}ç§’)")


if __name__ == "__main__":
    main()
