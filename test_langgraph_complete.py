#!/usr/bin/env python3
"""
åŸºäºLangGraphå®Œæ•´å·¥ä½œæµçš„å¤šå¹´ä»½RAGæµ‹è¯•
ä¼˜åŒ–ç‰ˆæœ¬ - è§£å†³å‚æ•°æå–ã€æ£€ç´¢ç­–ç•¥ã€å¯è§‚æµ‹æ€§é—®é¢˜

æ”¯æŒå¾·è¯­å’Œä¸­æ–‡ä¸¤ç§æ¨¡å¼ï¼š
  python test_langgraph_complete.py              # é»˜è®¤å¾·è¯­
  python test_langgraph_complete.py --language chinese  # ä¸­æ–‡æ¨¡å¼
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent
sys.path.append(str(project_root))
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger

logger = setup_logger()

# 7ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆå¾·è¯­ç‰ˆï¼‰
TEST_QUESTIONS_DE = [
    {
        "id": 1,
        "question": "Bitte fassen Sie die wichtigsten VerÃ¤nderungen in der FlÃ¼chtlingspolitik der CDU/CSU seit 2015 zusammen.",
        "type": "å¤šå¹´å˜åŒ–åˆ†æ",
        "years": "2015-2024",
        "expected_years": list(range(2015, 2025))
    },
    {
        "id": 2,
        "question": "Welche Positionen vertraten die verschiedenen Parteien im Deutschen Bundestag 2017 zur Reform des FachkrÃ¤fteeinwanderungsgesetzes?",
        "type": "å•å¹´å¤šå…šæ´¾å¯¹æ¯”",
        "years": "2017",
        "expected_years": [2017]
    },
    {
        "id": 3,
        "question": "Was waren die Hauptpositionen und Forderungen der GrÃ¼nen zur Migrationsfrage im Deutschen Bundestag 2015?",
        "type": "å•å¹´å•å…šæ´¾è§‚ç‚¹",
        "years": "2015",
        "expected_years": [2015]
    },
    {
        "id": 4,
        "question": "Wie haben sich die Diskussionen der verschiedenen Parteien im Deutschen Bundestag Ã¼ber die FamilienzusammenfÃ¼hrung von FlÃ¼chtlingen zwischen 2015 und 2018 entwickelt?",
        "type": "è·¨å¹´å¤šå…šæ´¾å˜åŒ–",
        "years": "2015-2018",
        "expected_years": list(range(2015, 2019))
    },
    {
        "id": 5,
        "question": "Bitte vergleichen Sie die Positionen der Unionsparteien und der GrÃ¼nen zur Integrationspolitik zwischen 2015 und 2017.",
        "type": "è·¨å¹´ä¸¤å…šå¯¹æ¯”",
        "years": "2015-2017",
        "expected_years": list(range(2015, 2018))
    },
    {
        "id": 6,
        "question": "Wie haben sich die Positionen der CDU/CSU zur Migrationspolitik zwischen 2017 und 2019 im Vergleich verÃ¤ndert?",
        "type": "ç¦»æ•£å¹´ä»½å¯¹æ¯”",
        "years": "2017, 2019",
        "expected_years": [2017, 2019]
    },
    {
        "id": 7,
        "question": "Welche wichtigen Ansichten und VorschlÃ¤ge vertrat die AfD zur FlÃ¼chtlingspolitik im Jahr 2018?",
        "type": "å•å¹´å•å…šæ´¾è§‚ç‚¹",
        "years": "2018",
        "expected_years": [2018]
    }
]

# 7ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆä¸­æ–‡ç‰ˆï¼‰
TEST_QUESTIONS_ZH = [
    {
        "id": 1,
        "question": "è¯·æ¦‚è¿°2015å¹´ä»¥æ¥å¾·å›½åŸºæ°‘ç›Ÿå¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºå‘ç”Ÿäº†å“ªäº›ä¸»è¦å˜åŒ–ã€‚",
        "type": "å¤šå¹´å˜åŒ–åˆ†æ",
        "years": "2015-2024",
        "expected_years": list(range(2015, 2025))
    },
    {
        "id": 2,
        "question": "2017å¹´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­å„å…šæ´¾å¯¹ä¸“ä¸šäººæ‰ç§»æ°‘åˆ¶åº¦æ”¹é©åˆ†åˆ«æŒä»€ä¹ˆç«‹åœºï¼Ÿ",
        "type": "å•å¹´å¤šå…šæ´¾å¯¹æ¯”",
        "years": "2017",
        "expected_years": [2017]
    },
    {
        "id": 3,
        "question": "2015å¹´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­ç»¿å…šåœ¨ç§»æ°‘å›½ç±é—®é¢˜ä¸Šçš„ä¸»è¦ç«‹åœºå’Œè¯‰æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
        "type": "å•å¹´å•å…šæ´¾è§‚ç‚¹",
        "years": "2015",
        "expected_years": [2015]
    },
    {
        "id": 4,
        "question": "åœ¨2015å¹´åˆ°2018å¹´æœŸé—´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­ä¸åŒå…šæ´¾åœ¨éš¾æ°‘å®¶åº­å›¢èšé—®é¢˜ä¸Šçš„è®¨è®ºå‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ï¼Ÿ",
        "type": "è·¨å¹´å¤šå…šæ´¾å˜åŒ–",
        "years": "2015-2018",
        "expected_years": list(range(2015, 2019))
    },
    {
        "id": 5,
        "question": "è¯·å¯¹æ¯”2015-2017å¹´è”ç›Ÿå…šä¸ç»¿å…šåœ¨ç§»æ°‘èåˆæ”¿ç­–æ–¹é¢çš„ä¸»å¼ ã€‚",
        "type": "è·¨å¹´ä¸¤å…šå¯¹æ¯”",
        "years": "2015-2017",
        "expected_years": list(range(2015, 2018))
    },
    {
        "id": 6,
        "question": "2019å¹´ä¸2017å¹´ç›¸æ¯”ï¼Œè”é‚¦è®®ä¼šå…³äºéš¾æ°‘é£è¿”çš„è®¨è®ºæœ‰ä½•å˜åŒ–ï¼Ÿ",
        "type": "ä¸¤å¹´å¯¹æ¯”",
        "years": "2017, 2019",
        "expected_years": [2017, 2019]
    },
    {
        "id": 7,
        "question": "æ–°å† ç–«æƒ…æœŸé—´ï¼ˆä¸»è¦æ˜¯2020å¹´ï¼‰ï¼Œè”é‚¦è®®é™¢å¯¹åšæŒæ°”å€™ç›®æ ‡çš„çœ‹æ³•å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿè¯·ä½¿ç”¨2019-2021å¹´çš„èµ„æ–™è¿›è¡Œå›ç­”ã€‚å¿…è¦æ—¶ç»™å‡ºå…·ä½“å¼•è¯­ã€‚",
        "type": "è·¨å¹´ç–«æƒ…å½±å“åˆ†æ",
        "years": "2019-2021",
        "expected_years": list(range(2019, 2022))
    }
]


def create_pinecone_workflow():
    """
    åˆ›å»ºä½¿ç”¨Pineconeçš„LangGraphå·¥ä½œæµ

    ä½¿ç”¨å¢å¼ºç‰ˆèŠ‚ç‚¹:
    - EnhancedExtractNode: æ”¯æŒæ—¶é—´è¯­ä¹‰ç†è§£
    - PineconeRetrieveNode: æ”¯æŒå¤šå¹´ä»½åˆ†å±‚æ£€ç´¢
    """
    from langgraph.graph import StateGraph, END
    from src.graph.state import GraphState
    from src.graph.nodes import ClassifyNode, ReRankNode
    from src.graph.nodes.intent_enhanced import EnhancedIntentNode
    from src.graph.nodes.extract_enhanced import EnhancedExtractNode
    from src.graph.nodes.decompose_enhanced import EnhancedDecomposeNode
    from src.graph.nodes.summarize_enhanced import EnhancedSummarizeNode
    from src.graph.nodes.exception_enhanced import EnhancedExceptionNode
    from src.graph.nodes.retrieve_pinecone import PineconeRetrieveNode
    from src.graph.nodes.query_expansion import QueryExpansionNode

    logger.info("[Workflow] åˆ›å»ºPineconeä¼˜åŒ–ç‰ˆå·¥ä½œæµ...")

    # ã€é‡è¦ã€‘å¯ç”¨ç”Ÿäº§æ¨¡å¼ä»¥è§¦å‘ä¸¤é˜¶æ®µé‡è¯•æœºåˆ¶
    from src.config import settings
    settings.production_mode = True
    logger.info("[Workflow] ğŸ”¥ å·²å¯ç”¨ç”Ÿäº§æ¨¡å¼ï¼ˆå«ä¸¤é˜¶æ®µé‡è¯•æœºåˆ¶ï¼‰")

    # åˆ›å»ºèŠ‚ç‚¹
    intent_node = EnhancedIntentNode()
    classify_node = ClassifyNode()
    extract_node = EnhancedExtractNode()  # å¢å¼ºç‰ˆ
    decompose_node = EnhancedDecomposeNode()
    query_expansion_node = QueryExpansionNode(expansion_count=5)  # Queryæ‰©å±•èŠ‚ç‚¹
    retrieve_node = PineconeRetrieveNode(
        top_k=50,  # æå‡åˆ°50
        enable_multi_year_strategy=True,
        limit_per_year=5,
        enable_concurrent=True  # å¯ç”¨å¹¶å‘æ£€ç´¢ï¼Œå¤§å¹…æé€Ÿ
    )
    rerank_node = ReRankNode()
    summarize_node = EnhancedSummarizeNode()
    exception_node = EnhancedExceptionNode()

    logger.info("[Workflow] èŠ‚ç‚¹åˆ›å»ºå®Œæˆ")

    # æ„å»ºå·¥ä½œæµå›¾
    workflow = StateGraph(GraphState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("intent_analysis", intent_node)
    workflow.add_node("classify", classify_node)
    workflow.add_node("extract", extract_node)
    workflow.add_node("decompose", decompose_node)
    workflow.add_node("query_expansion", query_expansion_node)  # Queryæ‰©å±•èŠ‚ç‚¹
    workflow.add_node("retrieve", retrieve_node)
    # workflow.add_node("rerank", rerank_node)  # ã€Phase 4ã€‘ç¦ç”¨ReRankï¼šCohereè¿‡æ»¤äº†BGE-M3çš„æœ€ä½³ç»“æœ
    workflow.add_node("summarize", summarize_node)
    workflow.add_node("exception", exception_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("intent_analysis")

    # è·¯ç”±å‡½æ•°
    def route_after_intent(state):
        if state.get("error"):
            return "exception"
        intent = state.get("intent")
        if intent == "complex":
            return "classify"
        else:
            return "extract"

    def route_after_classify(state):
        if state.get("error"):
            return "exception"
        return "extract"

    def route_after_extract(state):
        if state.get("error"):
            return "exception"
        is_decomposed = state.get("is_decomposed", False)
        if is_decomposed:
            return "decompose"
        else:
            return "retrieve"

    def route_after_decompose(state):
        if state.get("error"):
            return "exception"
        return "query_expansion"  # å…ˆè¿›è¡ŒQueryæ‰©å±•ï¼Œå†æ£€ç´¢

    def route_after_retrieve(state):
        """ã€Phase 4ä¿®æ”¹ã€‘ç›´æ¥è¿”å›summarizeï¼Œè·³è¿‡ReRank"""
        if state.get("error"):
            return "exception"
        no_material_found = state.get("no_material_found", False)
        if no_material_found:
            return "exception"
        else:
            return "summarize"  # ç›´æ¥åˆ°Summarizeï¼Œè·³è¿‡ReRank

    def route_after_rerank(state):
        if state.get("error"):
            return "exception"
        reranked_results = state.get("reranked_results", [])
        if not reranked_results:
            return "exception"
        else:
            return "summarize"

    # æ·»åŠ è·¯ç”±
    workflow.add_conditional_edges(
        "intent_analysis",
        route_after_intent,
        {"classify": "classify", "extract": "extract", "exception": "exception"}
    )
    workflow.add_conditional_edges(
        "classify",
        route_after_classify,
        {"extract": "extract", "exception": "exception"}
    )
    workflow.add_conditional_edges(
        "extract",
        route_after_extract,
        {"decompose": "decompose", "retrieve": "retrieve", "exception": "exception"}
    )
    workflow.add_conditional_edges(
        "decompose",
        route_after_decompose,
        {"query_expansion": "query_expansion", "exception": "exception"}
    )
    # Queryæ‰©å±•åç›´æ¥è¿›å…¥æ£€ç´¢
    workflow.add_edge("query_expansion", "retrieve")
    # ã€Phase 4ä¿®æ”¹ã€‘Retrieve -> Summarize (è·³è¿‡ReRank)
    # åŸå› ï¼šCohere ReRankè¿‡æ»¤æ‰äº†BGE-M3æ£€ç´¢æ’åç¬¬1çš„ç›®æ ‡æ–‡æ¡£
    workflow.add_conditional_edges(
        "retrieve",
        route_after_retrieve,
        {"summarize": "summarize", "exception": "exception"}
    )
    # workflow.add_conditional_edges(
    #     "rerank",
    #     route_after_rerank,
    #     {"summarize": "summarize", "exception": "exception"}
    # )  # ã€Phase 4ã€‘ç¦ç”¨ReRankèŠ‚ç‚¹åï¼Œä»rerankå‡ºå‘çš„edgesä¹Ÿè¦ç§»é™¤

    # æ·»åŠ ç»“æŸè¾¹
    workflow.add_edge("summarize", END)
    workflow.add_edge("exception", END)

    logger.info("[Workflow] å·¥ä½œæµå›¾æ„å»ºå®Œæˆ")

    return workflow.compile()


def test_one_question(workflow, question_data: dict, total_questions: int = 7):
    """æµ‹è¯•ä¸€ä¸ªé—®é¢˜"""
    from src.graph.state import create_initial_state

    qid = question_data['id']
    question = question_data['question']
    qtype = question_data['type']
    years = question_data['years']
    expected_years = question_data['expected_years']

    logger.info(f"\n{'='*100}")
    logger.info(f"ğŸ“ é—®é¢˜ {qid}/{total_questions}: {qtype} ({years})")
    logger.info(f"{'='*100}")
    logger.info(f"é—®é¢˜: {question}")
    logger.info(f"æœŸæœ›å¹´ä»½: {expected_years}")

    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = create_initial_state(question)

    # è¿è¡Œå·¥ä½œæµ
    start_time = time.time()
    try:
        final_state = workflow.invoke(initial_state)
        total_time = time.time() - start_time

        logger.info(f"\n{'='*100}")
        logger.info(f"âœ… é—®é¢˜ {qid} å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"{'='*100}")

        # æ”¶é›†ç»“æœ
        result = {
            "question_id": qid,
            "question": question,
            "type": qtype,
            "years": years,
            "expected_years": expected_years,
            "total_time": total_time,

            # å†…éƒ¨æ€è€ƒè¿‡ç¨‹
            "intent": final_state.get("intent"),
            "question_type": final_state.get("question_type"),
            "parameters": final_state.get("parameters", {}),
            "extraction_thinking": final_state.get("extraction_thinking", ""),
            "retrieval_thinking": final_state.get("retrieval_thinking", ""),

            # æ£€ç´¢ä¿¡æ¯
            "retrieval_results": final_state.get("retrieval_results", []),
            "overall_year_distribution": final_state.get("overall_year_distribution", {}),
            "reranked_results": final_state.get("reranked_results", []),

            # æœ€ç»ˆç­”æ¡ˆ
            "final_answer": final_state.get("final_answer", ""),
            "error": final_state.get("error"),

            # å­é—®é¢˜ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            "sub_questions": final_state.get("sub_questions"),
            "sub_answers": final_state.get("sub_answers"),
        }

        # éªŒè¯å¹´ä»½è¦†ç›–
        actual_years = list(final_state.get("overall_year_distribution", {}).keys())
        missing_years = [y for y in expected_years if str(y) not in actual_years]
        if missing_years:
            logger.warning(f"âš ï¸ ç¼ºå¤±å¹´ä»½: {missing_years}")
            result["missing_years"] = missing_years

        # æ‰“å°å…³é”®ä¿¡æ¯
        logger.info(f"\n=== é—®é¢˜ {qid} ç»“æœæ‘˜è¦ ===")
        logger.info(f"æ„å›¾: {result['intent']}")
        logger.info(f"ç±»å‹: {result['question_type']}")
        logger.info(f"æå–å‚æ•°: {json.dumps(result['parameters'], ensure_ascii=False)}")
        logger.info(f"å¹´ä»½åˆ†å¸ƒ: {result['overall_year_distribution']}")
        logger.info(f"ç­”æ¡ˆé•¿åº¦: {len(result['final_answer'])} å­—ç¬¦")

        if result.get("sub_questions"):
            logger.info(f"å­é—®é¢˜æ•°: {len(result['sub_questions'])}")

        # æ‰“å°å®Œæ•´ç­”æ¡ˆ
        logger.info(f"\n{'='*100}")
        logger.info(f"ğŸ“„ é—®é¢˜ {qid} å®Œæ•´ç­”æ¡ˆ:")
        logger.info(f"{'='*100}")
        logger.info(result['final_answer'])
        logger.info(f"{'='*100}\n")

        # ğŸ†• ç”Ÿæˆå®Œæ•´å¼•ç”¨æŠ¥å‘Š
        try:
            from generate_full_ref_report import FullRefReportGenerator
            generator = FullRefReportGenerator(output_dir="outputs")
            report_dir = generator.generate_report(final_state, question_id=f"Q{qid}")
            logger.info(f"[FullRef] âœ… å®Œæ•´å¼•ç”¨æŠ¥å‘Šå·²ç”Ÿæˆ: {report_dir}")
            result['report_dir'] = str(report_dir)
        except Exception as e:
            logger.warning(f"[FullRef] âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

        return result

    except Exception as e:
        logger.error(f"âŒ é—®é¢˜ {qid} æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        return {
            "question_id": qid,
            "question": question,
            "type": qtype,
            "error": str(e),
            "total_time": time.time() - start_time
        }


def generate_markdown_report(results: list, output_file: Path):
    """ç”Ÿæˆæ ¼å¼åŒ–çš„MarkdownæŠ¥å‘Š"""
    report_lines = []

    report_lines.append("# å¤šå¹´ä»½RAGç³»ç»Ÿå®Œæ•´æµ‹è¯•æŠ¥å‘Š")
    report_lines.append(f"\n**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"\n**æµ‹è¯•é—®é¢˜æ•°**: {len(results)}")
    report_lines.append(f"\n---\n")

    # æ€»è§ˆè¡¨æ ¼
    report_lines.append("## æµ‹è¯•æ€»è§ˆ\n")
    report_lines.append("| ID | ç±»å‹ | æœŸæœ›å¹´ä»½ | å®é™…å¹´ä»½ | è€—æ—¶(ç§’) | çŠ¶æ€ |")
    report_lines.append("|----|------|----------|----------|----------|------|")

    for r in results:
        qid = r['question_id']
        qtype = r['type']
        expected = r.get('expected_years', [])
        actual = list(r.get('overall_year_distribution', {}).keys())
        duration = r.get('total_time', 0)
        status = "âŒé”™è¯¯" if r.get('error') else "âœ…æˆåŠŸ"

        report_lines.append(
            f"| Q{qid} | {qtype} | {len(expected)}å¹´ | {len(actual)}å¹´ | {duration:.2f} | {status} |"
        )

    report_lines.append("\n---\n")

    # æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†ç»“æœ
    for r in results:
        qid = r['question_id']
        question = r['question']
        qtype = r['type']

        report_lines.append(f"## é—®é¢˜ {qid}: {qtype}\n")
        report_lines.append(f"**é—®é¢˜**: {question}\n")

        if r.get('error'):
            report_lines.append(f"**é”™è¯¯**: {r['error']}\n")
            report_lines.append("---\n")
            continue

        # å‚æ•°æå–
        report_lines.append("### å‚æ•°æå–\n")
        report_lines.append("```json")
        report_lines.append(json.dumps(r.get('parameters', {}), ensure_ascii=False, indent=2))
        report_lines.append("```\n")

        # æ£€ç´¢ä¿¡æ¯
        report_lines.append("### æ£€ç´¢ä¿¡æ¯\n")
        report_lines.append(f"- **æ„å›¾**: {r.get('intent')}")
        report_lines.append(f"- **é—®é¢˜ç±»å‹**: {r.get('question_type')}")
        report_lines.append(f"- **å¹´ä»½åˆ†å¸ƒ**: {r.get('overall_year_distribution', {})}")
        report_lines.append(f"- **æ£€ç´¢æ–‡æ¡£æ•°**: {sum(len(rr.get('chunks', [])) for rr in r.get('retrieval_results', []))}")
        report_lines.append(f"- **ReRankåæ–‡æ¡£æ•°**: {len(r.get('reranked_results', []))}\n")

        # å†…éƒ¨æ€è€ƒè¿‡ç¨‹
        if r.get('extraction_thinking'):
            report_lines.append("### å‚æ•°æå–æ€è€ƒè¿‡ç¨‹\n")
            report_lines.append("```")
            report_lines.append(r['extraction_thinking'])
            report_lines.append("```\n")

        if r.get('retrieval_thinking'):
            report_lines.append("### æ£€ç´¢æ€è€ƒè¿‡ç¨‹\n")
            report_lines.append("```")
            report_lines.append(r['retrieval_thinking'])
            report_lines.append("```\n")

        # å­é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        if r.get('sub_questions'):
            report_lines.append("### å­é—®é¢˜æ‹†è§£\n")
            for i, sq in enumerate(r['sub_questions'], 1):
                report_lines.append(f"{i}. {sq}")
            report_lines.append("")

        # æœ€ç»ˆç­”æ¡ˆ
        report_lines.append("### æœ€ç»ˆç­”æ¡ˆ\n")
        report_lines.append(r.get('final_answer', 'æ— ç­”æ¡ˆ'))
        report_lines.append("\n---\n")

    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(report_lines))

    logger.info(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='å¾·å›½è®®ä¼šRAGç³»ç»Ÿæµ‹è¯• - æ”¯æŒå¾·è¯­å’Œä¸­æ–‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python test_langgraph_complete.py                    # å¾·è¯­æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
  python test_langgraph_complete.py --language chinese # ä¸­æ–‡æ¨¡å¼
  python test_langgraph_complete.py --language german  # å¾·è¯­æ¨¡å¼ï¼ˆæ˜¾å¼ï¼‰
        """
    )
    parser.add_argument(
        '--language',
        choices=['german', 'chinese', 'de', 'zh'],
        default='german',
        help='é€‰æ‹©è¯­è¨€æ¨¡å¼ï¼šgerman/de (å¾·è¯­ï¼Œé»˜è®¤) æˆ– chinese/zh (ä¸­æ–‡)'
    )
    args = parser.parse_args()

    # ç¡®å®šä½¿ç”¨çš„é—®é¢˜é›†
    if args.language in ['chinese', 'zh']:
        TEST_QUESTIONS = TEST_QUESTIONS_ZH
        language_name = "ä¸­æ–‡"
        logger.info("ğŸŒ è¯­è¨€æ¨¡å¼: ä¸­æ–‡")
    else:
        TEST_QUESTIONS = TEST_QUESTIONS_DE
        language_name = "Deutsch (å¾·è¯­)"
        logger.info("ğŸ‡©ğŸ‡ª è¯­è¨€æ¨¡å¼: Deutsch (å¾·è¯­)")

    logger.info("="*100)
    logger.info(f"ğŸš€ å¤šå¹´ä»½RAGç³»ç»Ÿå®Œæ•´æµ‹è¯• (LangGraphä¼˜åŒ–ç‰ˆ) - {language_name}")
    logger.info("="*100)

    logger.info("\nä¼˜åŒ–é¡¹:")
    logger.info("  1. âœ… ä½¿ç”¨LangGraphå®Œæ•´å·¥ä½œæµ")
    logger.info("  2. âœ… å•å¹´é’ˆå¯¹æ€§æ£€ç´¢ (target_yearä¼˜åŒ–)")
    logger.info("  3. âœ… å»é‡æœºåˆ¶ (é¿å…é‡å¤æ–‡æ¡£)")
    logger.info("  4. âœ… å¹¶è¡Œæ£€ç´¢ (å¤šå­é—®é¢˜å¹¶è¡Œ)")
    logger.info("  5. âœ… è¯¦ç»†å†…éƒ¨æ€è€ƒè¿‡ç¨‹è¾“å‡º")
    logger.info("  6. âœ… å®Œæ•´å¼•ç”¨æŠ¥å‘Šç”Ÿæˆ\n")

    # 1. åˆ›å»ºå·¥ä½œæµ
    logger.info("\nğŸ“¦ 1. åˆ›å»ºPineconeä¼˜åŒ–ç‰ˆå·¥ä½œæµ")
    logger.info("-" * 100)
    workflow = create_pinecone_workflow()
    logger.info("âœ… å·¥ä½œæµåˆ›å»ºå®Œæˆ\n")

    # 2. è¿è¡Œæµ‹è¯•
    logger.info(f"\nğŸ“‹ 2. è¿è¡Œ7ä¸ªæµ‹è¯•é—®é¢˜ ({language_name})")
    logger.info("-" * 100)

    results = []
    for question_data in TEST_QUESTIONS:
        try:
            result = test_one_question(workflow, question_data, total_questions=len(TEST_QUESTIONS))
            results.append(result)
            time.sleep(3)  # é¿å…APIé€Ÿç‡é™åˆ¶
        except Exception as e:
            logger.error(f"âŒ é—®é¢˜ {question_data['id']} æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

    # 3. ç”ŸæˆæŠ¥å‘Š
    logger.info("\nğŸ“Š 3. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
    logger.info("-" * 100)

    # JSONæŠ¥å‘Š
    json_output = project_root / "langgraph_complete_test_results.json"
    with open(json_output, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… JSONæŠ¥å‘Š: {json_output}")

    # MarkdownæŠ¥å‘Š
    md_output = project_root / "LANGGRAPH_COMPLETE_TEST_REPORT.md"
    generate_markdown_report(results, md_output)

    # 4. æ‰“å°æ€»ç»“
    logger.info("\n" + "="*100)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("="*100)

    successful = [r for r in results if not r.get('error')]
    failed = [r for r in results if r.get('error')]

    logger.info(f"\nå®Œæˆæµ‹è¯•: {len(successful)}/{len(TEST_QUESTIONS)} æˆåŠŸ")
    logger.info(f"å¤±è´¥: {len(failed)}")

    if successful:
        avg_time = sum(r['total_time'] for r in successful) / len(successful)
        logger.info(f"\nå¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")

        # å¹´ä»½è¦†ç›–ç‡ç»Ÿè®¡
        logger.info(f"\nå¹´ä»½è¦†ç›–æƒ…å†µ:")
        for r in successful:
            expected = r.get('expected_years', [])
            actual = list(r.get('overall_year_distribution', {}).keys())
            coverage = len(actual) / len(expected) * 100 if expected else 0
            logger.info(
                f"  Q{r['question_id']}: {len(actual)}/{len(expected)} å¹´ä»½ "
                f"({coverage:.0f}% è¦†ç›–ç‡)"
            )

    logger.info(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜:")
    logger.info(f"  - JSON: {json_output}")
    logger.info(f"  - Markdown: {md_output}")

    logger.info("\n" + "="*100)
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    logger.info("="*100)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)
