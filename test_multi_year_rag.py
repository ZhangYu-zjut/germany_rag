#!/usr/bin/env python3
"""
å¤šå¹´ä»½RAGç³»ç»Ÿæµ‹è¯•
æµ‹è¯•è·¨å¹´ä»½çš„å¤æ‚æŸ¥è¯¢èƒ½åŠ›
"""

import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent
sys.path.append(str(project_root))
load_dotenv(project_root / ".env", override=True)

from pinecone import Pinecone
from src.llm.embeddings import GeminiEmbeddingClient
from src.llm.client import GeminiLLMClient
from src.utils.logger import setup_logger
import requests
import json
import re

logger = setup_logger()


# 7ä¸ªæµ‹è¯•é—®é¢˜
TEST_QUESTIONS = [
    {
        "id": 1,
        "question": "è¯·æ¦‚è¿°2015å¹´ä»¥æ¥å¾·å›½åŸºæ°‘ç›Ÿå¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºå‘ç”Ÿäº†å“ªäº›ä¸»è¦å˜åŒ–ã€‚",
        "type": "å¤šå¹´å˜åŒ–åˆ†æ",
        "years": "2015-2024"
    },
    {
        "id": 2,
        "question": "2017å¹´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­å„å…šæ´¾å¯¹ä¸“ä¸šäººæ‰ç§»æ°‘åˆ¶åº¦æ”¹é©åˆ†åˆ«æŒä»€ä¹ˆç«‹åœºï¼Ÿ",
        "type": "å•å¹´å¤šå…šæ´¾å¯¹æ¯”",
        "years": "2017"
    },
    {
        "id": 3,
        "question": "2015å¹´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­ç»¿å…šåœ¨ç§»æ°‘å›½ç±é—®é¢˜ä¸Šçš„ä¸»è¦ç«‹åœºå’Œè¯‰æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
        "type": "å•å¹´å•å…šæ´¾è§‚ç‚¹",
        "years": "2015"
    },
    {
        "id": 4,
        "question": "åœ¨2015å¹´åˆ°2018å¹´æœŸé—´ï¼Œå¾·å›½è”é‚¦è®®ä¼šä¸­ä¸åŒå…šæ´¾åœ¨éš¾æ°‘å®¶åº­å›¢èšé—®é¢˜ä¸Šçš„è®¨è®ºå‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ï¼Ÿ",
        "type": "è·¨å¹´å¤šå…šæ´¾å˜åŒ–",
        "years": "2015-2018"
    },
    {
        "id": 5,
        "question": "è¯·å¯¹æ¯”2015-2017å¹´è”ç›Ÿå…šä¸ç»¿å…šåœ¨ç§»æ°‘èåˆæ”¿ç­–æ–¹é¢çš„ä¸»å¼ ã€‚",
        "type": "è·¨å¹´ä¸¤å…šå¯¹æ¯”",
        "years": "2015-2017"
    },
    {
        "id": 6,
        "question": "2019å¹´ä¸2017å¹´ç›¸æ¯”ï¼Œè”é‚¦è®®ä¼šå…³äºéš¾æ°‘é£è¿”çš„è®¨è®ºæœ‰ä½•å˜åŒ–ï¼Ÿ",
        "type": "ä¸¤å¹´å¯¹æ¯”",
        "years": "2017, 2019"
    },
    {
        "id": 7,
        "question": "æ–°å† ç–«æƒ…æœŸé—´ï¼ˆä¸»è¦æ˜¯2020å¹´ï¼‰ï¼Œè”é‚¦è®®é™¢å¯¹åšæŒæ°”å€™ç›®æ ‡çš„çœ‹æ³•å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿè¯·ä½¿ç”¨2019-2021å¹´çš„èµ„æ–™è¿›è¡Œå›ç­”ã€‚å¿…è¦æ—¶ç»™å‡ºå…·ä½“å¼•è¯­ã€‚",
        "type": "è·¨å¹´ç–«æƒ…å½±å“åˆ†æ",
        "years": "2019-2021"
    }
]


def extract_params(question: str) -> dict:
    """ä»é—®é¢˜ä¸­æå–å‚æ•°"""
    params = {}

    # æå–å¹´ä»½
    year_patterns = [
        r'(\d{4})\s*å¹´',  # 2015å¹´
        r'(\d{4})\s*-\s*(\d{4})',  # 2015-2018
        r'(\d{4})',  # 2015
    ]

    years = set()
    for pattern in year_patterns:
        matches = re.findall(pattern, question)
        for match in matches:
            if isinstance(match, tuple):
                for year in match:
                    if 2000 <= int(year) <= 2030:
                        years.add(year)
            else:
                if 2000 <= int(match) <= 2030:
                    years.add(match)

    if years:
        params['years'] = sorted(list(years))

    # æå–å…šæ´¾
    parties_map = {
        'CDU/CSU': ['CDU/CSU', 'CDU', 'CSU', 'åŸºæ°‘ç›Ÿ', 'è”ç›Ÿå…š'],
        'SPD': ['SPD', 'ç¤¾æ°‘å…š'],
        'BÃœNDNIS 90/DIE GRÃœNEN': ['ç»¿å…š', 'GRÃœNEN', 'GRÃœNE'],
        'DIE LINKE': ['å·¦ç¿¼å…š', 'LINKE'],
        'FDP': ['FDP', 'è‡ªæ°‘å…š'],
        'AfD': ['AfD', 'é€‰æ‹©å…š']
    }

    detected_parties = []
    for standard_name, keywords in parties_map.items():
        for keyword in keywords:
            if keyword in question:
                if standard_name not in detected_parties:
                    detected_parties.append(standard_name)
                break

    if detected_parties:
        params['parties'] = detected_parties

    # æå–ä¸»é¢˜å…³é”®è¯
    topic_keywords = ['éš¾æ°‘', 'ç§»æ°‘', 'èåˆ', 'é£è¿”', 'å®¶åº­å›¢èš', 'ä¸“ä¸šäººæ‰', 'æ°”å€™', 'ç–«æƒ…']
    topics = [kw for kw in topic_keywords if kw in question]
    if topics:
        params['topics'] = topics

    return params


def retrieve_documents(index, embedding_client, question: str, params: dict, top_k: int = 20):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_vector = embedding_client.embed_text(question)

    # æ„å»ºè¿‡æ»¤å™¨
    filters = {}

    # å¹´ä»½è¿‡æ»¤
    if 'years' in params:
        years = params['years']
        if len(years) == 1:
            filters['year'] = {'$eq': years[0]}
        else:
            filters['year'] = {'$in': years}

    # å…šæ´¾è¿‡æ»¤ï¼ˆæš‚ä¸ä½¿ç”¨ï¼Œè®©æ£€ç´¢æ›´å®½æ³›ï¼‰
    # if 'parties' in params:
    #     filters['group'] = {'$in': params['parties']}

    # æŸ¥è¯¢
    query_args = {
        'vector': query_vector,
        'top_k': top_k,
        'include_metadata': True
    }

    if filters:
        query_args['filter'] = filters

    results = index.query(**query_args)

    return results.matches


def rerank_with_cohere(question: str, documents: list, top_n: int = 5):
    """ä½¿ç”¨Cohere ReRanké‡æ’åº"""
    cohere_key = os.getenv('COHERE_API_KEY')
    if not cohere_key:
        logger.warning("âš ï¸ COHERE_API_KEYæœªè®¾ç½®ï¼Œè·³è¿‡ReRank")
        return documents[:top_n]

    try:
        # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
        doc_texts = []
        for doc in documents:
            text = doc.metadata.get('text', '')
            doc_texts.append(text[:2000])  # é™åˆ¶é•¿åº¦

        # è°ƒç”¨Cohere API
        url = "https://api.cohere.com/v2/rerank"
        headers = {
            "Authorization": f"Bearer {cohere_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "rerank-v3.5",
            "query": question,
            "documents": doc_texts,
            "top_n": top_n
        }

        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()

        # é‡æ–°æ’åº
        reranked = []
        for item in result.get('results', []):
            idx = item['index']
            score = item['relevance_score']
            doc = documents[idx]
            doc.rerank_score = score
            reranked.append(doc)

        return reranked

    except Exception as e:
        logger.warning(f"âš ï¸ ReRankå¤±è´¥: {str(e)}, ä½¿ç”¨åŸå§‹æ’åº")
        return documents[:top_n]


def generate_answer(llm, question: str, documents: list) -> str:
    """ç”Ÿæˆç­”æ¡ˆ"""
    # æ„å»ºcontext
    context_parts = []
    for i, doc in enumerate(documents, 1):
        metadata = doc.metadata
        speaker = metadata.get('speaker', 'æœªçŸ¥')
        date = metadata.get('date', 'æœªçŸ¥')
        party = metadata.get('group', 'æœªçŸ¥')
        text = metadata.get('text', '')

        context_parts.append(
            f"[æ–‡æ¡£{i}] {date} | {speaker} ({party})\n{text}\n"
        )

    context = "\n".join(context_parts)

    # æ„å»ºæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¾·å›½è®®ä¼šç ”ç©¶åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹
2. å¯¹äºå¤šå¹´ä»½ã€å¤šå…šæ´¾çš„é—®é¢˜ï¼Œè¦æ¸…æ™°åœ°ç»„ç»‡ç­”æ¡ˆç»“æ„
3. é€‚å½“å¼•ç”¨å…·ä½“å‘è¨€äººå’Œæ—¥æœŸ
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
5. ä½¿ç”¨ä¸­æ–‡å›ç­”

é—®é¢˜ï¼š{question}

æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š"""

    answer = llm.invoke(prompt)
    return answer


def test_one_question(question_data: dict, index, embedding_client, llm):
    """æµ‹è¯•ä¸€ä¸ªé—®é¢˜"""
    qid = question_data['id']
    question = question_data['question']
    qtype = question_data['type']
    years = question_data['years']

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“ é—®é¢˜ {qid}: {qtype} ({years})")
    logger.info(f"{'='*80}")
    logger.info(f"é—®é¢˜: {question}")

    # 1. å‚æ•°æå–
    logger.info(f"\nğŸ” 1. å‚æ•°æå–")
    params = extract_params(question)
    logger.info(f"   æå–å‚æ•°: {params}")

    # 2. æ–‡æ¡£æ£€ç´¢
    logger.info(f"\nğŸ“š 2. æ–‡æ¡£æ£€ç´¢ (top_k=20)")
    start_time = time.time()
    documents = retrieve_documents(index, embedding_client, question, params, top_k=20)
    retrieve_time = time.time() - start_time
    logger.info(f"   æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    logger.info(f"   è€—æ—¶: {retrieve_time:.2f}ç§’")

    if documents:
        top_doc = documents[0]
        logger.info(f"   æœ€é«˜ç›¸ä¼¼åº¦: {top_doc.score:.4f}")
        logger.info(f"   ç¤ºä¾‹: {top_doc.metadata.get('speaker', 'æœªçŸ¥')}, {top_doc.metadata.get('date', 'æœªçŸ¥')}")

    # 3. ReRank
    logger.info(f"\nğŸ”„ 3. Cohere ReRank (top_n=5)")
    start_time = time.time()
    reranked_docs = rerank_with_cohere(question, documents, top_n=5)
    rerank_time = time.time() - start_time
    logger.info(f"   é‡æ’åæ–‡æ¡£æ•°: {len(reranked_docs)}")
    logger.info(f"   è€—æ—¶: {rerank_time:.2f}ç§’")

    if reranked_docs and hasattr(reranked_docs[0], 'rerank_score'):
        logger.info(f"   æœ€é«˜ReRankåˆ†æ•°: {reranked_docs[0].rerank_score:.4f}")

    # 4. ç”Ÿæˆç­”æ¡ˆ
    logger.info(f"\nğŸ’¬ 4. ç”Ÿæˆç­”æ¡ˆ")
    start_time = time.time()
    answer = generate_answer(llm, question, reranked_docs)
    generate_time = time.time() - start_time
    logger.info(f"   ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
    logger.info(f"   è€—æ—¶: {generate_time:.2f}ç§’")

    # 5. æ˜¾ç¤ºç­”æ¡ˆ
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… ç­”æ¡ˆ:")
    logger.info(f"{'='*80}")
    logger.info(answer)
    logger.info(f"\n{'='*80}")

    return {
        "question_id": qid,
        "question": question,
        "type": qtype,
        "years": years,
        "params": params,
        "retrieved_docs": len(documents),
        "reranked_docs": len(reranked_docs),
        "retrieve_time": retrieve_time,
        "rerank_time": rerank_time,
        "generate_time": generate_time,
        "total_time": retrieve_time + rerank_time + generate_time,
        "answer_length": len(answer),
        "answer": answer
    }


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80)
    logger.info("ğŸš€ å¤šå¹´ä»½RAGç³»ç»Ÿæµ‹è¯•")
    logger.info("="*80)

    # 1. åˆå§‹åŒ–
    logger.info("\nğŸ“¦ 1. åˆå§‹åŒ–ç»„ä»¶")
    logger.info("-" * 40)

    # Pinecone
    pc = Pinecone(api_key=os.getenv('PINECONE_VECTOR_DATABASE_API_KEY'))
    index = pc.Index('german-bge')
    logger.info("âœ… Pineconeè¿æ¥æˆåŠŸ")

    stats = index.describe_index_stats()
    logger.info(f"   ç´¢å¼•å‘é‡æ•°: {stats['total_vector_count']:,}")

    # Embedding
    embedding_client = GeminiEmbeddingClient(
        embedding_mode="local",
        model_name="BAAI/bge-m3",
        dimensions=1024
    )
    logger.info("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    # LLM
    llm = GeminiLLMClient(temperature=0.0)
    logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    # 2. è¿è¡Œæµ‹è¯•
    logger.info("\nğŸ“‹ 2. è¿è¡Œæµ‹è¯•é—®é¢˜")
    logger.info("-" * 40)

    results = []
    for question_data in TEST_QUESTIONS:
        try:
            result = test_one_question(question_data, index, embedding_client, llm)
            results.append(result)
            time.sleep(2)  # é¿å…APIé€Ÿç‡é™åˆ¶
        except Exception as e:
            logger.error(f"âŒ é—®é¢˜ {question_data['id']} æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

    # 3. ç”ŸæˆæŠ¥å‘Š
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("="*80)

    logger.info(f"\nå®Œæˆæµ‹è¯•: {len(results)}/{len(TEST_QUESTIONS)}")

    if results:
        avg_retrieve = sum(r['retrieve_time'] for r in results) / len(results)
        avg_rerank = sum(r['rerank_time'] for r in results) / len(results)
        avg_generate = sum(r['generate_time'] for r in results) / len(results)
        avg_total = sum(r['total_time'] for r in results) / len(results)

        logger.info(f"\nå¹³å‡æ€§èƒ½:")
        logger.info(f"  æ£€ç´¢æ—¶é—´: {avg_retrieve:.2f}ç§’")
        logger.info(f"  é‡æ’æ—¶é—´: {avg_rerank:.2f}ç§’")
        logger.info(f"  ç”Ÿæˆæ—¶é—´: {avg_generate:.2f}ç§’")
        logger.info(f"  æ€»æ—¶é—´: {avg_total:.2f}ç§’")

        logger.info(f"\nç­”æ¡ˆè´¨é‡:")
        logger.info(f"  å¹³å‡ç­”æ¡ˆé•¿åº¦: {sum(r['answer_length'] for r in results) / len(results):.0f} å­—ç¬¦")
        logger.info(f"  å¹³å‡æ£€ç´¢æ–‡æ¡£: {sum(r['retrieved_docs'] for r in results) / len(results):.0f}")
        logger.info(f"  å¹³å‡é‡æ’æ–‡æ¡£: {sum(r['reranked_docs'] for r in results) / len(results):.0f}")

    # 4. ä¿å­˜ç»“æœ
    output_file = project_root / "multi_year_rag_test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")

    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    logger.info("="*80)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)
