"""
Pineconeæ•°æ®æ£€ç´¢èŠ‚ç‚¹
ä»Pineconeå‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³ææ–™
æ”¯æŒå¤šå¹´ä»½åˆ†å±‚æ£€ç´¢ç­–ç•¥

ã€æ¶æ„è§£è€¦ä¼˜åŒ–ã€‘
- çŸ¥è¯†å›¾è°±æ‰©å±•ä¸é—®é¢˜å¤æ‚åº¦è§£è€¦
- ç®€å•é—®é¢˜ä¹Ÿå¯ä»¥è§¦å‘KGæ‰©å±•
- åœ¨Retrieveå±‚ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦éœ€è¦KGæ‰©å±•
"""

import asyncio
from typing import List, Dict, Optional
from ...vectordb.pinecone_retriever import PineconeRetriever, create_pinecone_retriever
from ...llm.embeddings import GeminiEmbeddingClient
from ...utils.logger import logger
from ...utils.performance_monitor import get_performance_monitor
from ..state import GraphState, update_state
from ..knowledge_graph import get_knowledge_graph_manager


class PineconeRetrieveNode:
    """
    Pineconeæ•°æ®æ£€ç´¢èŠ‚ç‚¹

    åŠŸèƒ½:
    1. ä¸ºæ¯ä¸ªé—®é¢˜(æˆ–å­é—®é¢˜)æ£€ç´¢ç›¸å…³ææ–™
    2. æ”¯æŒæ··åˆæ£€ç´¢(å‘é‡+å…ƒæ•°æ®è¿‡æ»¤)
    3. æ”¯æŒå¤šå¹´ä»½åˆ†å±‚æ£€ç´¢(ç¡®ä¿æ¯å¹´éƒ½æœ‰ä»£è¡¨æ€§æ–‡æ¡£)
    4. è¾“å‡ºè¯¦ç»†çš„æ£€ç´¢è¿‡ç¨‹ä¿¡æ¯(å¹´ä»½åˆ†å¸ƒã€ç›¸ä¼¼åº¦ç­‰)

    ä¼˜åŒ–:
    - é»˜è®¤top_k=50,æ”¯æŒ10+å¹´æ—¶é—´è·¨åº¦
    - æ™ºèƒ½è¯†åˆ«é•¿æ—¶é—´è·¨åº¦æŸ¥è¯¢,è‡ªåŠ¨ä½¿ç”¨åˆ†å±‚æ£€ç´¢
    - è¾“å‡ºå†…éƒ¨æ€è€ƒè¿‡ç¨‹,ä¾¿äºè°ƒè¯•
    """

    def __init__(
        self,
        retriever: PineconeRetriever = None,
        embedding_client: GeminiEmbeddingClient = None,
        top_k: int = 50,  # æå‡åˆ°50,æ”¯æŒé•¿æ—¶é—´è·¨åº¦
        index_name: str = "german-bge",
        enable_multi_year_strategy: bool = True,  # å¯ç”¨å¤šå¹´ä»½ç­–ç•¥
        limit_per_year: int = 5,  # å¤šå¹´ä»½ç­–ç•¥æ—¶æ¯å¹´çš„æ–‡æ¡£æ•°
        enable_concurrent: bool = True,  # å¯ç”¨å¹¶å‘æ£€ç´¢
        enable_kg_expansion: bool = True  # ã€æ¶æ„è§£è€¦ã€‘å¯ç”¨ç‹¬ç«‹çš„KGæ‰©å±•åˆ¤æ–­
    ):
        """
        åˆå§‹åŒ–Pineconeæ£€ç´¢èŠ‚ç‚¹

        Args:
            retriever: Pineconeæ£€ç´¢å™¨,å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            embedding_client: Embeddingå®¢æˆ·ç«¯
            top_k: é»˜è®¤è¿”å›çš„top-kç»“æœ
            index_name: Pineconeç´¢å¼•åç§°
            enable_multi_year_strategy: æ˜¯å¦å¯ç”¨å¤šå¹´ä»½åˆ†å±‚æ£€ç´¢ç­–ç•¥
            limit_per_year: å¤šå¹´ä»½ç­–ç•¥æ—¶æ¯å¹´è¿”å›çš„æ–‡æ¡£æ•°
            enable_concurrent: æ˜¯å¦å¯ç”¨å¹¶å‘æ£€ç´¢(é»˜è®¤True, å¯æå‡3-4å€é€Ÿåº¦)
            enable_kg_expansion: æ˜¯å¦å¯ç”¨ç‹¬ç«‹çš„çŸ¥è¯†å›¾è°±æ‰©å±•(ç”¨äºç®€å•é—®é¢˜)
        """
        self.index_name = index_name
        self.top_k = top_k
        self.enable_multi_year_strategy = enable_multi_year_strategy
        self.limit_per_year = limit_per_year
        self.enable_concurrent = enable_concurrent
        self.enable_kg_expansion = enable_kg_expansion

        # åˆ›å»ºæˆ–ä½¿ç”¨æä¾›çš„retriever
        if retriever is None:
            try:
                logger.info(f"[PineconeRetrieveNode] åˆå§‹åŒ–Pineconeæ£€ç´¢å™¨...")
                self.retriever = create_pinecone_retriever(
                    index_name=index_name,
                    default_limit=top_k
                )

                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = self.retriever.get_stats()
                logger.info(
                    f"[PineconeRetrieveNode] Pineconeè¿æ¥æˆåŠŸ: "
                    f"å‘é‡æ•°={stats['total_vectors']:,}, ç»´åº¦={stats['dimension']}"
                )

            except Exception as e:
                logger.error(f"[PineconeRetrieveNode] åˆ›å»ºæ£€ç´¢å™¨å¤±è´¥: {str(e)}")
                raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Pineconeæ£€ç´¢å™¨: {str(e)}")
        else:
            self.retriever = retriever

        self.embedding_client = embedding_client or GeminiEmbeddingClient()

        # ã€æ¶æ„è§£è€¦ã€‘åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨
        self.kg_manager = None
        if enable_kg_expansion:
            try:
                self.kg_manager = get_knowledge_graph_manager()
                logger.info("[PineconeRetrieveNode] çŸ¥è¯†å›¾è°±ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼ˆæ¶æ„è§£è€¦æ¨¡å¼ï¼‰")
            except Exception as e:
                logger.warning(f"[PineconeRetrieveNode] çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡KGæ‰©å±•")

        logger.info(
            f"[PineconeRetrieveNode] åˆå§‹åŒ–å®Œæˆ: "
            f"top_k={top_k}, å¤šå¹´ä»½ç­–ç•¥={enable_multi_year_strategy}, "
            f"æ¯å¹´æ–‡æ¡£æ•°={limit_per_year}, å¹¶å‘æ£€ç´¢={enable_concurrent}, "
            f"KGæ‰©å±•={enable_kg_expansion}"
        )

    def __call__(self, state: GraphState) -> GraphState:
        """
        æ‰§è¡Œæ•°æ®æ£€ç´¢

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        # æ€§èƒ½ç›‘æ§å¼€å§‹
        import time
        start_time = time.time()
        monitor = get_performance_monitor()

        # è·å–é—®é¢˜åˆ—è¡¨
        sub_questions = state.get("sub_questions")
        parameters = state.get("parameters", {})

        # ã€æ·±åº¦åˆ†ææ¨¡å¼ã€‘æ£€æµ‹æ˜¯å¦å¯ç”¨
        deep_thinking_mode = state.get("deep_thinking_mode", False)
        reasoning_steps = state.get("reasoning_steps", [])

        if deep_thinking_mode:
            logger.info("[PineconeRetrieveNode] ğŸ” æ·±åº¦åˆ†ææ¨¡å¼å·²å¯ç”¨")
            reasoning_steps.append("1. æ·±åº¦åˆ†ææ¨¡å¼å·²å¯ç”¨ï¼Œå°†è¿›è¡Œå…¨é¢çš„çŸ¥è¯†å›¾è°±æ‰©å±•")

        # ã€æ¶æ„è§£è€¦ã€‘æ£€æµ‹æ˜¯å¦æ˜¯ç®€å•é—®é¢˜ï¼Œå¹¶å°è¯•KGæ‰©å±•
        kg_expansion_info = None
        if sub_questions:
            questions = sub_questions
            logger.info(f"[PineconeRetrieveNode] æ£€ç´¢ {len(questions)} ä¸ªå­é—®é¢˜")
            if deep_thinking_mode:
                reasoning_steps.append(f"2. é—®é¢˜å·²æ‹†è§£ä¸º {len(questions)} ä¸ªå­é—®é¢˜")
        else:
            # ç®€å•é—®é¢˜è·¯å¾„ï¼šç‹¬ç«‹è¿›è¡ŒKGæ‰©å±•åˆ¤æ–­
            original_question = state["question"]
            questions = [original_question]
            logger.info(f"[PineconeRetrieveNode] æ£€ç´¢åŸå§‹é—®é¢˜ï¼ˆç®€å•é—®é¢˜è·¯å¾„ï¼‰")

            # ã€æ ¸å¿ƒæ”¹åŠ¨ã€‘å¯¹ç®€å•é—®é¢˜ç‹¬ç«‹è¿›è¡ŒKGæ‰©å±•åˆ¤æ–­
            # æ·±åº¦æ¨¡å¼ä¸‹å¼ºåˆ¶å¯ç”¨KGæ‰©å±•
            if self.enable_kg_expansion and self.kg_manager:
                kg_queries, kg_expansion_info = self._apply_kg_expansion_for_simple_question(
                    question=original_question,
                    intent=state.get("intent", "simple"),
                    question_type=state.get("question_type", "äº‹å®æŸ¥è¯¢"),
                    parameters=parameters,
                    force_expansion=deep_thinking_mode  # æ·±åº¦æ¨¡å¼å¼ºåˆ¶æ‰©å±•
                )
                if kg_queries:
                    # å°†KGæ‰©å±•æŸ¥è¯¢æ·»åŠ åˆ°æ£€ç´¢ä»»åŠ¡ä¸­
                    questions = self._merge_kg_queries_to_questions(
                        original_question, kg_queries
                    )
                    logger.info(f"[PineconeRetrieveNode] KGæ‰©å±•åæ£€ç´¢ {len(questions)} ä¸ªæŸ¥è¯¢")

                    if deep_thinking_mode:
                        reasoning_steps.append(
                            f"2. çŸ¥è¯†å›¾è°±æ‰©å±•: è§¦å‘{kg_expansion_info.get('expansion_level', '')}çº§åˆ«ï¼Œ"
                            f"ç”Ÿæˆ{len(kg_queries)}ä¸ªæ‰©å±•æŸ¥è¯¢"
                        )
                        if kg_expansion_info.get('matched_topics'):
                            reasoning_steps.append(
                                f"3. åŒ¹é…ä¸»é¢˜: {', '.join(kg_expansion_info.get('matched_topics', []))}"
                            )

        # è¾“å‡ºå†…éƒ¨æ€è€ƒè¿‡ç¨‹
        thinking_process = []
        thinking_process.append("=== æ£€ç´¢ç­–ç•¥åˆ†æ ===")
        thinking_process.append(f"é—®é¢˜æ•°é‡: {len(questions)}")
        thinking_process.append(f"æå–å‚æ•°: {parameters}")

        try:
            # === å¹¶å‘ä¼˜åŒ–ï¼šæ ¹æ®é…ç½®é€‰æ‹©ä¸²è¡Œæˆ–å¹¶å‘æ£€ç´¢ ===
            if self.enable_concurrent:
                logger.info(f"[PineconeRetrieveNode] ğŸš€ ä½¿ç”¨å¹¶å‘æ¨¡å¼æ£€ç´¢ {len(questions)} ä¸ªé—®é¢˜")
                retrieval_results, no_material_found, overall_year_distribution = asyncio.run(
                    self._retrieve_all_concurrent(questions, parameters, thinking_process)
                )
            else:
                logger.info(f"[PineconeRetrieveNode] ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ£€ç´¢ {len(questions)} ä¸ªé—®é¢˜")
                retrieval_results, no_material_found, overall_year_distribution = self._retrieve_all_sequential(
                    questions, parameters, thinking_process
                )

            # æ€»ç»“æ£€ç´¢æƒ…å†µ
            thinking_process.append("\n=== æ£€ç´¢æ€»ç»“ ===")
            thinking_process.append(f"æ€»æ–‡æ¡£æ•°: {sum(len(r['chunks']) for r in retrieval_results)}")
            thinking_process.append(f"æ•´ä½“å¹´ä»½åˆ†å¸ƒ: {overall_year_distribution}")
            thinking_process.append(f"æ‰¾åˆ°ææ–™: {'æ˜¯' if not no_material_found else 'å¦'}")

            # è®°å½•æ€§èƒ½ç›‘æ§
            end_time = time.time()
            duration = end_time - start_time
            monitor.record_timing("Pineconeæ£€ç´¢", duration)

            thinking_process.append(f"æ£€ç´¢è€—æ—¶: {duration:.2f}ç§’")

            # è¾“å‡ºæ€è€ƒè¿‡ç¨‹
            logger.info(f"\n[å†…éƒ¨æ€è€ƒè¿‡ç¨‹]\n" + "\n".join(thinking_process))

            # æ„å»ºæ›´æ–°å­—å…¸
            update_dict = {
                "retrieval_results": retrieval_results,
                "no_material_found": no_material_found,
                "retrieval_thinking": "\n".join(thinking_process),
                "overall_year_distribution": overall_year_distribution,
                "current_node": "retrieve",
                "next_node": "exception" if no_material_found else "rerank"
            }

            # ã€æ·±åº¦åˆ†ææ¨¡å¼ã€‘æ›´æ–°æ¨ç†æ­¥éª¤
            if deep_thinking_mode:
                reasoning_steps.append(
                    f"4. æ£€ç´¢å®Œæˆ: è·å–åˆ° {sum(len(r['chunks']) for r in retrieval_results)} ä¸ªæ–‡æ¡£"
                )
                update_dict["reasoning_steps"] = reasoning_steps

            # ã€æ¶æ„è§£è€¦ã€‘å¦‚æœæœ‰KGæ‰©å±•ä¿¡æ¯ï¼Œæ·»åŠ åˆ°é¡¶çº§å­—æ®µå’Œmetadata
            if kg_expansion_info:
                # æ·»åŠ åˆ°é¡¶çº§å­—æ®µï¼ˆç”¨äºUIæ˜¾ç¤ºï¼‰
                update_dict["kg_expansion_info"] = kg_expansion_info

                # åŒæ—¶æ·»åŠ åˆ°metadataï¼ˆå‘åå…¼å®¹ï¼‰
                existing_metadata = state.get("metadata", {}) or {}
                update_dict["metadata"] = {
                    **existing_metadata,
                    "kg_expansion": kg_expansion_info,
                    "kg_expansion_source": "retrieve_node"  # æ ‡è®°KGæ‰©å±•æ¥æº
                }
                logger.info(f"[PineconeRetrieveNode] KGæ‰©å±•ä¿¡æ¯å·²æ·»åŠ åˆ°state")

            return update_state(state, **update_dict)

        except Exception as e:
            logger.error(f"[PineconeRetrieveNode] æ£€ç´¢å¤±è´¥: {str(e)}")

            # è®°å½•æ€§èƒ½ç›‘æ§ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦è®°å½•ï¼‰
            end_time = time.time()
            duration = end_time - start_time
            monitor.record_timing("Pineconeæ£€ç´¢", duration)

            return update_state(
                state,
                error=f"æ£€ç´¢å¤±è´¥: {str(e)}",
                no_material_found=True,
                current_node="retrieve",
                next_node="exception"
            )

    def _retrieve_for_question(
        self,
        question: str,
        parameters: Dict,
        thinking_process: List[str],
        question_metadata: Dict = None
    ) -> tuple[List[Dict], Dict[str, int], str]:
        """
        ä¸ºå•ä¸ªé—®é¢˜æ£€ç´¢ææ–™ï¼ˆæ”¯æŒå•å¹´é’ˆå¯¹æ€§æ£€ç´¢ + Queryæ‰©å±•ï¼‰

        Args:
            question: é—®é¢˜
            parameters: æå–çš„å‚æ•°
            thinking_process: æ€è€ƒè¿‡ç¨‹åˆ—è¡¨(ç”¨äºè®°å½•)
            question_metadata: å­é—®é¢˜å…ƒæ•°æ®ï¼ˆåŒ…å«target_yearç­‰ï¼‰

        Returns:
            (æ£€ç´¢ç»“æœåˆ—è¡¨, å¹´ä»½åˆ†å¸ƒ, æ£€ç´¢æ–¹æ³•)
        """
        # === Phase 4: Queryæ‰©å±•ç­–ç•¥ ===
        # ç”ŸæˆæŸ¥è¯¢å˜ä½“ä»¥æé«˜å¬å›ç‡
        query_variants = self._generate_query_variants(question)
        thinking_process.append(f"ğŸ“ Queryæ‰©å±•: ç”Ÿæˆ {len(query_variants)} ä¸ªæŸ¥è¯¢å˜ä½“")
        for i, variant in enumerate(query_variants, 1):
            thinking_process.append(f"   å˜ä½“{i}: {variant[:80]}...")

        # ä¸ºæ¯ä¸ªå˜ä½“ç”Ÿæˆå‘é‡
        query_vectors = []
        for variant in query_variants:
            vector = self.embedding_client.embed_text(variant)
            query_vectors.append((variant, vector))

        # ===  æ–°å¢ï¼šå•å¹´é’ˆå¯¹æ€§æ£€ç´¢ç­–ç•¥ ===
        if question_metadata is None:
            question_metadata = {}

        target_year = question_metadata.get("target_year")
        retrieval_strategy = question_metadata.get("retrieval_strategy", "multi_year")

        # å­˜å‚¨æ‰€æœ‰å˜ä½“çš„æ£€ç´¢ç»“æœ
        all_results = []

        # ç­–ç•¥1: å•å¹´æ£€ç´¢ï¼ˆä¼˜å…ˆï¼‰
        if target_year and retrieval_strategy == "single_year":
            thinking_process.append(f"âœ… ä½¿ç”¨å•å¹´æ£€ç´¢ç­–ç•¥: target_year={target_year}")

            # æ„é€ å•å¹´è¿‡æ»¤æ¡ä»¶
            filters = self._extract_filters(parameters)
            # å¼ºåˆ¶è¦†ç›–yearä¸ºtarget_year
            filters['year'] = target_year

            thinking_process.append(f"å•å¹´è¿‡æ»¤æ¡ä»¶: {filters}")

            # å¯¹æ¯ä¸ªæŸ¥è¯¢å˜ä½“æ‰§è¡Œæ£€ç´¢
            for i, (variant_text, variant_vector) in enumerate(query_vectors, 1):
                variant_results = self.retriever.search(
                    query_vector=variant_vector,
                    limit=20,  # æ¯ä¸ªå˜ä½“å¬å›20ä¸ªï¼Œæ€»å…±æœ€å¤š60ä¸ª
                    filters=filters if filters else None
                )
                thinking_process.append(f"   å˜ä½“{i}å¬å›: {len(variant_results)}ä¸ªæ–‡æ¡£")
                all_results.extend(variant_results)

            retrieval_method = f"single_year_expanded(year={target_year}, variants={len(query_vectors)})"
            thinking_process.append(f"å•å¹´æ‰©å±•æ£€ç´¢å®Œæˆï¼Œæ€»è®¡ {len(all_results)} ä¸ªæ–‡æ¡£ï¼ˆå»é‡å‰ï¼‰")

        # ç­–ç•¥2: å¤šå¹´æ£€ç´¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        else:
            # æå–è¿‡æ»¤æ¡ä»¶
            filters = self._extract_filters(parameters)
            thinking_process.append(f"è¿‡æ»¤æ¡ä»¶: {filters}")

            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¤šå¹´ä»½ç­–ç•¥
            years = filters.get('year', [])
            if isinstance(years, str):
                years = [years]

            use_multi_year = (
                self.enable_multi_year_strategy and
                isinstance(years, list) and
                len(years) >= 3  # 3å¹´åŠä»¥ä¸Šä½¿ç”¨åˆ†å±‚æ£€ç´¢
            )

            if use_multi_year:
                thinking_process.append(f"æ£€æµ‹åˆ°{len(years)}å¹´è·¨åº¦ï¼Œä½¿ç”¨å¤šå¹´ä»½åˆ†å±‚æ£€ç´¢ç­–ç•¥")
                retrieval_method = f"multi_year_stratified_expanded(years={len(years)}, variants={len(query_vectors)})"

                # æå–å…¶ä»–è¿‡æ»¤æ¡ä»¶ï¼ˆå»é™¤yearï¼‰
                other_filters = {k: v for k, v in filters.items() if k != 'year'}

                # å¯¹æ¯ä¸ªæŸ¥è¯¢å˜ä½“æ‰§è¡Œå¤šå¹´ä»½æ£€ç´¢
                for i, (variant_text, variant_vector) in enumerate(query_vectors, 1):
                    variant_results = self.retriever.search_multi_year_parallel(
                        query_vector=variant_vector,
                        years=years,
                        limit_per_year=self.limit_per_year,
                        other_filters=other_filters if other_filters else None
                    )
                    thinking_process.append(f"   å˜ä½“{i}å¬å›: {len(variant_results)}ä¸ªæ–‡æ¡£")
                    all_results.extend(variant_results)
            else:
                thinking_process.append(f"ä½¿ç”¨æ ‡å‡†æ£€ç´¢ + Queryæ‰©å±•")
                retrieval_method = f"standard_expanded(variants={len(query_vectors)})"

                # å¯¹æ¯ä¸ªæŸ¥è¯¢å˜ä½“æ‰§è¡Œæ ‡å‡†æ£€ç´¢
                for i, (variant_text, variant_vector) in enumerate(query_vectors, 1):
                    variant_results = self.retriever.search(
                        query_vector=variant_vector,
                        limit=20,  # æ¯ä¸ªå˜ä½“20ä¸ª
                        filters=filters if filters else None
                    )
                    thinking_process.append(f"   å˜ä½“{i}å¬å›: {len(variant_results)}ä¸ªæ–‡æ¡£")
                    all_results.extend(variant_results)

                # ã€Phase 4 ä¿®å¤ã€‘é™çº§ç­–ç•¥ï¼šå½“speaker+partyè¿‡æ»¤è¿”å›0ç»“æœæ—¶ï¼Œåªç”¨speakeré‡è¯•
                if len(all_results) == 0 and filters and 'speaker' in filters and 'party' in filters:
                    logger.warning(f"[PineconeRetrieveNode] speaker+partyè¿‡æ»¤è¿”å›0ç»“æœï¼Œå°è¯•åªç”¨speakeré™çº§æ£€ç´¢")
                    thinking_process.append(f"âš ï¸ é™çº§ç­–ç•¥: ç§»é™¤partyè¿‡æ»¤ï¼Œåªç”¨speakeré‡è¯•")

                    # åˆ›å»ºåªæœ‰speakerçš„è¿‡æ»¤æ¡ä»¶
                    fallback_filters = {'speaker': filters['speaker']}
                    if 'year' in filters:
                        fallback_filters['year'] = filters['year']

                    thinking_process.append(f"é™çº§è¿‡æ»¤æ¡ä»¶: {fallback_filters}")

                    for i, (variant_text, variant_vector) in enumerate(query_vectors, 1):
                        variant_results = self.retriever.search(
                            query_vector=variant_vector,
                            limit=20,
                            filters=fallback_filters
                        )
                        thinking_process.append(f"   é™çº§å˜ä½“{i}å¬å›: {len(variant_results)}ä¸ªæ–‡æ¡£")
                        all_results.extend(variant_results)

                    if all_results:
                        retrieval_method = f"standard_expanded_fallback(variants={len(query_vectors)})"
                        logger.info(f"[PineconeRetrieveNode] é™çº§ç­–ç•¥æˆåŠŸï¼Œå¬å› {len(all_results)} ä¸ªæ–‡æ¡£")

        # å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦é‡æ–°æ’åº
        results = self._deduplicate_and_rerank(all_results, top_k=self.top_k)
        thinking_process.append(f"å»é‡å¹¶é‡æ’åºå: {len(results)}ä¸ªæ–‡æ¡£")

        # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
        year_distribution = {}
        for result in results:
            year = result['metadata'].get('year', 'unknown')
            year_distribution[year] = year_distribution.get(year, 0) + 1

        # æ ¼å¼åŒ–ç»“æœï¼ˆè½¬æ¢ä¸ºLangGraphç»Ÿä¸€æ ¼å¼ï¼‰
        chunks = []
        seen_texts = set()  # ç”¨äºå»é‡

        for result in results:
            metadata = result['metadata']
            text = result['text']

            # å»é‡ï¼šä½¿ç”¨æ–‡æœ¬çš„å‰100å­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
            text_signature = text[:100] if len(text) > 100 else text

            if text_signature in seen_texts:
                logger.debug(f"[å»é‡] è·³è¿‡é‡å¤æ–‡æ¡£: {metadata.get('speaker')}, {metadata.get('date')}")
                continue

            seen_texts.add(text_signature)

            chunks.append({
                "text": text,
                "metadata": {
                    "year": metadata.get("year"),
                    "month": metadata.get("month"),
                    "day": metadata.get("day"),
                    "date": metadata.get("date"),
                    "id": metadata.get("id"),  # Document ID for citation
                    "source_reference": metadata.get("source_reference"),  # User-friendly reference
                    "speaker": metadata.get("speaker"),
                    "party": metadata.get("party"),
                    "group": metadata.get("group"),
                    "group_chinese": metadata.get("group_chinese"),
                    "session": metadata.get("session"),
                    "lp": metadata.get("lp"),
                },
                "score": result['score'],
                "id": result['id']
            })

        if len(seen_texts) < len(results):
            logger.info(f"[å»é‡] ç§»é™¤äº† {len(results) - len(seen_texts)} ä¸ªé‡å¤æ–‡æ¡£ï¼Œä¿ç•™ {len(chunks)} ä¸ª")

        return chunks, year_distribution, retrieval_method

    # å…šæ´¾åç§°æ˜ å°„ï¼ˆç»Ÿä¸€ä¸ºPineconeå­˜å‚¨æ ¼å¼ï¼‰
    PARTY_NAME_MAPPING = {
        "BÃœNDNIS 90/DIE GRÃœNEN": "GrÃ¼ne/BÃ¼ndnis 90",
        "BÃœNDNIS 90": "GrÃ¼ne/BÃ¼ndnis 90",
        "DIE GRÃœNEN": "GrÃ¼ne/BÃ¼ndnis 90",
        "GRÃœNE": "GrÃ¼ne/BÃ¼ndnis 90",
        "ç»¿å…š": "GrÃ¼ne/BÃ¼ndnis 90",
        # å…¶ä»–å…šæ´¾ä¿æŒä¸å˜
        "CDU/CSU": "CDU/CSU",
        "SPD": "SPD",
        "FDP": "FDP",
        "DIE LINKE": "DIE LINKE",
        "AfD": "AfD",
    }

    def _extract_filters(self, parameters: Dict) -> Dict:
        """
        ä»å‚æ•°ä¸­æå–Pineconeè¿‡æ»¤æ¡ä»¶

        æ”¯æŒæ—¶é—´è¯­ä¹‰ç†è§£:
        - "2015å¹´ä»¥æ¥" -> ['2015', '2016', ..., '2024']
        - "2015-2018" -> ['2015', '2016', '2017', '2018']
        - "2019å¹´" -> ['2019']

        Args:
            parameters: æå–çš„å‚æ•°

        Returns:
            Pineconeæ ¼å¼çš„è¿‡æ»¤æ¡ä»¶
        """
        filters = {}

        # æ—¶é—´è¿‡æ»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
        time_range = parameters.get("time_range", {})

        # æå–å¹´ä»½å‚æ•°
        start_year = time_range.get("start_year")
        end_year = time_range.get("end_year")
        specific_years = time_range.get("specific_years", [])

        # ğŸ”§ ä¿®å¤: ä¼˜å…ˆä½¿ç”¨specific_yearsï¼ˆç¦»æ•£å¹´ä»½ï¼‰ï¼Œé¿å…ç¦»æ•£å¯¹æ¯”è¢«è¯¯åˆ¤ä¸ºè¿ç»­èŒƒå›´
        # ä¾‹å¦‚ "2019å¹´ä¸2017å¹´ç›¸æ¯”" åº”è¯¥åªæ£€ç´¢ ['2017', '2019']ï¼Œè€Œä¸æ˜¯ ['2017', '2018', '2019']
        if specific_years:
            # å…·ä½“å¹´ä»½åˆ—è¡¨ (ä¼˜å…ˆçº§æœ€é«˜ï¼Œæ”¯æŒç¦»æ•£å¯¹æ¯”)
            filters['year'] = specific_years if isinstance(specific_years, list) else [specific_years]
            logger.debug(f"[PineconeRetrieveNode] ä½¿ç”¨specific_years: {filters['year']}")
        elif start_year and end_year:
            # èŒƒå›´æŸ¥è¯¢: åªåœ¨æ²¡æœ‰specific_yearsæ—¶ä½¿ç”¨ï¼Œå±•å¼€ä¸ºè¿ç»­å¹´ä»½åˆ—è¡¨
            try:
                year_list = [str(y) for y in range(int(start_year), int(end_year) + 1)]
                filters['year'] = year_list
                logger.debug(f"[PineconeRetrieveNode] å¹´ä»½èŒƒå›´ {start_year}-{end_year} -> {year_list}")
            except:
                pass

        # å…šæ´¾è¿‡æ»¤
        parties = parameters.get("parties", [])
        if parties and parties != ["ALL_PARTIES"]:
            # è·³è¿‡ALL_PARTIESï¼ˆè¡¨ç¤ºä¸é™åˆ¶å…šæ´¾ï¼‰
            if "ALL_PARTIES" not in parties:
                # åº”ç”¨å…šæ´¾åç§°æ˜ å°„
                normalized_parties = [
                    self.PARTY_NAME_MAPPING.get(p, p) for p in parties
                ]
                filters['party'] = normalized_parties[0] if len(normalized_parties) == 1 else normalized_parties
                logger.debug(f"[PineconeRetrieveNode] å…šæ´¾æ˜ å°„: {parties} -> {normalized_parties}")

        # å‘è¨€äººè¿‡æ»¤
        speakers = parameters.get("speakers", [])
        if speakers:
            filters['speaker'] = speakers[0]

        return filters

    def _generate_query_variants(self, question: str) -> List[str]:
        """
        ç”ŸæˆæŸ¥è¯¢å˜ä½“ä»¥æé«˜å¬å›ç‡ï¼ˆPhase 4: Queryæ‰©å±•ï¼‰

        ç­–ç•¥:
        1. åŸå§‹æŸ¥è¯¢ï¼ˆä¿ç•™å®Œæ•´è¯­ä¹‰ï¼‰
        2. å…³é”®è¯æå–æŸ¥è¯¢ï¼ˆå»é™¤å†—ä½™è¯ï¼‰
        3. åŠ¨ä½œè¯å¼ºåŒ–æŸ¥è¯¢ï¼ˆé’ˆå¯¹å…·ä½“æªæ–½æ·»åŠ ç›¸å…³åŠ¨è¯ï¼‰

        Args:
            question: åŸå§‹é—®é¢˜

        Returns:
            æŸ¥è¯¢å˜ä½“åˆ—è¡¨ï¼ˆ3ä¸ªï¼‰
        """
        variants = []

        # å˜ä½“1: åŸå§‹æŸ¥è¯¢
        variants.append(question)

        # å˜ä½“2: å…³é”®è¯æå–ç‰ˆæœ¬
        keyword_query = self._extract_keywords(question)
        if keyword_query != question:  # åªæœ‰ä¸åŒæ—¶æ‰æ·»åŠ 
            variants.append(keyword_query)

        # å˜ä½“3: åŠ¨ä½œè¯å¼ºåŒ–ç‰ˆæœ¬
        action_query = self._generate_action_variant(question)
        if action_query not in variants:  # é¿å…é‡å¤
            variants.append(action_query)

        return variants

    def _extract_keywords(self, query: str) -> str:
        """
        æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆæ— éœ€LLMï¼Œçº¯è§„åˆ™ï¼‰

        å»é™¤:
        - ç–‘é—®è¯: Was ist, Wie, Welche, etc.
        - åŠ©åŠ¨è¯: die Position von, im Jahr, etc.
        - ä»‹è¯: zur, zu, von, im, etc.

        ç¤ºä¾‹:
        "Was ist die Position von CDU/CSU zur Abschiebung und RÃ¼ckfÃ¼hrung im Jahr 2017?"
        â†’ "CDU/CSU Abschiebung RÃ¼ckfÃ¼hrung 2017"

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            å…³é”®è¯æŸ¥è¯¢
        """
        import re

        # ç§»é™¤å¸¸è§ç–‘é—®è¯å’ŒçŸ­è¯­
        noise_patterns = [
            r'\b(Was ist|Was sind|Wie|Welche|Welcher|Welches|Warum|Wann)\b',
            r'\b(die Position|die Positionen|die Hauptansichten|die Hauptposition)\b',
            r'\b(von|vom|zu|zur|zum|im|in der|in den|auf|fÃ¼r|Ã¼ber)\b',
            r'\b(Jahr|Zeitraum|Thema)\b',
            r'\?',  # é—®å·
        ]

        result = query
        for pattern in noise_patterns:
            result = re.sub(pattern, ' ', result, flags=re.IGNORECASE)

        # æ¸…ç†å¤šä½™ç©ºæ ¼
        result = ' '.join(result.split())

        return result.strip()

    def _generate_action_variant(self, query: str) -> str:
        """
        ç”ŸæˆåŠ¨ä½œè¯å¼ºåŒ–å˜ä½“ï¼ˆé’ˆå¯¹å…·ä½“æ”¿ç­–æªæ–½ï¼‰

        ç­–ç•¥: å¦‚æœæŸ¥è¯¢åŒ…å«æŸäº›æ”¿ç­–å…³é”®è¯ï¼Œè‡ªåŠ¨æ·»åŠ ç›¸å…³çš„åŠ¨ä½œè¯

        ç¤ºä¾‹:
        - "Abschiebung" â†’ æ·»åŠ  "durchsetzen Zwang Ausreisepflicht"
        - "Integration" â†’ æ·»åŠ  "fÃ¶rdern MaÃŸnahmen Programme"
        - "Klimaschutz" â†’ æ·»åŠ  "umsetzen Reduktion MaÃŸnahmen"

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            åŠ¨ä½œè¯å¼ºåŒ–æŸ¥è¯¢
        """
        # åŠ¨ä½œè¯æ˜ å°„è¡¨ï¼ˆé’ˆå¯¹å¾·å›½è®®ä¼šæ”¿ç­–é¢†åŸŸï¼‰
        action_keywords_map = {
            # ç§»æ°‘/é£è¿”æ”¿ç­–
            "Abschiebung": "durchsetzen Zwang Ausreisepflicht konsequent",
            "RÃ¼ckfÃ¼hrung": "durchsetzen Zwang Ausreisepflicht konsequent",
            "Migrationspolitik": "Abschiebung RÃ¼ckfÃ¼hrung Zwang Ausreisepflicht",

            # èåˆæ”¿ç­–
            "Integration": "fÃ¶rdern MaÃŸnahmen Programme Sprache Bildung",
            "Aufnahme": "fÃ¶rdern Programme UnterstÃ¼tzung",

            # æ°”å€™æ”¿ç­–
            "Klimaschutz": "umsetzen Reduktion MaÃŸnahmen Emissionen",
            "Klimapolitik": "Emissionen Reduktion MaÃŸnahmen umsetzen",

            # æ•°å­—åŒ–
            "Digitalisierung": "vorantreiben Infrastruktur Ausbau fÃ¶rdern",

            # è¾¹å¢ƒæ§åˆ¶
            "Grenzkontrollen": "verstÃ¤rken durchsetzen Sicherheit",
            "Obergrenze": "festlegen durchsetzen begrenzen",
        }

        # æå–å…³é”®è¯ç‰ˆæœ¬ä½œä¸ºåŸºç¡€
        base = self._extract_keywords(query)

        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ”¿ç­–å…³é”®è¯
        for keyword, action_words in action_keywords_map.items():
            if keyword in query:
                # æ·»åŠ åŠ¨ä½œè¯
                return f"{base} {action_words}"

        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œè¿”å›å…³é”®è¯ç‰ˆæœ¬
        return base

    def _deduplicate_and_rerank(self, results: List[Dict], top_k: int) -> List[Dict]:
        """
        å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦é‡æ–°æ’åº

        Args:
            results: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«é‡å¤ï¼‰
            top_k: ä¿ç•™çš„æ–‡æ¡£æ•°

        Returns:
            å»é‡å¹¶æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        # ä½¿ç”¨æ–‡æ¡£IDå»é‡ï¼ˆPineconeçš„IDæ˜¯å”¯ä¸€çš„ï¼‰
        seen_ids = set()
        unique_results = []

        for result in results:
            doc_id = result.get('id')
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(result)

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        unique_results.sort(key=lambda x: x.get('score', 0), reverse=True)

        # ä¿ç•™top_kä¸ª
        return unique_results[:top_k]

    def _retrieve_all_sequential(
        self,
        questions: List,
        parameters: Dict,
        thinking_process: List[str]
    ) -> tuple[List[Dict], bool, Dict[str, int]]:
        """
        ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªæ£€ç´¢é—®é¢˜ï¼ˆåŸæœ‰é€»è¾‘ï¼‰

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            parameters: å‚æ•°
            thinking_process: æ€è€ƒè¿‡ç¨‹åˆ—è¡¨

        Returns:
            (æ£€ç´¢ç»“æœåˆ—è¡¨, æ˜¯å¦æœªæ‰¾åˆ°ææ–™, æ•´ä½“å¹´ä»½åˆ†å¸ƒ)
        """
        retrieval_results = []
        no_material_found = True
        overall_year_distribution = {}

        for i, question_item in enumerate(questions, 1):
            # æ”¯æŒå­—å…¸å’Œå­—ç¬¦ä¸²ä¸¤ç§æ ¼å¼
            if isinstance(question_item, dict):
                question_text = question_item.get("question", question_item)
                question_metadata = question_item
            else:
                question_text = question_item
                question_metadata = {
                    "question": question_text,
                    "target_year": None,
                    "retrieval_strategy": "multi_year"
                }

            logger.info(f"[PineconeRetrieveNode] æ£€ç´¢é—®é¢˜ {i}/{len(questions)}: {question_text}")
            thinking_process.append(f"\n--- å­é—®é¢˜ {i} ---")
            thinking_process.append(f"é—®é¢˜: {question_text}")
            if question_metadata.get("target_year"):
                thinking_process.append(f"ç›®æ ‡å¹´ä»½: {question_metadata['target_year']}")
                thinking_process.append(f"æ£€ç´¢ç­–ç•¥: {question_metadata.get('retrieval_strategy', 'single_year')}")

            # æ£€ç´¢ï¼ˆä¼ å…¥å…ƒæ•°æ®ï¼‰
            chunks, year_dist, retrieval_method = self._retrieve_for_question(
                question_text, parameters, thinking_process, question_metadata
            )

            if chunks:
                no_material_found = False

            # è®°å½•å¹´ä»½åˆ†å¸ƒ
            for year, count in year_dist.items():
                overall_year_distribution[year] = overall_year_distribution.get(year, 0) + count

            retrieval_results.append({
                "question": question_text,
                "question_metadata": question_metadata,
                "chunks": chunks,
                "answer": None,
                "year_distribution": year_dist,
                "retrieval_method": retrieval_method,
                "top_similarity_score": chunks[0]['score'] if chunks else 0.0
            })

            logger.info(
                f"[PineconeRetrieveNode] æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³chunks, "
                f"å¹´ä»½åˆ†å¸ƒ={year_dist}, æ–¹æ³•={retrieval_method}"
            )
            thinking_process.append(f"æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(chunks)}")
            thinking_process.append(f"å¹´ä»½åˆ†å¸ƒ: {year_dist}")
            thinking_process.append(f"æ£€ç´¢æ–¹æ³•: {retrieval_method}")
            if chunks:
                thinking_process.append(f"æœ€é«˜ç›¸ä¼¼åº¦: {chunks[0]['score']:.4f}")

        return retrieval_results, no_material_found, overall_year_distribution

    async def _retrieve_all_concurrent(
        self,
        questions: List,
        parameters: Dict,
        thinking_process: List[str],
        max_retries: int = 2  # å•ä¸ªæŸ¥è¯¢æœ€å¤§é‡è¯•æ¬¡æ•°
    ) -> tuple[List[Dict], bool, Dict[str, int]]:
        """
        å¹¶å‘æ¨¡å¼ï¼šåŒæ—¶æ£€ç´¢æ‰€æœ‰é—®é¢˜ï¼ˆ3-4å€é€Ÿåº¦æå‡ï¼‰ï¼Œå¸¦é‡è¯•æœºåˆ¶

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            parameters: å‚æ•°
            thinking_process: æ€è€ƒè¿‡ç¨‹åˆ—è¡¨
            max_retries: å•ä¸ªæŸ¥è¯¢å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            (æ£€ç´¢ç»“æœåˆ—è¡¨, æ˜¯å¦æœªæ‰¾åˆ°ææ–™, æ•´ä½“å¹´ä»½åˆ†å¸ƒ)
        """
        import time as time_module

        async def retrieve_single(idx: int, question_item, retry_count: int = 0):
            """å¼‚æ­¥æ£€ç´¢å•ä¸ªé—®é¢˜ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
            # æ”¯æŒå­—å…¸å’Œå­—ç¬¦ä¸²ä¸¤ç§æ ¼å¼
            if isinstance(question_item, dict):
                question_text = question_item.get("question", question_item)
                question_metadata = question_item
            else:
                question_text = question_item
                question_metadata = {
                    "question": question_text,
                    "target_year": None,
                    "retrieval_strategy": "multi_year"
                }

            logger.debug(f"[PineconeRetrieveNode] å¹¶å‘æ£€ç´¢é—®é¢˜ {idx}/{len(questions)}: {question_text[:50]}...")

            # åˆ›å»ºè¯¥é—®é¢˜çš„ç‹¬ç«‹æ€è€ƒè¿‡ç¨‹åˆ—è¡¨ï¼ˆé¿å…å¹¶å‘å†™å…¥å†²çªï¼‰
            question_thinking = []
            question_thinking.append(f"\n--- å­é—®é¢˜ {idx} ---")
            question_thinking.append(f"é—®é¢˜: {question_text}")
            if question_metadata.get("target_year"):
                question_thinking.append(f"ç›®æ ‡å¹´ä»½: {question_metadata['target_year']}")
                question_thinking.append(f"æ£€ç´¢ç­–ç•¥: {question_metadata.get('retrieval_strategy', 'single_year')}")

            try:
                # åœ¨executorä¸­æ‰§è¡Œæ£€ç´¢ï¼ˆå› ä¸º_retrieve_for_questionæ˜¯åŒæ­¥çš„ï¼‰
                loop = asyncio.get_event_loop()
                chunks, year_dist, retrieval_method = await loop.run_in_executor(
                    None,
                    lambda: self._retrieve_for_question(
                        question_text, parameters, question_thinking, question_metadata
                    )
                )

                question_thinking.append(f"æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(chunks)}")
                question_thinking.append(f"å¹´ä»½åˆ†å¸ƒ: {year_dist}")
                question_thinking.append(f"æ£€ç´¢æ–¹æ³•: {retrieval_method}")
                if chunks:
                    question_thinking.append(f"æœ€é«˜ç›¸ä¼¼åº¦: {chunks[0]['score']:.4f}")

                logger.info(
                    f"[PineconeRetrieveNode] é—®é¢˜{idx}æ£€ç´¢å®Œæˆ: {len(chunks)} chunks, "
                    f"å¹´ä»½åˆ†å¸ƒ={year_dist}"
                )

                return {
                    "question": question_text,
                    "question_metadata": question_metadata,
                    "chunks": chunks,
                    "answer": None,
                    "year_distribution": year_dist,
                    "retrieval_method": retrieval_method,
                    "top_similarity_score": chunks[0]['score'] if chunks else 0.0,
                    "thinking": question_thinking  # è¿”å›æ€è€ƒè¿‡ç¨‹
                }

            except Exception as e:
                # é‡è¯•æœºåˆ¶
                if retry_count < max_retries:
                    wait_time = min(2 ** (retry_count + 1), 8)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤šç­‰å¾…8ç§’
                    logger.warning(
                        f"ğŸ”„ [PineconeRetrieveNode] é—®é¢˜{idx}æ£€ç´¢å¤±è´¥ï¼Œ"
                        f"ç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry_count + 1}/{max_retries}): {str(e)[:100]}"
                    )
                    await asyncio.sleep(wait_time)
                    return await retrieve_single(idx, question_item, retry_count + 1)
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                    logger.error(
                        f"âŒ [PineconeRetrieveNode] é—®é¢˜{idx}æ£€ç´¢å¤±è´¥ï¼Œ"
                        f"å·²é‡è¯•{max_retries}æ¬¡: {str(e)}"
                    )
                    raise

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        logger.info(f"[PineconeRetrieveNode] å¯åŠ¨ {len(questions)} ä¸ªå¹¶å‘æ£€ç´¢ä»»åŠ¡...")
        tasks = [retrieve_single(idx, q) for idx, q in enumerate(questions, 1)]

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        retrieval_results = []
        no_material_found = True
        overall_year_distribution = {}

        for idx, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"[PineconeRetrieveNode] é—®é¢˜{idx}æ£€ç´¢å¤±è´¥: {result}")
                # æ·»åŠ å¤±è´¥å ä½ç¬¦
                retrieval_results.append({
                    "question": questions[idx-1] if isinstance(questions[idx-1], str) else questions[idx-1].get("question", ""),
                    "question_metadata": {},
                    "chunks": [],
                    "answer": None,
                    "year_distribution": {},
                    "retrieval_method": "failed",
                    "top_similarity_score": 0.0
                })
            else:
                # æˆåŠŸçš„ç»“æœ
                if result["chunks"]:
                    no_material_found = False

                # åˆå¹¶å¹´ä»½åˆ†å¸ƒ
                for year, count in result["year_distribution"].items():
                    overall_year_distribution[year] = overall_year_distribution.get(year, 0) + count

                # åˆå¹¶æ€è€ƒè¿‡ç¨‹åˆ°ä¸»åˆ—è¡¨
                thinking_process.extend(result.pop("thinking"))

                retrieval_results.append(result)

        logger.info(f"[PineconeRetrieveNode] âœ… å¹¶å‘æ£€ç´¢å®Œæˆï¼Œå…±å¤„ç† {len(retrieval_results)} ä¸ªé—®é¢˜")
        return retrieval_results, no_material_found, overall_year_distribution

    # ========== ã€æ¶æ„è§£è€¦ã€‘çŸ¥è¯†å›¾è°±æ‰©å±•ç›¸å…³æ–¹æ³• ==========

    def _apply_kg_expansion_for_simple_question(
        self,
        question: str,
        intent: str,
        question_type: str,
        parameters: Dict,
        force_expansion: bool = False  # æ·±åº¦æ¨¡å¼å¼ºåˆ¶æ‰©å±•
    ) -> tuple:
        """
        ã€æ¶æ„è§£è€¦ã€‘å¯¹ç®€å•é—®é¢˜ç‹¬ç«‹åº”ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•

        æ­¤æ–¹æ³•å®ç°äº†KGæ‰©å±•ä¸é—®é¢˜å¤æ‚åº¦çš„è§£è€¦ï¼š
        - ç®€å•é—®é¢˜ä¹Ÿå¯ä»¥è§¦å‘KGæ‰©å±•
        - ä¸å†ä¾èµ–DecomposeèŠ‚ç‚¹
        - æ·±åº¦åˆ†ææ¨¡å¼ä¸‹å¼ºåˆ¶è§¦å‘æ‰©å±•

        Args:
            question: åŸé—®é¢˜
            intent: é—®é¢˜æ„å›¾
            question_type: é—®é¢˜ç±»å‹
            parameters: é—®é¢˜å‚æ•°
            force_expansion: æ˜¯å¦å¼ºåˆ¶æ‰©å±•ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼ï¼‰

        Returns:
            (æ‰©å±•æŸ¥è¯¢åˆ—è¡¨, æ‰©å±•ä¿¡æ¯å­—å…¸)
        """
        if not self.kg_manager:
            return [], None

        try:
            if force_expansion:
                logger.info("[PineconeRetrieveNode] ğŸ” ã€æ·±åº¦åˆ†æã€‘å¼ºåˆ¶å¯ç”¨KGæ‰©å±•...")
            else:
                logger.info("[PineconeRetrieveNode] ã€æ¶æ„è§£è€¦ã€‘å¼€å§‹ç®€å•é—®é¢˜KGæ‰©å±•åˆ¤æ–­...")

            # è°ƒç”¨çŸ¥è¯†å›¾è°±æ‰©å±•åˆ¤æ–­
            use_kg, expansion_queries, kg_info = self.kg_manager.expand_query(
                question=question,
                intent=intent,
                question_type=question_type,
                parameters=parameters,
                force_expansion=force_expansion  # ä¼ é€’å¼ºåˆ¶æ‰©å±•å‚æ•°
            )

            if use_kg and expansion_queries:
                logger.info(f"[PineconeRetrieveNode] âœ… ç®€å•é—®é¢˜KGæ‰©å±•è§¦å‘æˆåŠŸ:")
                logger.info(f"  - æ‰©å±•çº§åˆ«: {kg_info.get('expansion_level', 'unknown')}")
                logger.info(f"  - è¯„åˆ†: {kg_info.get('score', 0)}")
                logger.info(f"  - è§¦å‘åŸå› : {kg_info.get('reasons', [])}")
                logger.info(f"  - æ‰©å±•æŸ¥è¯¢æ•°: {len(expansion_queries)}")

                # è®°å½•å‰5ä¸ªæ‰©å±•æŸ¥è¯¢
                for i, eq in enumerate(expansion_queries[:5], 1):
                    logger.info(f"    æ‰©å±•æŸ¥è¯¢{i}: {eq}")
                if len(expansion_queries) > 5:
                    logger.info(f"    ... å…±{len(expansion_queries)}ä¸ªæ‰©å±•æŸ¥è¯¢")

                return expansion_queries, kg_info
            else:
                logger.info(f"[PineconeRetrieveNode] ç®€å•é—®é¢˜KGæ‰©å±•æœªè§¦å‘: {kg_info.get('reasons', [])}")
                return [], kg_info

        except Exception as e:
            logger.error(f"[PineconeRetrieveNode] ç®€å•é—®é¢˜KGæ‰©å±•å¤±è´¥: {e}")
            return [], {"error": str(e)}

    def _merge_kg_queries_to_questions(
        self,
        original_question: str,
        kg_queries: List[str]
    ) -> List[Dict]:
        """
        ã€æ¶æ„è§£è€¦ã€‘å°†çŸ¥è¯†å›¾è°±æ‰©å±•æŸ¥è¯¢åˆå¹¶åˆ°æ£€ç´¢é—®é¢˜åˆ—è¡¨ä¸­

        Args:
            original_question: åŸå§‹é—®é¢˜
            kg_queries: KGæ‰©å±•æŸ¥è¯¢åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„é—®é¢˜åˆ—è¡¨ï¼ˆDictæ ¼å¼ï¼‰
        """
        questions = []

        # 1. åŸå§‹é—®é¢˜
        questions.append({
            "question": original_question,
            "target_year": None,
            "target_party": None,
            "retrieval_strategy": "multi_year",
            "source": "original"
        })

        # 2. æ·»åŠ KGæ‰©å±•æŸ¥è¯¢ï¼ˆå»é‡ï¼‰
        seen_questions = {original_question.lower().strip()}

        for query in kg_queries:
            query_normalized = query.lower().strip()
            if query_normalized not in seen_questions:
                questions.append({
                    "question": query,
                    "target_year": None,
                    "target_party": None,
                    "retrieval_strategy": "kg_expansion",
                    "source": "knowledge_graph"
                })
                seen_questions.add(query_normalized)

        # é™åˆ¶æ€»æ•°ï¼ˆé¿å…æŸ¥è¯¢çˆ†ç‚¸ï¼‰
        max_questions = 25  # ç®€å•é—®é¢˜çš„KGæ‰©å±•é™åˆ¶
        if len(questions) > max_questions:
            logger.warning(
                f"[PineconeRetrieveNode] KGæ‰©å±•æŸ¥è¯¢è¿‡å¤š({len(questions)})ï¼Œ"
                f"æˆªå–å‰{max_questions}ä¸ª"
            )
            questions = questions[:max_questions]

        logger.info(
            f"[PineconeRetrieveNode] KGæ‰©å±•åˆå¹¶å®Œæˆ: "
            f"åŸå§‹1ä¸ª + KGæ‰©å±•{len(questions)-1}ä¸ª = {len(questions)}ä¸ªæŸ¥è¯¢"
        )

        return questions


if __name__ == "__main__":
    # æµ‹è¯•Pineconeæ£€ç´¢èŠ‚ç‚¹
    from ..state import create_initial_state, update_state

    print("=== Pineconeæ£€ç´¢èŠ‚ç‚¹æµ‹è¯• ===")

    # æµ‹è¯•å¤šå¹´ä»½æ£€ç´¢
    question = "è¯·æ¦‚è¿°2015å¹´ä»¥æ¥å¾·å›½åŸºæ°‘ç›Ÿå¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºå‘ç”Ÿäº†å“ªäº›ä¸»è¦å˜åŒ–ã€‚"
    state = create_initial_state(question)
    state = update_state(
        state,
        intent="complex",
        question_type="å˜åŒ–ç±»",
        parameters={
            "time_range": {
                "start_year": "2015",
                "end_year": "2024",
                "specific_years": [str(y) for y in range(2015, 2025)]
            },
            "parties": ["CDU/CSU"],
            "topics": ["éš¾æ°‘"]
        }
    )

    print(f"é—®é¢˜: {question}")
    print(f"å‚æ•°: {state['parameters']}")
    print("\nå¦‚éœ€å®Œæ•´æµ‹è¯•,è¯·ç¡®ä¿:")
    print("1. PINECONE_VECTOR_DATABASE_API_KEYå·²è®¾ç½®")
    print("2. Pineconeç´¢å¼•german-bgeå­˜åœ¨ä¸”æœ‰2015-2024æ•°æ®")
    print("3. è¿è¡Œ: python -m src.graph.nodes.retrieve_pinecone")

    try:
        node = PineconeRetrieveNode()
        print(f"\nâœ… èŠ‚ç‚¹åˆ›å»ºæˆåŠŸï¼Œé…ç½®:")
        print(f"   - top_k: {node.top_k}")
        print(f"   - å¤šå¹´ä»½ç­–ç•¥: {node.enable_multi_year_strategy}")
        print(f"   - æ¯å¹´æ–‡æ¡£æ•°: {node.limit_per_year}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
