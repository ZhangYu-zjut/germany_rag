"""
QueryExpansion Node - LLMé©±åŠ¨çš„å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆï¼ˆæ”¯æŒå¹¶å‘ + å®ä½“æå–ï¼‰
"""
import json
import asyncio
import re
from typing import Dict, Any, List
from loguru import logger

from src.graph.state import GraphState
from src.llm.client import GeminiLLMClient
from src.llm.prompts_query_expansion import build_query_expansion_prompt, QUERY_EXPANSION_FALLBACK_PROMPT


class QueryExpansionNode:
    """Queryæ‰©å±•èŠ‚ç‚¹ - ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„æ£€ç´¢æŸ¥è¯¢"""

    def __init__(self, llm_client: GeminiLLMClient = None, expansion_count: int = 5, enable_concurrent: bool = True):
        """
        åˆå§‹åŒ–Queryæ‰©å±•èŠ‚ç‚¹

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
            expansion_count: æ‰©å±•æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤5ä¸ªï¼‰
            enable_concurrent: æ˜¯å¦å¯ç”¨å¹¶å‘æŸ¥è¯¢æ‰©å±•ï¼ˆé»˜è®¤Trueï¼Œå¯æå‡4-5å€é€Ÿåº¦ï¼‰
        """
        self.llm_client = llm_client or GeminiLLMClient()
        self.expansion_count = expansion_count
        self.enable_concurrent = enable_concurrent
        logger.info(f"[QueryExpansionNode] åˆå§‹åŒ–å®Œæˆ: expansion_count={expansion_count}, concurrent={enable_concurrent}")

    def __call__(self, state: GraphState) -> Dict[str, Any]:
        """
        æ‰§è¡ŒQueryæ‰©å±•

        Args:
            state: å½“å‰å›¾çŠ¶æ€

        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        logger.info("[QueryExpansionNode] å¼€å§‹Queryæ‰©å±•...")

        # ä½¿ç”¨questionå­—æ®µä½œä¸ºåŸå§‹é—®é¢˜
        original_question = state.get("question", "")
        sub_questions = state.get("sub_questions", [])

        if not sub_questions:
            logger.warning("[QueryExpansionNode] æ²¡æœ‰å­é—®é¢˜ï¼Œè·³è¿‡Queryæ‰©å±•")
            return {"expanded_queries_map": {}}

        # æå–å‚æ•°ï¼ˆç”¨äºPromptï¼‰ - ä½¿ç”¨parameterså­—æ®µ
        params = state.get("parameters", {})
        year = params.get("year", [""])[0] if params.get("year") else ""
        group = params.get("group", [""])[0] if params.get("group") else ""
        topic = params.get("topic", [""])[0] if params.get("topic") else ""

        # === å¹¶å‘ä¼˜åŒ–ï¼šæ ¹æ®é…ç½®é€‰æ‹©ä¸²è¡Œæˆ–å¹¶å‘ ===
        if self.enable_concurrent:
            logger.info(f"[QueryExpansionNode] ğŸš€ ä½¿ç”¨å¹¶å‘æ¨¡å¼æ‰©å±• {len(sub_questions)} ä¸ªå­é—®é¢˜")
            expanded_queries_map = asyncio.run(
                self._expand_all_concurrent(sub_questions, original_question, year, group, topic)
            )
        else:
            logger.info(f"[QueryExpansionNode] ä½¿ç”¨ä¸²è¡Œæ¨¡å¼æ‰©å±• {len(sub_questions)} ä¸ªå­é—®é¢˜")
            expanded_queries_map = self._expand_all_sequential(sub_questions, original_question, year, group, topic)

        return {
            "expanded_queries_map": expanded_queries_map
        }

    def _expand_all_sequential(self, sub_questions: List, original_question: str,
                               year: str, group: str, topic: str) -> Dict[str, List[str]]:
        """
        ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªæ‰©å±•å­é—®é¢˜ï¼ˆåŸæœ‰é€»è¾‘ï¼‰

        Args:
            sub_questions: å­é—®é¢˜åˆ—è¡¨
            original_question: åŸå§‹é—®é¢˜
            year: å¹´ä»½
            group: å…šæ´¾
            topic: ä¸»é¢˜

        Returns:
            æ‰©å±•æŸ¥è¯¢æ˜ å°„å­—å…¸
        """
        expanded_queries_map = {}

        for idx, sub_q in enumerate(sub_questions, start=1):
            # === æ”¯æŒå¤šç§å­é—®é¢˜æ ¼å¼ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰ ===
            if isinstance(sub_q, dict):
                sub_q_text = sub_q.get("question", str(sub_q))
            else:
                sub_q_text = str(sub_q)

            logger.info(f"[QueryExpansionNode] ä¸ºå­é—®é¢˜{idx}ç”Ÿæˆæ‰©å±•æŸ¥è¯¢: {sub_q_text[:100]}...")

            try:
                # æ„å»ºPrompt
                prompt = build_query_expansion_prompt(
                    original_question=original_question,
                    sub_question=sub_q_text,
                    year=year,
                    group=group,
                    topic=topic
                )

                # è°ƒç”¨LLMï¼ˆinvokeæ–¹æ³•ä¸æ”¯æŒtemperatureå’Œmax_tokenså‚æ•°ï¼‰
                response = self.llm_client.invoke(prompt)

                # è§£æJSON
                expanded_queries = self._parse_llm_response(response)

                # éªŒè¯å¹¶é™åˆ¶æ•°é‡
                expanded_queries = self._validate_queries(expanded_queries, sub_q_text, year, group)

                # === ã€Layer 2å¢å¼ºã€‘æ·»åŠ å®ä½“ç²¾ç¡®æŸ¥è¯¢ï¼ˆä¿®å¤Q5/Q7ï¼‰ ===
                expanded_queries = self._add_entity_queries(expanded_queries, sub_q_text)

                # === ä½¿ç”¨å­é—®é¢˜æ–‡æœ¬ä½œä¸ºkeyï¼ˆå­—å…¸ä¸èƒ½ä½œä¸ºdict keyï¼‰ ===
                # å¦‚æœsub_qæ˜¯dictï¼Œåˆ™ä½¿ç”¨questionå­—æ®µä½œä¸ºkeyï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                map_key = sub_q_text if isinstance(sub_q, dict) else sub_q
                expanded_queries_map[map_key] = expanded_queries

                logger.success(
                    f"[QueryExpansionNode] å­é—®é¢˜{idx}ç”Ÿæˆ{len(expanded_queries)}ä¸ªæ‰©å±•æŸ¥è¯¢"
                )
                for i, query in enumerate(expanded_queries, start=1):
                    logger.debug(f"  {i}. {query}")

            except Exception as e:
                logger.error(f"[QueryExpansionNode] å­é—®é¢˜{idx}æ‰©å±•å¤±è´¥: {e}")
                # Fallback: ä½¿ç”¨åŸå§‹å­é—®é¢˜æ–‡æœ¬
                map_key = sub_q_text if isinstance(sub_q, dict) else sub_q
                expanded_queries_map[map_key] = [sub_q_text]

        return expanded_queries_map

    async def _expand_all_concurrent(self, sub_questions: List, original_question: str,
                                    year: str, group: str, topic: str) -> Dict[str, List[str]]:
        """
        å¹¶å‘æ¨¡å¼ï¼šåŒæ—¶æ‰©å±•æ‰€æœ‰å­é—®é¢˜ï¼ˆ4-5å€é€Ÿåº¦æå‡ï¼‰

        Args:
            sub_questions: å­é—®é¢˜åˆ—è¡¨
            original_question: åŸå§‹é—®é¢˜
            year: å¹´ä»½
            group: å…šæ´¾
            topic: ä¸»é¢˜

        Returns:
            æ‰©å±•æŸ¥è¯¢æ˜ å°„å­—å…¸
        """
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for idx, sub_q in enumerate(sub_questions, start=1):
            task = self._expand_single_async(idx, sub_q, original_question, year, group, topic)
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info(f"[QueryExpansionNode] å¯åŠ¨ {len(tasks)} ä¸ªå¹¶å‘LLMè°ƒç”¨...")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ„å»ºç»“æœæ˜ å°„
        expanded_queries_map = {}
        for idx, (sub_q, result) in enumerate(zip(sub_questions, results), start=1):
            # æå–å­é—®é¢˜æ–‡æœ¬
            if isinstance(sub_q, dict):
                sub_q_text = sub_q.get("question", str(sub_q))
            else:
                sub_q_text = str(sub_q)

            map_key = sub_q_text if isinstance(sub_q, dict) else sub_q

            # å¤„ç†ç»“æœæˆ–å¼‚å¸¸
            if isinstance(result, Exception):
                logger.error(f"[QueryExpansionNode] å­é—®é¢˜{idx}å¹¶å‘æ‰©å±•å¤±è´¥: {result}")
                expanded_queries_map[map_key] = [sub_q_text]
            else:
                expanded_queries_map[map_key] = result
                logger.success(f"[QueryExpansionNode] å­é—®é¢˜{idx}ç”Ÿæˆ{len(result)}ä¸ªæ‰©å±•æŸ¥è¯¢")

        logger.info(f"[QueryExpansionNode] âœ… å¹¶å‘æ‰©å±•å®Œæˆï¼Œå…±å¤„ç† {len(expanded_queries_map)} ä¸ªå­é—®é¢˜")
        return expanded_queries_map

    async def _expand_single_async(self, idx: int, sub_q: Any, original_question: str,
                                  year: str, group: str, topic: str) -> List[str]:
        """
        å¼‚æ­¥æ‰©å±•å•ä¸ªå­é—®é¢˜

        Args:
            idx: å­é—®é¢˜ç´¢å¼•
            sub_q: å­é—®é¢˜ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            original_question: åŸå§‹é—®é¢˜
            year: å¹´ä»½
            group: å…šæ´¾
            topic: ä¸»é¢˜

        Returns:
            æ‰©å±•æŸ¥è¯¢åˆ—è¡¨
        """
        # æå–å­é—®é¢˜æ–‡æœ¬
        if isinstance(sub_q, dict):
            sub_q_text = sub_q.get("question", str(sub_q))
        else:
            sub_q_text = str(sub_q)

        logger.debug(f"[QueryExpansionNode] å¼‚æ­¥æ‰©å±•å­é—®é¢˜{idx}: {sub_q_text[:50]}...")

        try:
            # æ„å»ºPrompt
            prompt = build_query_expansion_prompt(
                original_question=original_question,
                sub_question=sub_q_text,
                year=year,
                group=group,
                topic=topic
            )

            # å¼‚æ­¥è°ƒç”¨LLMï¼ˆä½¿ç”¨LangChainçš„ainvokeï¼‰
            from langchain_core.messages import HumanMessage
            response = await self.llm_client.llm.ainvoke([HumanMessage(content=prompt)])

            # è§£æJSON
            expanded_queries = self._parse_llm_response(response.content)

            # éªŒè¯å¹¶é™åˆ¶æ•°é‡
            expanded_queries = self._validate_queries(expanded_queries, sub_q_text, year, group)

            # === ã€Layer 2å¢å¼ºã€‘æ·»åŠ å®ä½“ç²¾ç¡®æŸ¥è¯¢ï¼ˆä¿®å¤Q5/Q7ï¼‰ ===
            expanded_queries = self._add_entity_queries(expanded_queries, sub_q_text)

            return expanded_queries

        except Exception as e:
            logger.error(f"[QueryExpansionNode] å­é—®é¢˜{idx}å¼‚æ­¥æ‰©å±•å¤±è´¥: {e}")
            # Fallback: ä½¿ç”¨åŸå§‹å­é—®é¢˜
            return [sub_q_text]

    def _parse_llm_response(self, response: str) -> List[str]:
        """
        è§£æLLMè¿”å›çš„JSONæ ¼å¼å“åº”

        Args:
            response: LLMå“åº”å­—ç¬¦ä¸²

        Returns:
            æ‰©å±•æŸ¥è¯¢åˆ—è¡¨

        Raises:
            ValueError: JSONè§£æå¤±è´¥
        """
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            data = json.loads(response)
            queries = data.get("expanded_queries", [])

            if not queries or not isinstance(queries, list):
                raise ValueError("expanded_querieså­—æ®µä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯")

            return queries

        except json.JSONDecodeError:
            # å°è¯•æå–JSONå—
            logger.warning("[QueryExpansionNode] ç›´æ¥JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONå—...")

            import re
            # æŸ¥æ‰¾ ```json ... ``` æˆ– { ... }
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'\{.*"expanded_queries".*\}', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–JSON")

            data = json.loads(json_str)
            queries = data.get("expanded_queries", [])

            if not queries or not isinstance(queries, list):
                raise ValueError("expanded_querieså­—æ®µä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯")

            return queries

    def _validate_queries(self, queries: List[str], original_sub_q: str,
                         year: str = "", group: str = "") -> List[str]:
        """
        éªŒè¯å¹¶è¿‡æ»¤æ‰©å±•æŸ¥è¯¢

        Args:
            queries: åŸå§‹æŸ¥è¯¢åˆ—è¡¨
            original_sub_q: åŸå§‹å­é—®é¢˜
            year: å¹´ä»½
            group: å…šæ´¾

        Returns:
            éªŒè¯åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        validated = []

        for query in queries:
            if not query or not isinstance(query, str):
                continue

            query = query.strip()

            # é•¿åº¦éªŒè¯ï¼ˆ15-120å­—ç¬¦ï¼‰
            if len(query) < 15 or len(query) > 120:
                logger.debug(f"[QueryExpansionNode] æŸ¥è¯¢é•¿åº¦ä¸åˆè§„: {query}")
                continue

            # å¿…é¡»åŒ…å«å¹´ä»½ï¼ˆå¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼‰
            if year and year not in query:
                logger.debug(f"[QueryExpansionNode] æŸ¥è¯¢ç¼ºå°‘å¹´ä»½{year}: {query}")
                # å°è¯•ä¿®å¤ï¼šæ·»åŠ å¹´ä»½
                query = f"{query} {year}"

            # å¿…é¡»åŒ…å«å…šæ´¾ï¼ˆå¦‚æœæŒ‡å®šäº†å…šæ´¾ï¼‰
            if group and group not in query:
                logger.debug(f"[QueryExpansionNode] æŸ¥è¯¢ç¼ºå°‘å…šæ´¾{group}: {query}")
                # å°è¯•ä¿®å¤ï¼šæ·»åŠ å…šæ´¾
                query = f"{group} {query}"

            validated.append(query)

        # ç¡®ä¿è‡³å°‘æœ‰åŸå§‹å­é—®é¢˜
        if not validated:
            logger.warning("[QueryExpansionNode] æ‰€æœ‰æ‰©å±•æŸ¥è¯¢éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­é—®é¢˜")
            validated = [original_sub_q]

        # é™åˆ¶æ•°é‡
        if len(validated) > self.expansion_count:
            validated = validated[:self.expansion_count]

        # å»é‡
        validated = list(dict.fromkeys(validated))  # ä¿æŒé¡ºåºçš„å»é‡

        return validated

    def _extract_entities(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–ä¸“æœ‰åè¯ï¼ˆå®ä½“ï¼‰- ä¿®å¤Q5/Q7ç±»é—®é¢˜

        æå–ç±»å‹ï¼š
        1. å›½å®¶åï¼ˆä»¥-ien/-land/-stanç»“å°¾ï¼‰: Georgien, Syrien, Deutschland
        2. é¡¹ç›®åï¼ˆå¼•å·åŒ…è£¹æˆ–ç‰¹å®šæ¨¡å¼ï¼‰: "Kultur baut BrÃ¼cken"
        3. æ³•å¾‹/ç¼©å†™ï¼ˆè¿ç»­å¤§å†™ï¼‰: GEAS, EU, CDU
        4. æ”¿ç­–æœ¯è¯­ï¼ˆç‰¹å®šæ¨¡å¼ï¼‰: Asylpaket II, sichere HerkunftslÃ¤nder

        Args:
            text: è¾“å…¥æ–‡æœ¬ï¼ˆé—®é¢˜æˆ–å­é—®é¢˜ï¼‰

        Returns:
            æå–çš„å®ä½“åˆ—è¡¨
        """
        entities = []

        # 1. å›½å®¶åï¼ˆå¾·è¯­å›½å®¶ååç¼€ï¼‰
        # åŒ¹é…æ¨¡å¼ï¼šå¤§å†™å­—æ¯å¼€å¤´ + å°å†™å­—æ¯ + ien/land/stan
        countries = re.findall(r'\b[A-ZÃ„Ã–Ãœ][a-zÃ¤Ã¶Ã¼ÃŸ]+(?:ien|land|stan)\b', text)
        entities.extend(countries)

        # 2. é¡¹ç›®/è®¡åˆ’åï¼ˆå¼•å·åŒ…è£¹ï¼‰
        # åŒ¹é… "Kultur baut BrÃ¼cken" æˆ– â€Kultur baut BrÃ¼cken"
        quoted_programs = re.findall(r'["""â€]([^"""â€]+)[""""]', text)
        entities.extend(quoted_programs)

        # 3. æ³•å¾‹/ç¼©å†™ï¼ˆ2ä¸ªåŠä»¥ä¸Šè¿ç»­å¤§å†™å­—æ¯ï¼‰
        # æ’é™¤å¸¸è§è¯ï¼ˆå¦‚CD, TVç­‰ï¼‰
        acronyms = re.findall(r'\b[A-ZÃ„Ã–Ãœ]{2,}\b', text)
        # è¿‡æ»¤ï¼šæ’é™¤çº¯æ•°å­—ã€å•ä¸ªå­—æ¯
        acronyms = [a for a in acronyms if not a.isdigit() and len(a) >= 2]
        entities.extend(acronyms)

        # 4. æ”¿ç­–æœ¯è¯­ï¼ˆç‰¹å®šæ¨¡å¼ï¼‰
        # åŒ¹é… "è¯ + ç½—é©¬æ•°å­—" æˆ– "sichere + åè¯"
        policy_terms = re.findall(r'\b(?:Asylpaket|Integrationsgesetz|Masterplan)\s+[IVX]+\b', text)
        entities.extend(policy_terms)

        # åŒ¹é… "sichere/sicherer/sichere + åè¯"
        safe_terms = re.findall(r'\bsicher[e|er|es]?\s+[A-ZÃ„Ã–Ãœ][a-zÃ¤Ã¶Ã¼ÃŸ]+\b', text)
        entities.extend(safe_terms)

        # å»é‡å¹¶è¿‡æ»¤
        entities = list(dict.fromkeys(entities))  # ä¿æŒé¡ºåºå»é‡

        # è¿‡æ»¤ï¼šé•¿åº¦è‡³å°‘3ä¸ªå­—ç¬¦
        entities = [e for e in entities if len(e.strip()) >= 3]

        if entities:
            logger.info(f"[QueryExpansionNode] æå–åˆ°{len(entities)}ä¸ªå®ä½“: {entities}")

        return entities

    def _add_entity_queries(self, validated_queries: List[str], sub_q_text: str) -> List[str]:
        """
        ä¸ºæå–çš„å®ä½“æ·»åŠ ç²¾ç¡®æŸ¥è¯¢ï¼ˆä¿®å¤Q5/Q7ï¼‰

        Args:
            validated_queries: å·²éªŒè¯çš„æŸ¥è¯¢åˆ—è¡¨
            sub_q_text: å­é—®é¢˜æ–‡æœ¬

        Returns:
            å¢å¼ºåçš„æŸ¥è¯¢åˆ—è¡¨ï¼ˆåŸæŸ¥è¯¢ + å®ä½“ç²¾ç¡®æŸ¥è¯¢ï¼‰
        """
        # ä»å­é—®é¢˜ä¸­æå–å®ä½“
        entities = self._extract_entities(sub_q_text)

        if not entities:
            return validated_queries

        # ä¸ºæ¯ä¸ªå®ä½“ç”Ÿæˆç²¾ç¡®åŒ¹é…æŸ¥è¯¢
        entity_queries = []
        for entity in entities:
            # ä½¿ç”¨å¼•å·åŒ…è£¹å®ä½“ï¼Œç¡®ä¿ç²¾ç¡®åŒ¹é…
            # æ³¨æ„ï¼šPineconeæ”¯æŒå¼•å·ç²¾ç¡®åŒ¹é…
            entity_query = f'"{entity}"'
            entity_queries.append(entity_query)

        # åˆå¹¶ï¼šå®ä½“æŸ¥è¯¢ + åŸæŸ¥è¯¢
        # å®ä½“æŸ¥è¯¢æ”¾åœ¨å‰é¢ï¼Œä¼˜å…ˆçº§æ›´é«˜
        enhanced_queries = entity_queries + validated_queries

        logger.info(
            f"[QueryExpansionNode] æ·»åŠ {len(entity_queries)}ä¸ªå®ä½“ç²¾ç¡®æŸ¥è¯¢ï¼Œ"
            f"æ€»æŸ¥è¯¢æ•°: {len(enhanced_queries)}"
        )

        return enhanced_queries
