"""
Embeddingå®¢æˆ·ç«¯æ¨¡å—
å°è£…Gemini Embeddingçš„è°ƒç”¨
æ”¯æŒå¤šç§æ¨¡å¼ï¼šlocalï¼ˆæœ¬åœ°BGE-M3ï¼‰ã€openaiã€deepinfraã€vertex
"""

from langchain_openai import OpenAIEmbeddings
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import requests
import json
from src.config import settings
from src.utils import logger

# å»¶è¿Ÿå¯¼å…¥ LocalEmbeddingClientï¼Œé¿å…å¾ªç¯å¯¼å…¥
_local_client = None


def _get_local_client():
    """å»¶è¿Ÿå¯¼å…¥ LocalEmbeddingClient"""
    global _local_client
    if _local_client is None:
        from src.llm.local_embeddings import LocalEmbeddingClient
        _local_client = LocalEmbeddingClient
    return _local_client


class GeminiEmbeddingClient:
    """
    Gemini Embeddingå®¢æˆ·ç«¯
    
    åŠŸèƒ½:
    1. æ–‡æœ¬å‘é‡åŒ–(Embedding)
    2. æ‰¹é‡Embedding
    3. æ”¯æŒä¸­æ–‡å’Œå¾·è¯­
    """
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        dimensions: Optional[int] = None,
        embedding_mode: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯
        
        Args:
            model_name: Embeddingæ¨¡å‹åç§°,é»˜è®¤ä»é…ç½®è¯»å–
            dimensions: å‘é‡ç»´åº¦,é»˜è®¤ä»é…ç½®è¯»å–
            embedding_mode: Embeddingæ¨¡å¼ï¼ˆ"openai", "deepinfra", "local", "vertex"ï¼‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        # ä»é…ç½®è¯»å–embeddingæ¨¡å¼
        self.embedding_mode = embedding_mode or settings.embedding_mode
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©é…ç½®
        if self.embedding_mode == "local":
            # æœ¬åœ°æ¨¡å¼ï¼šä½¿ç”¨ LocalEmbeddingClient
            self.model_name = model_name or settings.local_embedding_model
            self.dimensions = dimensions or settings.local_embedding_dimension
            logger.info("âœ… ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ï¼ˆå®Œå…¨å…è´¹ï¼Œæ”¯æŒ GPU åŠ é€Ÿï¼‰")
            
            # å¯¼å…¥å¹¶åˆå§‹åŒ–æœ¬åœ°å®¢æˆ·ç«¯
            LocalEmbeddingClient = _get_local_client()
            self.local_client = LocalEmbeddingClient(model_name=self.model_name)
            self.dimensions = self.local_client.dimensions  # ä»å®é™…æ¨¡å‹è·å–ç»´åº¦
            self.embeddings = None
            self.api_key = None
            self.api_url = None
            
        elif self.embedding_mode == "deepinfra":
            self.model_name = model_name or settings.deepinfra_embedding_model
            self.dimensions = dimensions or settings.deepinfra_embedding_dimension
            api_key = settings.deepinfra_embedding_api_key
            base_url = settings.deepinfra_embedding_base_url
            logger.info("âœ… ä½¿ç”¨DeepInfra Embedding APIï¼ˆé€Ÿåº¦æ›´å¿«ã€ä»·æ ¼æ›´ä¾¿å®œï¼‰")
        elif self.embedding_mode == "openai":
            self.model_name = model_name or settings.openai_embedding_model
            self.dimensions = dimensions or settings.openai_embedding_dimension
            api_key = settings.openai_embedding_api_key
            base_url = settings.openai_embedding_base_url
            logger.info("âœ… ä½¿ç”¨OpenAIå®˜æ–¹API")
        else:
            # å…¶ä»–æ¨¡å¼æš‚ä¸æ”¯æŒï¼Œä½¿ç”¨OpenAIä½œä¸ºfallback
            logger.warning(f"âš ï¸  ä¸æ”¯æŒçš„embeddingæ¨¡å¼: {self.embedding_mode}ï¼Œä½¿ç”¨OpenAIä½œä¸ºfallback")
            self.model_name = model_name or settings.openai_embedding_model
            self.dimensions = dimensions or settings.openai_embedding_dimension
            api_key = settings.openai_embedding_api_key
            base_url = settings.openai_embedding_base_url
        
        # æ ¹æ®æ¨¡å¼è®°å½•æ—¥å¿—
        if self.embedding_mode == "local":
            logger.info(
                f"åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯: mode={self.embedding_mode}, "
                f"model={self.model_name}, dimensions={self.dimensions}"
            )
        else:
            logger.info(
                f"åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯: mode={self.embedding_mode}, "
                f"model={self.model_name}, dimensions={self.dimensions}, base_url={base_url}"
            )
        
        try:
            # æœ¬åœ°æ¨¡å¼å·²åœ¨ä¸Šé¢åˆå§‹åŒ–ï¼Œè·³è¿‡
            if self.embedding_mode == "local":
                logger.success("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            # DeepInfraä½¿ç”¨requestsç›´æ¥è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿcurlï¼‰
            elif self.embedding_mode == "deepinfra":
                self.api_key = api_key
                self.api_url = f"{base_url.rstrip('/')}/embeddings"
                self.embeddings = None  # ä¸ä½¿ç”¨LangChainåŒ…è£…å™¨
                logger.info("ğŸ”§ DeepInfraä½¿ç”¨requestsç›´æ¥è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿcurlï¼‰")
                logger.info(f"   API URL: {self.api_url}")
                logger.success("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                # å…¶ä»–æ¨¡å¼ä½¿ç”¨LangChainåŒ…è£…å™¨
                embeddings_kwargs = {
                    "model": self.model_name,
                    "api_key": api_key,
                    "base_url": base_url
                }
                
                if self.embedding_mode == "openai" and self.dimensions:
                    embeddings_kwargs["dimensions"] = self.dimensions
                
                self.embeddings = OpenAIEmbeddings(**embeddings_kwargs)
                self.api_key = None
                self.api_url = None
                logger.success("âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.warning("âš ï¸  è¯·æ£€æŸ¥:")
            if self.embedding_mode == "local":
                logger.warning("  1. FlagEmbedding æˆ– sentence-transformers æ˜¯å¦æ­£ç¡®å®‰è£…")
                logger.warning("  2. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                logger.warning("  3. GPU æ˜¯å¦å¯ç”¨ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰")
            else:
                logger.warning("  1. API Keyæ˜¯å¦æ­£ç¡®")
                logger.warning("  2. base_urlæ˜¯å¦æ”¯æŒembeddingæ¥å£")
                logger.warning("  3. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                if self.embedding_mode == "deepinfra":
                    logger.warning("  4. ç¡®ä¿.envæ–‡ä»¶ä¸­é…ç½®äº†DEEPINFRA_EMBEDDING_API_KEYå’ŒDEEPINFRA_EMBEDDING_BASE_URL")
            raise
    
    def _call_deepinfra_api(self, input_data) -> dict:
        """
        ä½¿ç”¨requestsè°ƒç”¨DeepInfra APIï¼ˆæ¨¡æ‹Ÿcurlï¼‰
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼ˆå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        
        Returns:
            APIå“åº”çš„JSONæ•°æ®
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "input": input_data,
            "model": self.model_name,
            "encoding_format": "float"
        }
        
        logger.debug(f"DeepInfra APIè°ƒç”¨: {len(input_data) if isinstance(input_data, list) else 1}ä¸ªè¾“å…¥")
        
        response = requests.post(
            self.api_url,
            headers=headers,
            data=json.dumps(payload),
            timeout=60  # 60ç§’è¶…æ—¶
        )
        
        if response.status_code != 200:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code} - {response.text}")
        
        return response.json()

    def embed_text(self, text: str) -> List[float]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            æ–‡æœ¬çš„å‘é‡è¡¨ç¤º(list of floats)
        """
        try:
            if self.embedding_mode == "local":
                # æœ¬åœ°æ¨¡å¼ä½¿ç”¨ LocalEmbeddingClient
                vector = self.local_client.embed_text(text)
            elif self.embedding_mode == "deepinfra":  # DeepInfraä½¿ç”¨requests
                response_data = self._call_deepinfra_api(text)
                vector = response_data["data"][0]["embedding"]
            else:  # å…¶ä»–æ¨¡å¼ä½¿ç”¨LangChain
                vector = self.embeddings.embed_query(text)
            
            logger.debug(
                f"æ–‡æœ¬embeddingæˆåŠŸ: æ–‡æœ¬é•¿åº¦={len(text)}, "
                f"å‘é‡ç»´åº¦={len(vector)}"
            )
            
            return vector
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬embeddingå¤±è´¥: {e}")
            raise
    
    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 800,  # ğŸš€ GPUæ˜¾å­˜ä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨16GBæ˜¾å­˜ï¼Œæ€§èƒ½æå‡13x
        max_workers: int = 20,   # æé«˜å¹¶å‘æ•°åˆ°20ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        max_retries: int = 5,   # å¢åŠ é‡è¯•æ¬¡æ•°
        request_delay: float = 0.5  # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘å»¶è¿Ÿï¼Œæé«˜ååé‡
    ) -> List[List[float]]:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒå¤§æ‰¹æ¬¡å’Œå¹¶å‘ï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤100ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§ï¼‰
            max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤20ï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤5ï¼‰
            request_delay: æ‰¹æ¬¡é—´å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤1ç§’ï¼‰ï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶

        Returns:
            å‘é‡åˆ—è¡¨,æ¯ä¸ªå‘é‡å¯¹åº”ä¸€ä¸ªæ–‡æœ¬
        """
        try:
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šæœ¬åœ°GPUæ¨¡å‹ä¸æ”¯æŒå¹¶å‘ï¼Œå¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹
            if self.embedding_mode == "local":
                max_workers = 1
                logger.warning("âš ï¸  æœ¬åœ°GPUæ¨¡å¼æ£€æµ‹åˆ°ï¼Œè‡ªåŠ¨ç¦ç”¨å¹¶å‘ï¼ˆmax_workers=1ï¼‰ä»¥é¿å…GPUèµ„æºç«äº‰")

            total = len(texts)
            total_batches = (total + batch_size - 1) // batch_size

            logger.info(
                f"å¼€å§‹æ‰¹é‡Embeddingï¼ˆä¼˜åŒ–ç‰ˆï¼‰: {total}ä¸ªæ–‡æœ¬ï¼Œ"
                f"æ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œå¹¶å‘æ•°: {max_workers}ï¼Œ"
                f"æ‰¹æ¬¡é—´å»¶è¿Ÿ: {request_delay}sï¼Œé¢„è®¡{total_batches}æ‰¹"
            )
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            try:
                from tqdm import tqdm
                use_tqdm = True
            except ImportError:
                use_tqdm = False
            
            # å‡†å¤‡æ‰¹æ¬¡ç´¢å¼•
            batch_indices = list(range(0, total, batch_size))
            
            # ç”¨äºå­˜å‚¨ç»“æœï¼ˆæŒ‰é¡ºåºï¼‰
            results = [None] * len(batch_indices)
            
            def process_batch(batch_idx: int, batch_num: int) -> tuple:
                """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
                start_idx = batch_idx
                end_idx = min(start_idx + batch_size, total)
                batch = texts[start_idx:end_idx]
                
                # æ·»åŠ æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                # batch_numä»1å¼€å§‹ï¼Œç¬¬ä¸€ä¸ªæ‰¹æ¬¡ä¸å»¶è¿Ÿ
                if batch_num > 1 and request_delay > 0:
                    time.sleep(request_delay)
                
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„embeddingæ–¹æ³•
                        if self.embedding_mode == "local":
                            # æœ¬åœ°æ¨¡å¼ï¼šç›´æ¥è°ƒç”¨ï¼Œæ— éœ€é‡è¯•ï¼ˆæœ¬åœ°ä¸ä¼šå‡ºç°ç½‘ç»œé”™è¯¯ï¼‰
                            vectors = self.local_client.embed_batch(batch, batch_size=len(batch))
                            return batch_num, vectors
                        elif self.embedding_mode == "deepinfra":  # DeepInfraä½¿ç”¨requests
                            response_data = self._call_deepinfra_api(batch)
                            vectors = [data["embedding"] for data in response_data["data"]]
                        else:  # å…¶ä»–æ¨¡å¼ä½¿ç”¨LangChain
                            vectors = self.embeddings.embed_documents(batch)
                        
                        return batch_num, vectors
                    except Exception as e:
                        retry_count += 1
                        error_msg = str(e).lower()
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶æˆ–è¿æ¥é”™è¯¯
                        is_rate_limit = "rate limit" in error_msg or "429" in error_msg or "too many requests" in error_msg
                        is_connection_error = "connection" in error_msg or "ssl" in error_msg or "eof" in error_msg
                        
                        if is_rate_limit or is_connection_error:
                            # æ›´é•¿çš„æŒ‡æ•°é€€é¿ï¼š5ç§’ -> 10ç§’ -> 20ç§’ ...
                            wait_time = min(5 * (2 ** (retry_count - 1)), 60)
                            error_type = "é€Ÿç‡é™åˆ¶" if is_rate_limit else "è¿æ¥é”™è¯¯"
                            logger.warning(
                                f"æ‰¹æ¬¡ {batch_num} é‡åˆ°{error_type}ï¼Œ"
                                f"ç­‰å¾… {wait_time} ç§’åé‡è¯• ({retry_count}/{max_retries})"
                            )
                            time.sleep(wait_time)
                        elif "quota" in error_msg or "quota exceeded" in error_msg:
                            # é…é¢é”™è¯¯éœ€è¦æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                            wait_time = min(15 * retry_count, 120)
                            logger.warning(
                                f"æ‰¹æ¬¡ {batch_num} é‡åˆ°é…é¢é™åˆ¶ï¼Œ"
                                f"ç­‰å¾… {wait_time} ç§’åé‡è¯• ({retry_count}/{max_retries})"
                            )
                            time.sleep(wait_time)
                        else:
                            # éé€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                            logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                            raise
                
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                raise Exception(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥ï¼šå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
            
            # ä½¿ç”¨å¹¶å‘å¤„ç†
            start_time = time.time()
            completed = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_batch = {
                    executor.submit(process_batch, idx, i + 1): (idx, i + 1)
                    for i, idx in enumerate(batch_indices)
                }
                
                # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                if use_tqdm:
                    pbar = tqdm(total=total_batches, desc="ç”ŸæˆEmbedding", unit="æ‰¹")
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_batch):
                    batch_idx, batch_num = future_to_batch[future]
                    try:
                        result_batch_num, vectors = future.result()
                        batch_list_idx = batch_indices.index(batch_idx)
                        results[batch_list_idx] = vectors
                        completed += 1
                        
                        if use_tqdm:
                            pbar.update(1)
                        else:
                            logger.info(
                                f"å®Œæˆæ‰¹æ¬¡ {result_batch_num}/{total_batches}: "
                                f"{len(vectors)}ä¸ªå‘é‡ (å·²å®Œæˆ: {completed}/{total_batches})"
                            )
                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡ {batch_num} æ‰§è¡Œå¤±è´¥: {e}")
                        raise
                
                if use_tqdm:
                    pbar.close()
            
            # åˆå¹¶ç»“æœ
            all_vectors = []
            for vectors in results:
                if vectors is not None:
                    all_vectors.extend(vectors)
            
            elapsed_time = time.time() - start_time
            throughput = total / elapsed_time if elapsed_time > 0 else 0
            
            logger.success(
                f"æ‰¹é‡embeddingå®Œæˆ: {total}ä¸ªæ–‡æœ¬ -> {len(all_vectors)}ä¸ªå‘é‡ï¼Œ"
                f"è€—æ—¶: {elapsed_time:.1f}ç§’ï¼Œååé‡: {throughput:.1f}æ¡/ç§’"
            )
            
            return all_vectors
            
        except Exception as e:
            logger.error(f"æ‰¹é‡embeddingå¤±è´¥: {e}")
            raise
    
    def embed_chunks(
        self,
        chunks: List[dict],
        text_key: str = 'text',
        batch_size: int = 800,  # ğŸš€ GPUæ˜¾å­˜ä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨16GBæ˜¾å­˜ï¼Œæ€§èƒ½æå‡13x
        max_workers: int = 20,   # æé«˜å¹¶å‘æ•°åˆ°20ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        request_delay: float = 0.5  # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘å»¶è¿Ÿï¼Œæé«˜ååé‡
    ) -> List[dict]:
        """
        å¯¹chunksè¿›è¡Œæ‰¹é‡embedding
        
        Args:
            chunks: chunkå­—å…¸åˆ—è¡¨,æ¯ä¸ªchunkåŒ…å«textå’Œmetadata
            text_key: æ–‡æœ¬å­—æ®µçš„keyåç§°
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            æ·»åŠ äº†vectorå­—æ®µçš„chunksåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¯¹{len(chunks)}ä¸ªchunksè¿›è¡Œembedding")
        
        # æå–æ–‡æœ¬
        texts = [chunk[text_key] for chunk in chunks]
        
        # æ‰¹é‡embeddingï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        vectors = self.embed_batch(
            texts, 
            batch_size=batch_size, 
            max_workers=max_workers,
            request_delay=request_delay
        )
        
        # å°†å‘é‡æ·»åŠ åˆ°chunksä¸­
        embedded_chunks = []
        for chunk, vector in zip(chunks, vectors):
            embedded_chunk = chunk.copy()
            embedded_chunk['vector'] = vector
            embedded_chunks.append(embedded_chunk)
        
        logger.success(f"Chunks embeddingå®Œæˆ: {len(embedded_chunks)}ä¸ª")
        
        return embedded_chunks


if __name__ == "__main__":
    # æµ‹è¯•Embeddingå®¢æˆ·ç«¯
    client = GeminiEmbeddingClient()
    
    # æµ‹è¯•1: å•æ–‡æœ¬embedding
    print("\n=== æµ‹è¯•1: å•æ–‡æœ¬Embedding ===")
    text = "å¾·å›½è”é‚¦è®®é™¢æ˜¯å¾·å›½çš„æœ€é«˜ç«‹æ³•æœºæ„ã€‚"
    vector = client.embed_text(text)
    print(f"æ–‡æœ¬: {text}")
    print(f"å‘é‡ç»´åº¦: {len(vector)}")
    print(f"å‘é‡å‰5ç»´: {vector[:5]}")
    
    # æµ‹è¯•2: æ‰¹é‡embedding
    print("\n=== æµ‹è¯•2: æ‰¹é‡Embedding ===")
    texts = [
        "ç¤¾æ°‘å…šæ˜¯å¾·å›½å†å²æœ€æ‚ ä¹…çš„æ”¿å…šä¹‹ä¸€ã€‚",
        "åŸºæ°‘ç›Ÿåœ¨å¾·å›½æ”¿æ²»ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚",
        "ç»¿å…šå…³æ³¨ç¯å¢ƒå’Œæ°”å€™é—®é¢˜ã€‚",
        "The German Bundestag is located in Berlin.",
        "Die GrÃ¼nen setzen sich fÃ¼r Umweltschutz ein."
    ]
    vectors = client.embed_batch(texts, batch_size=2)
    print(f"æ‰¹é‡embedding: {len(texts)}ä¸ªæ–‡æœ¬ -> {len(vectors)}ä¸ªå‘é‡")
    for i, (text, vector) in enumerate(zip(texts, vectors), 1):
        print(f"\næ–‡æœ¬{i}: {text}")
        print(f"  å‘é‡ç»´åº¦: {len(vector)}")
        print(f"  å‘é‡å‰3ç»´: {vector[:3]}")
    
    # æµ‹è¯•3: Chunks embedding
    print("\n=== æµ‹è¯•3: Chunks Embedding ===")
    test_chunks = [
        {
            'text': 'è¿™æ˜¯ç¬¬ä¸€ä¸ªchunkçš„å†…å®¹ã€‚',
            'metadata': {'id': 1, 'speaker': 'Speaker A'}
        },
        {
            'text': 'è¿™æ˜¯ç¬¬äºŒä¸ªchunkçš„å†…å®¹ã€‚',
            'metadata': {'id': 2, 'speaker': 'Speaker B'}
        }
    ]
    embedded_chunks = client.embed_chunks(test_chunks)
    print(f"Chunks embeddingå®Œæˆ: {len(embedded_chunks)}ä¸ª")
    for chunk in embedded_chunks:
        print(f"\nChunk ID: {chunk['metadata']['id']}")
        print(f"  æ–‡æœ¬: {chunk['text']}")
        print(f"  å‘é‡ç»´åº¦: {len(chunk['vector'])}")
