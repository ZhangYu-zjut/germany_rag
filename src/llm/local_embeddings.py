"""
æœ¬åœ° Embedding å®¢æˆ·ç«¯
æ”¯æŒ sentence-transformers å’Œ BGE-M3 æ¨¡å‹ï¼Œå®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key
æ”¯æŒ GPU åŠ é€Ÿ
"""

from typing import List, Optional
import torch
from src.utils import logger

# å°è¯•å¯¼å…¥ä¸åŒçš„æ¨¡å‹åº“
try:
    from FlagEmbedding import BGEM3FlagModel
    FLAG_EMBEDDING_AVAILABLE = True
except ImportError:
    FLAG_EMBEDDING_AVAILABLE = False
    logger.warning("âš ï¸  FlagEmbedding æœªå®‰è£…ï¼ŒBGE-M3 æ¨¡å‹ä¸å¯ç”¨")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.warning("âš ï¸  sentence-transformers æœªå®‰è£…")


class LocalEmbeddingClient:
    """
    æœ¬åœ° Embedding å®¢æˆ·ç«¯
    
    ä¼˜ç‚¹:
    - âœ… å®Œå…¨å…è´¹
    - âœ… æ— éœ€ API Key
    - âœ… ç¦»çº¿å¯ç”¨
    - âœ… æ”¯æŒä¸­æ–‡å’Œå¾·è¯­
    - âœ… æ”¯æŒ GPU åŠ é€Ÿï¼ˆå¦‚æœæœ‰ GPUï¼‰
    - âœ… æ”¯æŒ BGE-M3 æ¨¡å‹ï¼ˆ1024ç»´ï¼Œæ€§èƒ½ä¼˜å¼‚ï¼‰
    
    æ”¯æŒçš„æ¨¡å‹:
    - BGE-M3 ç³»åˆ—ï¼ˆæ¨èï¼‰:
        - BAAI/bge-m3: 1024ç»´ï¼Œå¤šè¯­è¨€æ”¯æŒï¼Œæ€§èƒ½æœ€ä½³
        - BAAI/bge-m3-base: åŸºç¡€ç‰ˆæœ¬
    - sentence-transformers æ¨¡å‹:
        - paraphrase-multilingual-MiniLM-L12-v2: 384ç»´ï¼Œ50+è¯­è¨€
        - distiluse-base-multilingual-cased-v2: 512ç»´ï¼Œ15+è¯­è¨€
    """
    
    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        use_gpu: Optional[bool] = None,
        device: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
                - BAAI/bge-m3: BGE-M3 æ¨¡å‹ï¼Œ1024ç»´ â­æ¨èï¼ˆæ€§èƒ½æœ€ä½³ï¼‰
                - BAAI/bge-m3-base: BGE-M3 åŸºç¡€ç‰ˆæœ¬
                - paraphrase-multilingual-MiniLM-L12-v2: sentence-transformers æ¨¡å‹ï¼Œ384ç»´
                - distiluse-base-multilingual-cased-v2: sentence-transformers æ¨¡å‹ï¼Œ512ç»´
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPUï¼ˆNone æ—¶è‡ªåŠ¨æ£€æµ‹ï¼‰
            device: æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ 'cuda:0'ï¼‰ï¼Œä¼˜å…ˆçº§é«˜äº use_gpu
        """
        logger.info(f"ğŸ”„ åŠ è½½æœ¬åœ° Embedding æ¨¡å‹: {model_name}")
        
        # è‡ªåŠ¨æ£€æµ‹ GPU
        if device is None:
            if use_gpu is None:
                # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
                use_gpu = torch.cuda.is_available()
            device = 'cuda:0' if use_gpu else 'cpu'
        
        self.device = device
        self.model_name = model_name
        self.use_bge_m3 = False
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ BGE-M3 æ¨¡å‹
        if 'bge-m3' in model_name.lower() or 'bge_m3' in model_name.lower():
            if not FLAG_EMBEDDING_AVAILABLE:
                raise ImportError(
                    "BGE-M3 æ¨¡å‹éœ€è¦ FlagEmbedding åº“ã€‚è¯·è¿è¡Œ: pip install FlagEmbedding"
                )
            self.use_bge_m3 = True
            logger.info("ğŸ”§ ä½¿ç”¨ BGE-M3 æ¨¡å‹ï¼ˆFlagEmbeddingï¼‰")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info("â³ åŠ è½½æœ¬åœ°ç¼“å­˜çš„ BGE-M3 æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            
            # åŠ è½½ BGE-M3 æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
            import os
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            os.environ['HF_HUB_OFFLINE'] = '1'
            
            # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_path = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ¬åœ°ç¼“å­˜
            if os.path.exists(model_cache_path):
                logger.info(f"ğŸ” å‘ç°æœ¬åœ°ç¼“å­˜: {model_cache_path}")
                # æŸ¥æ‰¾å®é™…çš„æ¨¡å‹ç›®å½•
                snapshots_dir = os.path.join(model_cache_path, "snapshots")
                if os.path.exists(snapshots_dir):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªsnapshotç›®å½•
                    snapshots = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                    if snapshots:
                        actual_model_path = os.path.join(snapshots_dir, snapshots[0])
                        logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜æ¨¡å‹è·¯å¾„: {actual_model_path}")
                        model_name = actual_model_path
            
            self.model = BGEM3FlagModel(
                model_name_or_path=model_name,
                device=device,
                use_fp16=True if 'cuda' in device else False  # GPU ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
            )
            self.dimensions = 1024  # BGE-M3 å›ºå®šä¸º 1024 ç»´
            
            logger.success(f"âœ… BGE-M3 æ¨¡å‹åŠ è½½æˆåŠŸï¼å‘é‡ç»´åº¦: {self.dimensions}")
            if 'cuda' in device:
                logger.info(f"   ğŸš€ GPU åŠ é€Ÿå·²å¯ç”¨ï¼ˆè®¾å¤‡: {device}ï¼‰")
        else:
            # ä½¿ç”¨ sentence-transformers
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                raise ImportError(
                    "sentence-transformers æ¨¡å‹éœ€è¦ sentence-transformers åº“ã€‚è¯·è¿è¡Œ: pip install sentence-transformers"
                )
            logger.info("ğŸ”§ ä½¿ç”¨ sentence-transformers æ¨¡å‹")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info("â³ é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            
            self.model = SentenceTransformer(model_name, device=device)
            self.dimensions = self.model.get_sentence_embedding_dimension()
            
            logger.success(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼å‘é‡ç»´åº¦: {self.dimensions}")
            if 'cuda' in device:
                logger.info(f"   ğŸš€ GPU åŠ é€Ÿå·²å¯ç”¨ï¼ˆè®¾å¤‡: {device}ï¼‰")
    
    def embed_text(self, text: str) -> List[float]:
        """
        å•æ–‡æœ¬ embeddingï¼ˆå…¼å®¹ GeminiEmbeddingClient æ¥å£ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å‘é‡
        """
        return self.embed_query(text)
    
    def embed_query(self, text: str) -> List[float]:
        """
        å•æ–‡æœ¬ embedding
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å‘é‡
        """
        if self.use_bge_m3:
            # BGE-M3 ä½¿ç”¨ encode æ–¹æ³•ï¼Œè¿”å› dense embeddings
            embeddings = self.model.encode([text], return_dense=True)
            vector = embeddings['dense_vecs'][0]
            return vector.tolist()
        else:
            # sentence-transformers
            vector = self.model.encode(text, show_progress_bar=False)
            return vector.tolist()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ embedding
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        return self.embed_batch(texts, batch_size=32)
    
    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        max_workers: int = 1,  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å¹¶å‘ï¼Œä¿æŒæ¥å£å…¼å®¹
        request_delay: float = 0.0  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å»¶è¿Ÿï¼Œä¿æŒæ¥å£å…¼å®¹
    ) -> List[List[float]]:
        """
        æ‰¹é‡å¤„ç†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆGPU å¯ä»¥è®¾ç½®æ›´å¤§ï¼Œå¦‚ 64 æˆ– 128ï¼‰
            max_workers: å¹¶å‘æ•°ï¼ˆæœ¬åœ°æ¨¡å‹ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹ï¼‰
            request_delay: å»¶è¿Ÿæ—¶é—´ï¼ˆæœ¬åœ°æ¨¡å‹ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹ï¼‰
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        logger.info(f"ğŸ“¦ æ‰¹é‡ embedding: {len(texts)} ä¸ªæ–‡æœ¬ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        if self.use_bge_m3:
            # BGE-M3 æ‰¹é‡å¤„ç†
            all_vectors = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                embeddings = self.model.encode(batch, return_dense=True)
                vectors = embeddings['dense_vecs']
                all_vectors.extend([v.tolist() for v in vectors])
            
            logger.success(f"âœ… æ‰¹é‡ embedding å®Œæˆ: {len(all_vectors)} ä¸ªå‘é‡")
            return all_vectors
        else:
            # sentence-transformers æ‰¹é‡å¤„ç†
            all_vectors = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                vectors = self.model.encode(
                    batch,
                    show_progress_bar=False,
                    batch_size=batch_size
                )
                all_vectors.extend([v.tolist() for v in vectors])
            
            logger.success(f"âœ… æ‰¹é‡ embedding å®Œæˆ: {len(all_vectors)} ä¸ªå‘é‡")
            return all_vectors
    
    def embed_chunks(
        self,
        chunks: List[dict],
        text_key: str = 'text',
        batch_size: int = 32,
        max_workers: int = 1,  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å¹¶å‘ï¼Œä¿æŒæ¥å£å…¼å®¹
        request_delay: float = 0.0  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦å»¶è¿Ÿï¼Œä¿æŒæ¥å£å…¼å®¹
    ) -> List[dict]:
        """
        Chunks embedding
        
        Args:
            chunks: chunk åˆ—è¡¨
            text_key: æ–‡æœ¬å­—æ®µå
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆGPU å¯ä»¥è®¾ç½®æ›´å¤§ï¼Œå¦‚ 64 æˆ– 128ï¼‰
            max_workers: å¹¶å‘æ•°ï¼ˆæœ¬åœ°æ¨¡å‹ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹ï¼‰
            request_delay: å»¶è¿Ÿæ—¶é—´ï¼ˆæœ¬åœ°æ¨¡å‹ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹ï¼‰
            
        Returns:
            æ·»åŠ äº† vector å­—æ®µçš„ chunks
        """
        logger.info(f"ğŸ“š å¼€å§‹å¯¹ {len(chunks)} ä¸ª chunks è¿›è¡Œ embedding")
        
        # æå–æ–‡æœ¬
        texts = [chunk[text_key] for chunk in chunks]
        
        # æ‰¹é‡ embedding
        vectors = self.embed_batch(texts, batch_size=batch_size, max_workers=max_workers, request_delay=request_delay)
        
        # æ·»åŠ å‘é‡åˆ° chunks
        embedded_chunks = []
        for chunk, vector in zip(chunks, vectors):
            embedded_chunk = chunk.copy()
            embedded_chunk['vector'] = vector
            embedded_chunks.append(embedded_chunk)
        
        logger.success(f"âœ… Chunks embedding å®Œæˆ: {len(embedded_chunks)} ä¸ª")
        
        return embedded_chunks


if __name__ == "__main__":
    # æµ‹è¯•æœ¬åœ° Embedding
    print("\n" + "="*60)
    print("æµ‹è¯•æœ¬åœ° Embedding æ¨¡å‹")
    print("="*60)
    
    # æµ‹è¯• BGE-M3 æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if FLAG_EMBEDDING_AVAILABLE:
        print("\nã€æµ‹è¯• BGE-M3 æ¨¡å‹ã€‘")
        try:
            client_bge = LocalEmbeddingClient(model_name="BAAI/bge-m3")
            
            # æµ‹è¯•ä¸­æ–‡
            print("\næµ‹è¯•1: ä¸­æ–‡æ–‡æœ¬")
            text_cn = "å¾·å›½è”é‚¦è®®é™¢æ˜¯å¾·å›½çš„æœ€é«˜ç«‹æ³•æœºæ„ã€‚"
            vector_cn = client_bge.embed_query(text_cn)
            print(f"æ–‡æœ¬: {text_cn}")
            print(f"å‘é‡ç»´åº¦: {len(vector_cn)}")
            print(f"å‘é‡å‰5ç»´: {vector_cn[:5]}")
            
            # æµ‹è¯•å¾·è¯­
            print("\næµ‹è¯•2: å¾·è¯­æ–‡æœ¬")
            text_de = "Der Deutsche Bundestag ist das Parlament der Bundesrepublik Deutschland."
            vector_de = client_bge.embed_query(text_de)
            print(f"æ–‡æœ¬: {text_de}")
            print(f"å‘é‡ç»´åº¦: {len(vector_de)}")
            print(f"å‘é‡å‰5ç»´: {vector_de[:5]}")
            
            # æµ‹è¯•æ‰¹é‡
            print("\næµ‹è¯•3: æ‰¹é‡ embedding")
            texts = [
                "ç¤¾æ°‘å…šæ˜¯å¾·å›½å†å²æœ€æ‚ ä¹…çš„æ”¿å…šä¹‹ä¸€ã€‚",
                "åŸºæ°‘ç›Ÿåœ¨å¾·å›½æ”¿æ²»ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚",
                "ç»¿å…šå…³æ³¨ç¯å¢ƒå’Œæ°”å€™é—®é¢˜ã€‚"
            ]
            vectors = client_bge.embed_batch(texts, batch_size=2)
            print(f"æ‰¹é‡å¤„ç†: {len(texts)} ä¸ªæ–‡æœ¬ -> {len(vectors)} ä¸ªå‘é‡")
            
            print("\nâœ… BGE-M3 æµ‹è¯•å®Œæˆï¼")
        except Exception as e:
            print(f"\nâŒ BGE-M3 æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• sentence-transformers æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        print("\nã€æµ‹è¯• sentence-transformers æ¨¡å‹ã€‘")
        try:
            client_st = LocalEmbeddingClient(model_name="paraphrase-multilingual-MiniLM-L12-v2")
            
            # æµ‹è¯•ä¸­æ–‡
            print("\næµ‹è¯•1: ä¸­æ–‡æ–‡æœ¬")
            text_cn = "å¾·å›½è”é‚¦è®®é™¢æ˜¯å¾·å›½çš„æœ€é«˜ç«‹æ³•æœºæ„ã€‚"
            vector_cn = client_st.embed_query(text_cn)
            print(f"æ–‡æœ¬: {text_cn}")
            print(f"å‘é‡ç»´åº¦: {len(vector_cn)}")
            print(f"å‘é‡å‰5ç»´: {vector_cn[:5]}")
            
            print("\nâœ… sentence-transformers æµ‹è¯•å®Œæˆï¼")
        except Exception as e:
            print(f"\nâŒ sentence-transformers æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
