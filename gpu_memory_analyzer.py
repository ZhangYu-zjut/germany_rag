#!/usr/bin/env python3
"""
GPUæ˜¾å­˜åˆ©ç”¨ç‡åˆ†æå’Œä¼˜åŒ–å»ºè®®
"""

import sys
import os
import subprocess
import time
from pathlib import Path
from typing import Dict, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.llm.embeddings import GeminiEmbeddingClient

def get_gpu_memory_info():
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free,utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            gpu_info = []
            for i, line in enumerate(lines):
                parts = [x.strip() for x in line.split(',')]
                if len(parts) >= 5:
                    gpu_info.append({
                        'gpu_id': i,
                        'total_memory': int(parts[0]),
                        'used_memory': int(parts[1]), 
                        'free_memory': int(parts[2]),
                        'utilization': int(parts[3]),
                        'temperature': int(parts[4])
                    })
            return gpu_info
    except Exception as e:
        print(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    return []

def test_batch_sizes(client: GeminiEmbeddingClient, test_texts: List[str]):
    """æµ‹è¯•ä¸åŒbatch sizeçš„æ˜¾å­˜ä½¿ç”¨å’Œæ€§èƒ½"""
    
    print("\nğŸ§ª æµ‹è¯•ä¸åŒbatch sizeçš„æ€§èƒ½")
    print("=" * 60)
    
    # æµ‹è¯•çš„batch sizeåˆ—è¡¨
    batch_sizes = [50, 100, 150, 200, 300, 400, 500, 600, 800]
    results = []
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“Š æµ‹è¯• batch_size = {batch_size}")
        print("-" * 40)
        
        # è·å–æµ‹è¯•å‰GPUçŠ¶æ€
        gpu_before = get_gpu_memory_info()
        if gpu_before:
            print(f"æµ‹è¯•å‰GPUæ˜¾å­˜: {gpu_before[0]['used_memory']}/{gpu_before[0]['total_memory']} MB")
        
        try:
            # é™åˆ¶æµ‹è¯•æ–‡æœ¬æ•°é‡é¿å…è¿‡é•¿æ—¶é—´
            test_batch = test_texts[:min(batch_size, len(test_texts))]
            
            start_time = time.time()
            vectors = client.embed_batch(test_batch, batch_size=batch_size)
            end_time = time.time()
            
            duration = end_time - start_time
            speed = len(test_batch) / duration
            
            # è·å–æµ‹è¯•åGPUçŠ¶æ€
            gpu_after = get_gpu_memory_info()
            memory_used = gpu_after[0]['used_memory'] if gpu_after else 0
            memory_peak = memory_used
            
            result = {
                'batch_size': batch_size,
                'texts_processed': len(test_batch),
                'duration': duration,
                'speed': speed,
                'memory_used': memory_used,
                'memory_total': gpu_after[0]['total_memory'] if gpu_after else 16384,
                'memory_utilization': memory_used / (gpu_after[0]['total_memory'] if gpu_after else 16384) * 100,
                'gpu_utilization': gpu_after[0]['utilization'] if gpu_after else 0,
                'temperature': gpu_after[0]['temperature'] if gpu_after else 0,
                'success': True
            }
            
            results.append(result)
            
            print(f"âœ… æˆåŠŸ")
            print(f"   å¤„ç†æ–‡æœ¬æ•°: {len(test_batch)}")
            print(f"   è€—æ—¶: {duration:.2f}ç§’")
            print(f"   é€Ÿåº¦: {speed:.1f} embeddings/ç§’")
            print(f"   æ˜¾å­˜ä½¿ç”¨: {memory_used} MB ({memory_used/16384*100:.1f}%)")
            print(f"   GPUåˆ©ç”¨ç‡: {gpu_after[0]['utilization'] if gpu_after else 0}%")
            print(f"   GPUæ¸©åº¦: {gpu_after[0]['temperature'] if gpu_after else 0}Â°C")
            
            # å¦‚æœæ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡90%ï¼Œåœæ­¢æµ‹è¯•æ›´å¤§çš„batch size
            if memory_used > 14745:  # 90% of 16GB
                print(f"âš ï¸  æ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡90%ï¼Œåœæ­¢æµ‹è¯•æ›´å¤§batch size")
                break
                
        except Exception as e:
            print(f"âŒ å¤±è´¥: {str(e)}")
            result = {
                'batch_size': batch_size,
                'texts_processed': 0,
                'duration': 0,
                'speed': 0,
                'memory_used': 0,
                'success': False,
                'error': str(e)
            }
            results.append(result)
            
            # å¦‚æœå‡ºç°OOMï¼Œåœæ­¢æµ‹è¯•
            if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                print(f"ğŸš« æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        # ç»™GPUé™æ¸©æ—¶é—´
        time.sleep(2)
    
    return results

def analyze_results(results: List[Dict]):
    """åˆ†ææµ‹è¯•ç»“æœå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®"""
    
    print("\n" + "=" * 60)
    print("ğŸ“Š GPUæ€§èƒ½æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 60)
    
    # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
    successful_results = [r for r in results if r['success']]
    
    if not successful_results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
        return
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨:")
    print("-" * 80)
    print(f"{'Batch Size':<10} {'Speed(emb/s)':<12} {'Memory(MB)':<12} {'Memory%':<10} {'GPU%':<8} {'TempÂ°C':<8}")
    print("-" * 80)
    
    for result in successful_results:
        print(f"{result['batch_size']:<10} "
              f"{result['speed']:<12.1f} "
              f"{result['memory_used']:<12} "
              f"{result['memory_utilization']:<10.1f} "
              f"{result['gpu_utilization']:<8} "
              f"{result['temperature']:<8}")
    
    # æ‰¾åˆ°æœ€ä½³é…ç½®
    best_speed = max(successful_results, key=lambda x: x['speed'])
    best_memory_efficiency = min(successful_results, key=lambda x: x['memory_utilization'])
    
    print(f"\nğŸ¯ æ€§èƒ½åˆ†æ:")
    print(f"   æœ€é«˜é€Ÿåº¦: {best_speed['speed']:.1f} embeddings/ç§’ (batch_size={best_speed['batch_size']})")
    print(f"   å½“å‰åŸºçº¿: 54.7 embeddings/ç§’")
    print(f"   æ€§èƒ½æå‡: {(best_speed['speed'] / 54.7 - 1) * 100:.1f}%")
    
    # æ˜¾å­˜åˆ©ç”¨ç‡åˆ†æ
    max_memory_result = max(successful_results, key=lambda x: x['memory_used'])
    print(f"\nğŸ’¾ æ˜¾å­˜åˆ†æ:")
    print(f"   16GBæ˜¾å­˜æ€»é‡: 16,384 MB")
    print(f"   æœ€å¤§ä½¿ç”¨: {max_memory_result['memory_used']} MB ({max_memory_result['memory_utilization']:.1f}%)")
    print(f"   å‰©ä½™å¯ç”¨: {16384 - max_memory_result['memory_used']} MB")
    
    # æ¨èé…ç½®
    print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
    
    # æ‰¾åˆ°åœ¨å®‰å…¨æ˜¾å­˜èŒƒå›´å†…çš„æœ€ä½³batch size (85%æ˜¾å­˜ä»¥ä¸‹)
    safe_results = [r for r in successful_results if r['memory_utilization'] <= 85]
    if safe_results:
        recommended = max(safe_results, key=lambda x: x['speed'])
        print(f"   æ¨èbatch_size: {recommended['batch_size']}")
        print(f"   é¢„æœŸé€Ÿåº¦: {recommended['speed']:.1f} embeddings/ç§’")
        print(f"   æ˜¾å­˜å®‰å…¨åº¦: {85 - recommended['memory_utilization']:.1f}% ä½™é‡")
    
    # æé™é…ç½® (95%æ˜¾å­˜)
    extreme_results = [r for r in successful_results if r['memory_utilization'] <= 95]
    if extreme_results:
        extreme = max(extreme_results, key=lambda x: x['speed'])
        print(f"   æé™batch_size: {extreme['batch_size']}")
        print(f"   æé™é€Ÿåº¦: {extreme['speed']:.1f} embeddings/ç§’")
        print(f"   âš ï¸  é£é™©: æ˜¾å­˜ä½¿ç”¨ç‡ {extreme['memory_utilization']:.1f}%")
    
    return successful_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPUæ˜¾å­˜åˆ©ç”¨ç‡åˆ†æå’ŒEmbeddingæ€§èƒ½ä¼˜åŒ–")
    print("=" * 60)
    
    # åˆå§‹GPUçŠ¶æ€
    gpu_info = get_gpu_memory_info()
    if gpu_info:
        print(f"\nğŸ–¥ï¸  GPUä¿¡æ¯:")
        for gpu in gpu_info:
            print(f"   GPU {gpu['gpu_id']}: {gpu['total_memory']} MB æ€»æ˜¾å­˜")
            print(f"   å½“å‰ä½¿ç”¨: {gpu['used_memory']} MB ({gpu['used_memory']/gpu['total_memory']*100:.1f}%)")
            print(f"   ç©ºé—²æ˜¾å­˜: {gpu['free_memory']} MB")
            print(f"   GPUåˆ©ç”¨ç‡: {gpu['utilization']}%")
            print(f"   GPUæ¸©åº¦: {gpu['temperature']}Â°C")
    
    # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
    print(f"\nâš¡ åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯...")
    client = GeminiEmbeddingClient(embedding_mode="local")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = [
        f"å¾·å›½è”é‚¦è®®é™¢ç¬¬{i}æ¬¡ä¼šè®®å…³äºå¤–äº¤æ”¿ç­–ã€ç»æµæ”¿ç­–ã€ç¤¾ä¼šä¿éšœå’Œç¯å¢ƒä¿æŠ¤ç­‰é‡è¦è®®é¢˜çš„æ·±å…¥è®¨è®ºï¼Œæ¶‰åŠå¤šä¸ªæ”¿å…šçš„ä¸åŒè§‚ç‚¹å’Œæ”¿ç­–å»ºè®®ã€‚"
        for i in range(1000)  # ç”Ÿæˆ1000æ¡æµ‹è¯•æ–‡æœ¬
    ]
    
    print(f"ğŸ“ å‡†å¤‡äº† {len(test_texts)} æ¡æµ‹è¯•æ–‡æœ¬")
    
    try:
        # è¿è¡Œbatch sizeæµ‹è¯•
        results = test_batch_sizes(client, test_texts)
        
        # åˆ†æç»“æœ
        analyze_results(results)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
