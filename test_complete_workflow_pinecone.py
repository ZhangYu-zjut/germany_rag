#!/usr/bin/env python3
"""
å®Œæ•´Workflowæµ‹è¯•ï¼ˆPineconeç‰ˆæœ¬ï¼‰
ä½¿ç”¨å®Œæ•´çš„workflowæµç¨‹ï¼ˆæ„å›¾åˆ†æã€å‚æ•°æå–ã€ReRankï¼‰ï¼Œä½†æ£€ç´¢ä½¿ç”¨Pinecone
"""

import os
import sys
import time
import json
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

from src.utils.logger import setup_logger
from src.llm.embeddings import GeminiEmbeddingClient
from src.llm.client import GeminiLLMClient
from pinecone import Pinecone
import requests

logger = setup_logger()


class CompletePineconeRAG:
    """å®Œæ•´çš„RAGæµç¨‹ï¼ˆä½¿ç”¨Pineconeï¼‰"""

    def __init__(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        # Embeddingå®¢æˆ·ç«¯
        self.embedding_client = GeminiEmbeddingClient(
            embedding_mode="local",
            model_name="BAAI/bge-m3",
            dimensions=1024
        )

        # Pinecone
        api_key = os.getenv("PINECONE_VECTOR_DATABASE_API_KEY")
        self.pc = Pinecone(api_key=api_key)
        self.index = self.pc.Index("german-bge")

        # LLMå®¢æˆ·ç«¯
        self.llm = GeminiLLMClient(temperature=0.0)

        # Cohere ReRank
        self.cohere_api_key = os.getenv("COHERE_API_KEY")
        self.rerank_model = "rerank-v3.5"

        logger.info("âœ… CompletePineconeRAGåˆå§‹åŒ–å®Œæˆ")

    def extract_parameters(self, question: str) -> dict:
        """å‚æ•°æå–ï¼ˆæ¨¡æ‹ŸExtractNodeï¼‰"""
        # ç®€å•è§„åˆ™æå–
        params = {}

        # æå–å¹´ä»½
        import re
        year_match = re.search(r'20\d{2}', question)
        if year_match:
            params['year'] = year_match.group()

        # æå–å…šæ´¾
        parties = []
        if 'CDU/CSU' in question or 'CDU' in question:
            parties.append('CDU/CSU')
        if 'SPD' in question:
            parties.append('SPD')
        if parties:
            params['parties'] = parties

        # æå–ä¸»é¢˜
        if 'éš¾æ°‘' in question or 'ç§»æ°‘' in question:
            params['topic'] = 'refugee'
        elif 'æ¬§ç›Ÿ' in question:
            params['topic'] = 'EU'

        logger.info(f"ğŸ“ æå–å‚æ•°: {json.dumps(params, ensure_ascii=False)}")
        return params

    def retrieve(self, question: str, params: dict, top_k=20) -> list:
        """æ£€ç´¢ï¼ˆä½¿ç”¨Pinecone + metadataè¿‡æ»¤ï¼‰"""
        # ç”Ÿæˆé—®é¢˜å‘é‡
        query_vector = self.embedding_client.embed_text(question)

        # æ„å»ºæŸ¥è¯¢å‚æ•°
        query_params = {
            "vector": query_vector,
            "top_k": top_k,
            "include_metadata": True
        }

        # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
        filters = []
        if 'year' in params:
            filters.append({"year": {"$eq": params['year']}})

        if 'parties' in params and len(params['parties']) > 0:
            # Pineconeçš„$inè¿‡æ»¤
            filters.append({"group": {"$in": params['parties']}})

        if filters:
            if len(filters) == 1:
                query_params["filter"] = filters[0]
            else:
                query_params["filter"] = {"$and": filters}

        # æŸ¥è¯¢Pinecone
        results = self.index.query(**query_params)

        # æå–æ–‡æ¡£å—
        chunks = []
        for match in results.matches:
            chunk = {
                "id": match.id,
                "score": match.score,
                "text": match.metadata.get("text", ""),
                "metadata": match.metadata
            }
            chunks.append(chunk)

        logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(chunks)} ä¸ªæ–‡æ¡£å— (top_k={top_k})")
        return chunks

    def rerank(self, question: str, chunks: list, top_n=10) -> list:
        """ReRankï¼ˆä½¿ç”¨Cohere APIï¼‰"""
        if not chunks:
            return []

        # å‡†å¤‡æ–‡æ¡£
        documents = [chunk['text'] for chunk in chunks]

        # è°ƒç”¨Cohere ReRank API
        url = "https://api.cohere.com/v2/rerank"
        headers = {
            "Authorization": f"Bearer {self.cohere_api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.rerank_model,
            "query": question,
            "documents": documents,
            "top_n": top_n
        }

        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()

            # æå–rerankç»“æœ
            reranked_chunks = []
            for item in result.get("results", []):
                index = item["index"]
                relevance_score = item["relevance_score"]

                reranked_chunk = chunks[index].copy()
                reranked_chunk["rerank_score"] = relevance_score
                reranked_chunks.append(reranked_chunk)

            logger.info(f"ğŸ¯ ReRankå®Œæˆ: {len(chunks)} â†’ {len(reranked_chunks)} ä¸ªæ–‡æ¡£å—")
            return reranked_chunks

        except Exception as e:
            logger.error(f"âŒ ReRankå¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹ç»“æœï¼ˆå–top_nï¼‰
            return chunks[:top_n]

    def generate_answer(self, question: str, chunks: list) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        if not chunks:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚"

        # æ„å»ºcontext
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.get('metadata', {})
            speaker = metadata.get('speaker', 'æœªçŸ¥')
            date = metadata.get('date', 'æœªçŸ¥')
            group = metadata.get('group', 'æœªçŸ¥')
            text = chunk.get('text', '')

            context_parts.append(
                f"[æ–‡æ¡£{i}] å‘è¨€äºº: {speaker}, å…šæ´¾: {group}, æ—¥æœŸ: {date}\n{text}"
            )

        context = "\n\n".join(context_parts)

        # æ„å»ºprompt
        prompt = f"""è¯·åŸºäºä»¥ä¸‹å¾·å›½è®®ä¼šå‘è¨€è®°å½•å›ç­”é—®é¢˜ã€‚

ã€é—®é¢˜ã€‘
{question}

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºæä¾›çš„èµ„æ–™è¿›è¡Œæ€»ç»“å’Œåˆ†æ
2. å¦‚æœèµ„æ–™ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“å‘è¨€äººã€æ—¥æœŸå’Œå…šæ´¾
4. ä¿æŒå®¢è§‚å’Œå‡†ç¡®

è¯·å›ç­”ï¼š"""

        # è°ƒç”¨LLM (ç›´æ¥ä¼ promptå­—ç¬¦ä¸²ï¼Œå®¢æˆ·ç«¯ä¼šå°è£…æˆæ¶ˆæ¯)
        response = self.llm.invoke(prompt)
        # responseå·²ç»æ˜¯å­—ç¬¦ä¸²ç±»å‹
        return response

    def answer_question(self, question: str) -> dict:
        """å®Œæ•´çš„é—®ç­”æµç¨‹"""
        start_time = time.time()

        # 1. å‚æ•°æå–
        logger.info("ğŸ“‹ æ­¥éª¤1: å‚æ•°æå–")
        params = self.extract_parameters(question)

        # 2. æ£€ç´¢
        logger.info("ğŸ“‹ æ­¥éª¤2: æ£€ç´¢")
        retrieve_start = time.time()
        chunks = self.retrieve(question, params, top_k=20)
        retrieve_time = time.time() - retrieve_start

        # 3. ReRank
        logger.info("ğŸ“‹ æ­¥éª¤3: ReRank")
        rerank_start = time.time()
        reranked_chunks = self.rerank(question, chunks, top_n=10)
        rerank_time = time.time() - rerank_start

        # 4. ç”Ÿæˆç­”æ¡ˆ
        logger.info("ğŸ“‹ æ­¥éª¤4: ç”Ÿæˆç­”æ¡ˆ")
        generate_start = time.time()
        answer = self.generate_answer(question, reranked_chunks)
        generate_time = time.time() - generate_start

        total_time = time.time() - start_time

        return {
            "question": question,
            "params": params,
            "chunks_before_rerank": len(chunks),
            "chunks_after_rerank": len(reranked_chunks),
            "answer": answer,
            "timing": {
                "retrieve": retrieve_time,
                "rerank": rerank_time,
                "generate": generate_time,
                "total": total_time
            },
            "chunks": reranked_chunks
        }


def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´workflow"""

    logger.info("="*80)
    logger.info("ğŸ§ª å®Œæ•´Workflowæµ‹è¯•ï¼ˆPinecone + ReRankï¼‰")
    logger.info("="*80)

    # åˆå§‹åŒ–RAG
    rag = CompletePineconeRAG()

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        {
            "id": "Q1",
            "type": "æ€»ç»“ç±»",
            "question": "è¯·æ€»ç»“2015å¹´å¾·å›½è®®ä¼šå…³äºéš¾æ°‘æ”¿ç­–çš„ä¸»è¦è®¨è®ºå†…å®¹"
        },
        {
            "id": "Q2",
            "type": "å¯¹æ¯”ç±»",
            "question": "CDU/CSUå’ŒSPDåœ¨2015å¹´å¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"
        },
        {
            "id": "Q3",
            "type": "è§‚ç‚¹ç±»",
            "question": "2015å¹´å¾·å›½è®®ä¼šè®®å‘˜å¯¹æ¬§ç›Ÿä¸€ä½“åŒ–çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
        },
        {
            "id": "Q4",
            "type": "äº‹å®æŸ¥è¯¢",
            "question": "2015å¹´å¾·å›½è®®ä¼šæœ‰å“ªäº›é‡è¦æ³•æ¡ˆè¢«è®¨è®ºï¼Ÿ"
        }
    ]

    results = []

    for test_case in test_questions:
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ” æµ‹è¯• {test_case['id']}: {test_case['type']}")
        logger.info(f"   é—®é¢˜: {test_case['question']}")
        logger.info(f"{'='*80}\n")

        try:
            result = rag.answer_question(test_case['question'])

            logger.info(f"âœ… {test_case['id']} å®Œæˆ")
            logger.info(f"   å‚æ•°: {json.dumps(result['params'], ensure_ascii=False)}")
            logger.info(f"   æ£€ç´¢å‰: {result['chunks_before_rerank']} å—")
            logger.info(f"   ReRankå: {result['chunks_after_rerank']} å—")
            logger.info(f"   æ£€ç´¢è€—æ—¶: {result['timing']['retrieve']:.2f}ç§’")
            logger.info(f"   ReRankè€—æ—¶: {result['timing']['rerank']:.2f}ç§’")
            logger.info(f"   ç”Ÿæˆè€—æ—¶: {result['timing']['generate']:.2f}ç§’")
            logger.info(f"   æ€»è€—æ—¶: {result['timing']['total']:.2f}ç§’")

            # æ˜¾ç¤ºç­”æ¡ˆé¢„è§ˆ
            preview = result['answer'][:200] + "..." if len(result['answer']) > 200 else result['answer']
            logger.info(f"\nğŸ“ ç­”æ¡ˆé¢„è§ˆ:\n{preview}\n")

            # æ”¶é›†ç»“æœ
            results.append({
                "question_id": test_case['id'],
                "question_type": test_case['type'],
                **result
            })

        except Exception as e:
            logger.error(f"âŒ {test_case['id']} å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

            results.append({
                "question_id": test_case['id'],
                "question_type": test_case['type'],
                "status": "failed",
                "error": str(e)
            })

    # ä¿å­˜ç»“æœ
    output_file = project_root / "complete_workflow_pinecone_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results)

    return results


def generate_comparison_report(results):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""

    logger.info("ğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")

    report_file = project_root / "WORKFLOW_PINECONE_COMPARISON.md"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# å®Œæ•´Workflow vs ç®€åŒ–è„šæœ¬å¯¹æ¯”æŠ¥å‘Šï¼ˆPineconeï¼‰\n\n")
        f.write("## æµ‹è¯•é…ç½®\n\n")
        f.write("- **æµ‹è¯•æ—¶é—´**: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n")
        f.write("- **æ•°æ®èŒƒå›´**: 2015å¹´å¾·å›½è®®ä¼šæ•°æ®\n")
        f.write("- **Vector DB**: Pinecone (german-bge index)\n")
        f.write("- **Embedding**: BGE-M3 (local, 1024-dim)\n")
        f.write("- **ReRank**: Cohere rerank-v3.5\n")
        f.write("- **LLM**: Gemini 2.5 Pro\n\n")

        f.write("## å®Œæ•´Workflowæµç¨‹\n\n")
        f.write("```\n")
        f.write("é—®é¢˜ â†’ å‚æ•°æå– â†’ æ£€ç´¢(top_k=20) â†’ ReRank(top_n=10) â†’ LLMç”Ÿæˆç­”æ¡ˆ\n")
        f.write("```\n\n")

        f.write("## æµ‹è¯•ç»“æœ\n\n")

        for result in results:
            if result.get('status') == 'failed':
                f.write(f"### {result['question_id']}: {result['question_type']}\n\n")
                f.write(f"**é—®é¢˜**: {result['question']}\n\n")
                f.write(f"**çŠ¶æ€**: âŒ å¤±è´¥\n\n")
                f.write(f"**é”™è¯¯**: {result['error']}\n\n")
                f.write("---\n\n")
                continue

            f.write(f"### {result['question_id']}: {result['question_type']}\n\n")
            f.write(f"**é—®é¢˜**: {result['question']}\n\n")

            f.write(f"#### å¤„ç†æµç¨‹\n\n")
            f.write(f"- **æå–å‚æ•°**: {json.dumps(result['params'], ensure_ascii=False)}\n")
            f.write(f"- **æ£€ç´¢**: {result['chunks_before_rerank']} ä¸ªæ–‡æ¡£å— (top_k=20, å¸¦metadataè¿‡æ»¤)\n")
            f.write(f"- **ReRank**: {result['chunks_after_rerank']} ä¸ªæ–‡æ¡£å— (top_n=10, Cohere rerank-v3.5)\n\n")

            f.write(f"#### æ€§èƒ½æŒ‡æ ‡\n\n")
            f.write(f"- **æ£€ç´¢è€—æ—¶**: {result['timing']['retrieve']:.2f}ç§’\n")
            f.write(f"- **ReRankè€—æ—¶**: {result['timing']['rerank']:.2f}ç§’\n")
            f.write(f"- **ç”Ÿæˆè€—æ—¶**: {result['timing']['generate']:.2f}ç§’\n")
            f.write(f"- **æ€»è€—æ—¶**: {result['timing']['total']:.2f}ç§’\n\n")

            f.write(f"#### å®Œæ•´Workflowç­”æ¡ˆ\n\n")
            f.write(f"```\n{result['answer']}\n```\n\n")

            # æ£€ç´¢åˆ°çš„æ—¶é—´ç‚¹åˆ†æ
            if result['chunks']:
                dates = set()
                for chunk in result['chunks']:
                    metadata = chunk.get('metadata', {})
                    date = metadata.get('date', 'N/A')
                    if date != 'N/A':
                        dates.add(date)

                dates_sorted = sorted(list(dates))
                f.write(f"#### æ£€ç´¢åˆ°çš„æ—¶é—´ç‚¹\n\n")
                f.write(f"- å…± {len(dates_sorted)} ä¸ªä¸åŒæ—¥æœŸ\n")
                for date in dates_sorted[:10]:
                    f.write(f"- {date}\n")
                if len(dates_sorted) > 10:
                    f.write(f"- ... è¿˜æœ‰ {len(dates_sorted) - 10} ä¸ªæ—¥æœŸ\n")
                f.write("\n")

            f.write("---\n\n")

        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("| æŒ‡æ ‡ | å®Œæ•´Workflow | ç®€åŒ–è„šæœ¬ |\n")
        f.write("|------|-------------|--------|\n")

        avg_total = sum(r['timing']['total'] for r in results if 'timing' in r) / len([r for r in results if 'timing' in r])
        f.write(f"| å¹³å‡æ€»è€—æ—¶ | {avg_total:.2f}ç§’ | ~30ç§’ |\n")

        avg_retrieve = sum(r['timing']['retrieve'] for r in results if 'timing' in r) / len([r for r in results if 'timing' in r])
        f.write(f"| å¹³å‡æ£€ç´¢è€—æ—¶ | {avg_retrieve:.2f}ç§’ | ~1ç§’ |\n")

        avg_rerank = sum(r['timing']['rerank'] for r in results if 'timing' in r) / len([r for r in results if 'timing' in r])
        f.write(f"| ReRankè€—æ—¶ | {avg_rerank:.2f}ç§’ | æ—  |\n")

        avg_chunks_before = sum(r['chunks_before_rerank'] for r in results if 'chunks_before_rerank' in r) / len([r for r in results if 'chunks_before_rerank' in r])
        avg_chunks_after = sum(r['chunks_after_rerank'] for r in results if 'chunks_after_rerank' in r) / len([r for r in results if 'chunks_after_rerank' in r])
        f.write(f"| æ–‡æ¡£å—æ•° | æ£€ç´¢{avg_chunks_before:.0f} â†’ ReRank{avg_chunks_after:.0f} | 10ä¸ª |\n")

        f.write("| å‚æ•°æå– | âœ… è‡ªåŠ¨æå–å¹´ä»½ã€å…šæ´¾ã€ä¸»é¢˜ | âŒ æ—  |\n")
        f.write("| å…ƒæ•°æ®è¿‡æ»¤ | âœ… åŸºäºå‚æ•°è¿‡æ»¤ | âœ… æ‰‹åŠ¨æŒ‡å®š |\n")
        f.write("| ReRank | âœ… Cohere rerank-v3.5 | âŒ æ—  |\n\n")

        f.write("## ç­”æ¡ˆè´¨é‡åˆ†æ\n\n")
        f.write("### Q1: éš¾æ°‘æ”¿ç­–æ€»ç»“é—®é¢˜\n\n")
        f.write("**ç®€åŒ–è„šæœ¬ç­”æ¡ˆ**æåˆ°: \"æ£€ç´¢åˆ°çš„å†…å®¹ä»…åŒ…å«2015å¹´5æœˆå’Œ10æœˆä¸¤ä¸ªæ—¶é—´ç‚¹\"\n\n")

        q1_result = next((r for r in results if r['question_id'] == 'Q1'), None)
        if q1_result and q1_result.get('chunks'):
            dates = set()
            for chunk in q1_result['chunks']:
                metadata = chunk.get('metadata', {})
                date = metadata.get('date', 'N/A')
                if date != 'N/A':
                    dates.add(date)

            dates_sorted = sorted(list(dates))
            f.write(f"**å®Œæ•´Workflowæ£€ç´¢åˆ°çš„æ—¶é—´ç‚¹**: {len(dates_sorted)} ä¸ªä¸åŒæ—¥æœŸ\n\n")
            for date in dates_sorted[:20]:
                f.write(f"- {date}\n")
            if len(dates_sorted) > 20:
                f.write(f"- ... è¿˜æœ‰ {len(dates_sorted) - 20} ä¸ªæ—¥æœŸ\n")
            f.write("\n")

            # æ£€æŸ¥æœˆä»½åˆ†å¸ƒ
            months = set()
            for date in dates_sorted:
                if len(date.split('-')) >= 2:
                    month = date.split('-')[1]
                    months.add(month)

            f.write(f"**æœˆä»½è¦†ç›–**: {sorted(list(months))}\n\n")

            if len(months) > 2:
                f.write("âœ… **ç»“è®º**: å®Œæ•´workflowæ£€ç´¢åˆ°äº†æ›´å¤šæœˆä»½çš„æ•°æ®ï¼Œä¸ä»…é™äº5æœˆå’Œ10æœˆã€‚\n\n")
            else:
                f.write("âš ï¸ **ç»“è®º**: å®Œæ•´workflowä¹Ÿä¸»è¦æ£€ç´¢åˆ°5æœˆå’Œ10æœˆçš„æ•°æ®ï¼Œå¯èƒ½è¿™ä¸¤ä¸ªæœˆç¡®å®æ˜¯ä¸»è¦è®¨è®ºæ—¶é—´ã€‚\n\n")

        f.write("## æ€»ä½“è¯„ä¼°\n\n")
        f.write("### å®Œæ•´Workflowä¼˜åŠ¿\n\n")
        f.write("1. **è‡ªåŠ¨å‚æ•°æå–**: æ— éœ€æ‰‹åŠ¨æŒ‡å®šå¹´ä»½ã€å…šæ´¾ç­‰è¿‡æ»¤æ¡ä»¶\n")
        f.write("2. **ReRankä¼˜åŒ–**: ä½¿ç”¨Cohere APIé‡æ–°æ’åºï¼Œæå‡ç›¸å…³æ€§\n")
        f.write("3. **æ›´å¤§æ£€ç´¢èŒƒå›´**: top_k=20åReRankåˆ°10ä¸ªï¼Œæ¯”ç›´æ¥top_k=10æ›´å…¨é¢\n")
        f.write("4. **æ›´å¥½çš„å…ƒæ•°æ®è¿‡æ»¤**: åŸºäºæå–çš„å‚æ•°è‡ªåŠ¨æ„å»ºè¿‡æ»¤æ¡ä»¶\n\n")

        f.write("### ç®€åŒ–è„šæœ¬ä¼˜åŠ¿\n\n")
        f.write("1. **æ›´å¿«é€Ÿåº¦**: æ— ReRankå’Œå‚æ•°æå–ï¼Œç›´æ¥æ£€ç´¢+ç”Ÿæˆ\n")
        f.write("2. **æ›´ç®€å•**: ä»£ç å°‘ï¼Œå®¹æ˜“ç†è§£å’Œè°ƒè¯•\n")
        f.write("3. **é€‚åˆç®€å•é—®é¢˜**: å¯¹äºå•ä¸€ç»´åº¦é—®é¢˜å·²è¶³å¤Ÿ\n\n")

        f.write("## å»ºè®®\n\n")
        f.write("- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨å®Œæ•´Workflowï¼Œç­”æ¡ˆè´¨é‡æ›´é«˜\n")
        f.write("- **å¿«é€ŸåŸå‹**: ä½¿ç”¨ç®€åŒ–è„šæœ¬ï¼Œå¼€å‘è°ƒè¯•æ›´å¿«\n")
        f.write("- **æ··åˆæ–¹æ¡ˆ**: ç®€å•é—®é¢˜ç”¨ç®€åŒ–ç‰ˆï¼Œå¤æ‚é—®é¢˜ç”¨å®Œæ•´ç‰ˆ\n\n")

    logger.info(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


if __name__ == "__main__":
    try:
        results = test_complete_workflow()

        success_count = len([r for r in results if r.get('status') != 'failed'])
        failed_count = len([r for r in results if r.get('status') == 'failed'])

        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ‰ æµ‹è¯•å®Œæˆ!")
        logger.info(f"   æˆåŠŸ: {success_count}")
        logger.info(f"   å¤±è´¥: {failed_count}")
        logger.info(f"{'='*80}\n")

        exit(0 if failed_count == 0 else 1)

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)
