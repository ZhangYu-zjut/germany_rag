#!/usr/bin/env python3
"""
è½»é‡çº§æ€§èƒ½åˆ†æè„šæœ¬
é¿å…ä¸åå°è¿ç§»å†²çªï¼Œåˆ†ææ•°æ®å¤„ç†å„ç¯èŠ‚æ€§èƒ½
"""

import json
import time
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.data_loader.splitter import ParliamentTextSplitter
from src.llm.embeddings import GeminiEmbeddingClient

class LightweightPerformanceAnalyzer:
    """è½»é‡çº§æ€§èƒ½åˆ†æå™¨ï¼ˆä¸ä½¿ç”¨Qdrantï¼Œé¿å…å†²çªï¼‰"""
    
    def __init__(self, sample_size: int = 1000):
        self.sample_size = sample_size
        self.timings = {}
        
        print(f"ğŸ” è½»é‡çº§æ€§èƒ½åˆ†æå™¨ï¼ˆæ ·æœ¬å¤§å°: {sample_size}ï¼‰")
        print("âš ï¸  æ³¨æ„ï¼šä¸ºé¿å…ä¸åå°è¿ç§»å†²çªï¼Œä¸è¿›è¡Œå®é™…æ•°æ®åº“æ“ä½œ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ParliamentTextSplitter(chunk_size=512, chunk_overlap=50)
        
    def start_timer(self, name: str):
        """å¼€å§‹è®¡æ—¶"""
        self.timings[name] = {"start": time.time()}
        
    def end_timer(self, name: str):
        """ç»“æŸè®¡æ—¶"""
        if name in self.timings:
            self.timings[name]["end"] = time.time()
            self.timings[name]["duration"] = self.timings[name]["end"] - self.timings[name]["start"]
            
    def analyze_step1_json_parsing(self, file_path: str):
        """æ­¥éª¤1ï¼šJSONè§£æå’Œæ–‡æœ¬é¢„å¤„ç†"""
        print("\nğŸ“ æ­¥éª¤1ï¼šJSONè§£æå’Œæ–‡æœ¬é¢„å¤„ç†")
        print("-" * 40)
        
        self.start_timer("json_parsing")
        
        # JSONæ–‡ä»¶è¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        transcript = data.get('transcript', [])
        sample_records = transcript[:self.sample_size]
        
        # åŸºç¡€æ–‡æœ¬é¢„å¤„ç†
        processed_records = []
        total_chars = 0
        
        for record in sample_records:
            if isinstance(record, dict):
                text_content = record.get('speech', '')
                if text_content and len(text_content.strip()) >= 50:
                    processed_records.append(record)
                    total_chars += len(text_content)
        
        self.end_timer("json_parsing")
        
        print(f"âœ… JSONè§£æå’Œé¢„å¤„ç†å®Œæˆ")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / (1024*1024):.1f} MB")
        print(f"   - åŸå§‹è®°å½•æ•°: {len(sample_records):,}")
        print(f"   - æœ‰æ•ˆè®°å½•æ•°: {len(processed_records):,}")
        print(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"   - å¤„ç†è€—æ—¶: {self.timings['json_parsing']['duration']:.3f}ç§’")
        print(f"   - å¤„ç†é€Ÿåº¦: {len(processed_records)/self.timings['json_parsing']['duration']:.1f} è®°å½•/ç§’")
        
        return processed_records
        
    def analyze_step2_text_chunking(self, records: List[Dict]):
        """æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—"""
        print("\nâœ‚ï¸ æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—å¤„ç†")
        print("-" * 40)
        
        self.start_timer("text_chunking")
        
        all_chunks = []
        total_chars = 0
        
        for record in records:
            text_content = record.get('speech', '')
            if not text_content:
                continue
                
            # æ–‡æœ¬åˆ†å—
            chunks = self.text_splitter.text_splitter.split_text(text_content)
            valid_chunks = [chunk for chunk in chunks if len(chunk.strip()) >= 30]
            
            for chunk in valid_chunks:
                chunk_data = {
                    "text": chunk,
                    "metadata": record.get("metadata", {})
                }
                all_chunks.append(chunk_data)
            
            total_chars += len(text_content)
            
        self.end_timer("text_chunking")
        
        print(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆ")
        print(f"   - å¤„ç†è®°å½•æ•°: {len(records):,}")
        print(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"   - ç”Ÿæˆchunksæ•°: {len(all_chunks):,}")
        print(f"   - å¹³å‡æ¯æ¡è®°å½•chunksæ•°: {len(all_chunks)/len(records):.1f}")
        print(f"   - åˆ†å—è€—æ—¶: {self.timings['text_chunking']['duration']:.3f}ç§’")
        print(f"   - åˆ†å—é€Ÿåº¦: {len(all_chunks)/self.timings['text_chunking']['duration']:.1f} chunks/ç§’")
        
        return all_chunks
        
    def analyze_step3_data_validation(self, chunks: List[Dict]):
        """æ­¥éª¤3ï¼šæ•°æ®éªŒè¯å’Œè¿‡æ»¤"""
        print("\nğŸ” æ­¥éª¤3ï¼šæ•°æ®éªŒè¯å’Œè¿‡æ»¤")
        print("-" * 40)
        
        self.start_timer("data_validation")
        
        valid_chunks = []
        filtered_count = 0
        
        for chunk_data in chunks:
            # éªŒè¯æ–‡æœ¬å†…å®¹
            text = chunk_data["text"]
            if len(text.strip()) < 30:
                filtered_count += 1
                continue
                
            # éªŒè¯å’Œæ¸…æ´—å…ƒæ•°æ®
            metadata = chunk_data["metadata"]
            try:
                cleaned_metadata = {
                    "year": int(metadata.get("year", 0)) if metadata.get("year") else None,
                    "month": int(metadata.get("month", 0)) if metadata.get("month") else None,
                    "day": int(metadata.get("day", 0)) if metadata.get("day") else None,
                    "speaker": metadata.get("speaker", "").strip(),
                    "party": metadata.get("group", "").strip(),
                    "session": metadata.get("session", "").strip(),
                    "text": text.strip()
                }
                
                # åŸºç¡€éªŒè¯
                if cleaned_metadata["year"] and cleaned_metadata["speaker"]:
                    valid_chunks.append(cleaned_metadata)
                else:
                    filtered_count += 1
                    
            except (ValueError, TypeError):
                filtered_count += 1
                continue
            
        self.end_timer("data_validation")
        
        print(f"âœ… æ•°æ®éªŒè¯å’Œè¿‡æ»¤å®Œæˆ")
        print(f"   - è¾“å…¥chunksæ•°: {len(chunks):,}")
        print(f"   - æœ‰æ•ˆchunksæ•°: {len(valid_chunks):,}")
        print(f"   - è¿‡æ»¤chunksæ•°: {filtered_count:,}")
        print(f"   - æœ‰æ•ˆç‡: {len(valid_chunks)/len(chunks)*100:.1f}%")
        print(f"   - éªŒè¯è€—æ—¶: {self.timings['data_validation']['duration']:.3f}ç§’")
        print(f"   - éªŒè¯é€Ÿåº¦: {len(chunks)/self.timings['data_validation']['duration']:.1f} chunks/ç§’")
        
        return valid_chunks
        
    def analyze_step4_embedding_simulation(self, chunks: List[Dict]):
        """æ­¥éª¤4ï¼šEmbeddingç”Ÿæˆï¼ˆè½»é‡çº§æµ‹è¯•ï¼‰"""
        print("\nğŸ§  æ­¥éª¤4ï¼šEmbeddingç”Ÿæˆï¼ˆå°æ ·æœ¬æµ‹è¯•ï¼‰")
        print("-" * 40)
        
        # åªæµ‹è¯•å‰50ä¸ªchunksä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
        test_chunks = chunks[:50]
        texts_to_embed = [chunk["text"] for chunk in test_chunks]
        
        self.start_timer("embedding_generation")
        
        # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
        embedding_client = GeminiEmbeddingClient(embedding_mode="local")
        
        # ç”Ÿæˆembedding
        vectors = embedding_client.embed_batch(texts_to_embed, batch_size=50)
        
        self.end_timer("embedding_generation")
        
        # è®¡ç®—å•ä¸ªembeddingçš„å¹³å‡æ—¶é—´
        avg_time_per_embedding = self.timings['embedding_generation']['duration'] / len(test_chunks)
        
        print(f"âœ… Embeddingç”Ÿæˆæµ‹è¯•å®Œæˆ")
        print(f"   - æµ‹è¯•æ ·æœ¬æ•°: {len(test_chunks):,}")
        print(f"   - ç”Ÿæˆè€—æ—¶: {self.timings['embedding_generation']['duration']:.3f}ç§’")
        print(f"   - ç”Ÿæˆé€Ÿåº¦: {len(test_chunks)/self.timings['embedding_generation']['duration']:.1f} embeddings/ç§’")
        print(f"   - å•ä¸ªembeddingå¹³å‡æ—¶é—´: {avg_time_per_embedding:.4f}ç§’")
        
        return vectors, avg_time_per_embedding
        
    def analyze_step5_database_simulation(self, chunks: List[Dict], avg_embedding_time: float):
        """æ­¥éª¤5ï¼šæ•°æ®åº“æ’å…¥æ€§èƒ½æ¨¡æ‹Ÿ"""
        print("\nğŸ’¾ æ­¥éª¤5ï¼šæ•°æ®åº“æ’å…¥æ€§èƒ½ï¼ˆæ¨¡æ‹Ÿæµ‹è¯•ï¼‰")
        print("-" * 40)
        
        # æ¨¡æ‹Ÿæ•°æ®åº“æ’å…¥æ“ä½œ
        self.start_timer("db_insertion_simulation")
        
        # æ¨¡æ‹Ÿæ‰¹é‡æ’å…¥è¿‡ç¨‹
        batch_size = 100
        total_chunks = len(chunks)
        
        # æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡æ—¶é—´ï¼ˆåˆ›å»ºpayloadç­‰ï¼‰
        preparation_time = 0
        for i in range(0, min(100, total_chunks), batch_size):
            batch_chunks = chunks[i:i+batch_size]
            
            # æ¨¡æ‹Ÿæ•°æ®åºåˆ—åŒ–å’Œå‡†å¤‡
            start_prep = time.time()
            for chunk in batch_chunks:
                # æ¨¡æ‹Ÿpayloadå‡†å¤‡
                payload = {
                    "text": chunk["text"],
                    "year": chunk.get("year"),
                    "speaker": chunk.get("speaker", ""),
                    "party": chunk.get("party", "")
                }
                # æ¨¡æ‹Ÿå‘é‡IDç”Ÿæˆ
                point_id = hash(chunk["text"]) % 1000000
                
            preparation_time += time.time() - start_prep
            
        self.end_timer("db_insertion_simulation")
        
        # åŸºäºå·²çŸ¥æ€§èƒ½æ•°æ®ä¼°ç®—å®é™…æ’å…¥æ—¶é—´
        # ä»ä¹‹å‰çš„è¿ç§»æ—¥å¿—å¯çŸ¥ï¼š92716ä¸ªç‚¹å¤§çº¦éœ€è¦å‡ åˆ†é’Ÿæ’å…¥æ—¶é—´
        estimated_insertion_rate = 500  # points/ç§’ (ä¿å®ˆä¼°è®¡)
        estimated_insertion_time = total_chunks / estimated_insertion_rate
        
        print(f"âœ… æ•°æ®åº“æ’å…¥åˆ†æå®Œæˆ")
        print(f"   - æ•°æ®å‡†å¤‡è€—æ—¶: {self.timings['db_insertion_simulation']['duration']:.3f}ç§’")
        print(f"   - æ•°æ®å‡†å¤‡é€Ÿåº¦: {100/self.timings['db_insertion_simulation']['duration']:.1f} points/ç§’")
        print(f"   - ä¼°ç®—æ’å…¥é€Ÿåº¦: {estimated_insertion_rate} points/ç§’")
        print(f"   - ä¼°ç®—{total_chunks:,}ä¸ªç‚¹æ’å…¥æ—¶é—´: {estimated_insertion_time:.1f}ç§’")
        
        return estimated_insertion_time
        
    def generate_comprehensive_report(self, total_chunks: int, avg_embedding_time: float, estimated_db_time: float):
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®å¤„ç†æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # è®¡ç®—å„ç¯èŠ‚å®é™…è€—æ—¶
        actual_timings = {}
        
        # æ­¥éª¤1-3çš„å®é™…æµ‹è¯•æ—¶é—´
        for step in ["json_parsing", "text_chunking", "data_validation"]:
            if step in self.timings:
                actual_timings[step] = self.timings[step]["duration"]
        
        # æ­¥éª¤4: æ ¹æ®å°æ ·æœ¬æµ‹è¯•æ¨ç®—å®Œæ•´embeddingæ—¶é—´
        full_embedding_time = total_chunks * avg_embedding_time
        actual_timings["embedding_full"] = full_embedding_time
        
        # æ­¥éª¤5: æ•°æ®åº“æ’å…¥ä¼°ç®—æ—¶é—´
        actual_timings["db_insertion_full"] = estimated_db_time
        
        # è®¡ç®—æ€»æ—¶é—´å’Œæ¯”ä¾‹
        total_time = sum(actual_timings.values())
        
        print(f"\nâ±ï¸  å„ç¯èŠ‚è€—æ—¶åˆ†æï¼ˆå®Œæ•´æ•°æ®æ¨ç®—ï¼‰:")
        print("-" * 50)
        
        step_names = {
            "json_parsing": "1ï¸âƒ£ JSONè§£æå’Œé¢„å¤„ç†",
            "text_chunking": "2ï¸âƒ£ æ–‡æœ¬åˆ†å—å¤„ç†", 
            "data_validation": "3ï¸âƒ£ æ•°æ®éªŒè¯å’Œè¿‡æ»¤",
            "embedding_full": "4ï¸âƒ£ Embeddingç”Ÿæˆ",
            "db_insertion_full": "5ï¸âƒ£ æ•°æ®åº“æ‰¹é‡æ’å…¥"
        }
        
        for step, duration in actual_timings.items():
            percentage = (duration / total_time) * 100 if total_time > 0 else 0
            step_name = step_names.get(step, step)
            
            if duration >= 60:
                time_str = f"{duration/60:>6.1f}åˆ†é’Ÿ"
            else:
                time_str = f"{duration:>8.1f}ç§’"
                
            print(f"{step_name:<25}: {time_str} ({percentage:>5.1f}%)")
            
        print("-" * 50)
        if total_time >= 60:
            print(f"{'ğŸ• æ€»è®¡':<25}: {total_time/60:>6.1f}åˆ†é’Ÿ (100.0%)")
        else:
            print(f"{'ğŸ• æ€»è®¡':<25}: {total_time:>8.1f}ç§’ (100.0%)")
        
        # è¯†åˆ«ä¸»è¦ç“¶é¢ˆ
        max_duration = 0
        bottleneck = ""
        for step, duration in actual_timings.items():
            if duration > max_duration:
                max_duration = duration
                bottleneck = step_names.get(step, step)
                
        print(f"\nğŸ¯ æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
        print(f"   - ä¸»è¦ç“¶é¢ˆ: {bottleneck}")
        print(f"   - ç“¶é¢ˆè€—æ—¶: {max_duration/60:.1f}åˆ†é’Ÿ ({max_duration/total_time*100:.1f}%)")
        
        # å¯¹æ¯”28åˆ†é’Ÿçš„å®é™…æƒ…å†µ
        actual_28_min = 28 * 60  # 28åˆ†é’Ÿ = 1680ç§’
        predicted_time = total_time
        
        print(f"\nğŸ“Š é¢„æµ‹å‡†ç¡®æ€§:")
        print(f"   - å®é™…è¿ç§»æ—¶é—´: {actual_28_min/60:.1f}åˆ†é’Ÿ")
        print(f"   - é¢„æµ‹å¤„ç†æ—¶é—´: {predicted_time/60:.1f}åˆ†é’Ÿ")
        print(f"   - é¢„æµ‹å‡†ç¡®åº¦: {min(predicted_time, actual_28_min)/max(predicted_time, actual_28_min)*100:.1f}%")
        
        return actual_timings

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è½»é‡çº§æ•°æ®å¤„ç†æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    analyzer = LightweightPerformanceAnalyzer(sample_size=1000)
    
    try:
        # ä½¿ç”¨2019å¹´æ•°æ®è¿›è¡Œåˆ†æ
        data_file = "./data/pp_json_49-21/pp_2019.json"
        
        # æ­¥éª¤1: JSONè§£æå’Œé¢„å¤„ç†
        records = analyzer.analyze_step1_json_parsing(data_file)
        
        # æ­¥éª¤2: æ–‡æœ¬åˆ†å—
        chunks = analyzer.analyze_step2_text_chunking(records)
        
        # æ­¥éª¤3: æ•°æ®éªŒè¯
        validated_chunks = analyzer.analyze_step3_data_validation(chunks)
        
        # æ­¥éª¤4: Embeddingç”Ÿæˆï¼ˆå°æ ·æœ¬æµ‹è¯•ï¼‰
        vectors, avg_embedding_time = analyzer.analyze_step4_embedding_simulation(validated_chunks)
        
        # æ­¥éª¤5: æ•°æ®åº“æ’å…¥ï¼ˆæ¨¡æ‹Ÿï¼‰
        estimated_db_time = analyzer.analyze_step5_database_simulation(validated_chunks, avg_embedding_time)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        analyzer.generate_comprehensive_report(len(validated_chunks), avg_embedding_time, estimated_db_time)
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
