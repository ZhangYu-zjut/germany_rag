#!/usr/bin/env python3
"""
å¾·å›½è®®ä¼šRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - Streamlit UIæ¼”ç¤ºç•Œé¢
æ”¯æŒå¾·è¯­å’Œä¸­æ–‡é—®é¢˜è¾“å…¥
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime

import streamlit as st
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
project_root = Path(__file__).parent
sys.path.append(str(project_root))
load_dotenv(project_root / ".env", override=True)

# å¯¼å…¥å¿…è¦æ¨¡å—
from src.utils.logger import setup_logger
from test_langgraph_complete import create_pinecone_workflow
from src.graph.state import create_initial_state

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¾·å›½è®®ä¼šRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ‡©ğŸ‡ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–logger
logger = setup_logger()


# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
        border-bottom: 3px solid #1f77b4;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .question-box {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    .answer-box {
        background-color: #f5f5f5;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #4caf50;
        margin: 1rem 0;
        line-height: 1.8;
    }
    .metadata-box {
        background-color: #fff3e0;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        font-size: 0.9rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #1f77b4;
        color: white;
        font-weight: bold;
        padding: 0.75rem;
        border-radius: 8px;
    }
    .stButton>button:hover {
        background-color: #1565c0;
    }
</style>
""", unsafe_allow_html=True)


def initialize_session_state():
    """åˆå§‹åŒ–session state"""
    if 'workflow' not in st.session_state:
        st.session_state.workflow = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'processing' not in st.session_state:
        st.session_state.processing = False
    if 'selected_question' not in st.session_state:
        st.session_state.selected_question = ""
    # æ·±åº¦åˆ†ææ¨¡å¼
    if 'deep_thinking_mode' not in st.session_state:
        st.session_state.deep_thinking_mode = False


def load_workflow():
    """åŠ è½½å·¥ä½œæµï¼ˆç¼“å­˜ï¼‰"""
    if st.session_state.workflow is None:
        with st.spinner("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ..."):
            try:
                st.session_state.workflow = create_pinecone_workflow()
                logger.info("[Streamlit] Workflowåˆå§‹åŒ–æˆåŠŸ")
                return True
            except Exception as e:
                st.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                logger.error(f"[Streamlit] Workflowåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
    return True


def process_question(question: str):
    """å¤„ç†ç”¨æˆ·é—®é¢˜"""
    if not question.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜")
        return

    # è·å–æ·±åº¦åˆ†ææ¨¡å¼çŠ¶æ€
    deep_thinking_mode = st.session_state.get('deep_thinking_mode', False)

    # æ·»åŠ åˆ°å†å²è®°å½•
    st.session_state.chat_history.append({
        "role": "user",
        "content": question,
        "timestamp": datetime.now().strftime("%H:%M:%S"),
        "deep_mode": deep_thinking_mode  # è®°å½•æ˜¯å¦ä½¿ç”¨æ·±åº¦æ¨¡å¼
    })

    # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„çŠ¶æ€æç¤º
    if deep_thinking_mode:
        status_title = "ğŸ§  æ·±åº¦åˆ†ææ¨¡å¼ - RAGç³»ç»Ÿæ­£åœ¨æ·±åº¦æ€è€ƒ..."
        time_hint = "*ï¼ˆæ·±åº¦åˆ†ææ¨¡å¼å·²å¯ç”¨ï¼Œå°†è¿›è¡Œå…¨é¢çš„çŸ¥è¯†å›¾è°±æ‰©å±•ï¼Œé¢„è®¡éœ€è¦ 3-5 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰*"
    else:
        status_title = "ğŸ¤” RAGç³»ç»Ÿæ­£åœ¨æ€è€ƒ..."
        time_hint = "*ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼Œé¢„è®¡éœ€è¦2-3åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰*"

    # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
    with st.status(status_title, expanded=True) as status:
        start_time = time.time()

        try:
            # æ­¥éª¤ 1
            st.write(f"ğŸ“‹ **æ­¥éª¤ 1/5**: åˆ†æé—®é¢˜æ„å›¾... (0.0ç§’)")
            initial_state = create_initial_state(question, deep_thinking_mode=deep_thinking_mode)
            elapsed_1 = time.time() - start_time

            # æ·±åº¦æ¨¡å¼é¢å¤–æç¤º
            if deep_thinking_mode:
                st.write("ğŸ§  *æ·±åº¦åˆ†ææ¨¡å¼ï¼šå°†å¼ºåˆ¶å¯ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•*")

            # æ­¥éª¤ 2
            st.write(f"ğŸ” **æ­¥éª¤ 2/5**: æå–æŸ¥è¯¢å‚æ•°... ({elapsed_1:.1f}ç§’)")
            time.sleep(0.5)
            elapsed_2 = time.time() - start_time

            # æ­¥éª¤ 3
            st.write(f"âœ‚ï¸ **æ­¥éª¤ 3/5**: åˆ†è§£å¤æ‚é—®é¢˜... ({elapsed_2:.1f}ç§’)")
            time.sleep(0.5)
            elapsed_3 = time.time() - start_time

            # æ­¥éª¤ 4ï¼ˆæœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
            st.write(f"ğŸ” **æ­¥éª¤ 4/5**: æ£€ç´¢ç›¸å…³æ–‡æ¡£... ({elapsed_3:.1f}ç§’)")
            st.write(time_hint)

            # è¿è¡Œå·¥ä½œæµï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
            final_state = st.session_state.workflow.invoke(initial_state)
            elapsed_4 = time.time() - start_time

            # æ­¥éª¤ 5
            st.write(f"ğŸ“ **æ­¥éª¤ 5/5**: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ... ({elapsed_4:.1f}ç§’)")

            total_time = time.time() - start_time
            st.write(f"â±ï¸ **æ€»è€—æ—¶**: {total_time:.1f} ç§’")

            # æ£€æŸ¥é”™è¯¯
            if final_state.get("error"):
                status.update(label="âŒ å¤„ç†å¤±è´¥", state="error")
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{final_state['error']}",
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "error": True
                })
                return

            # æå–ç»“æœ
            final_answer = final_state.get("final_answer", "æœªèƒ½ç”Ÿæˆç­”æ¡ˆ")
            parameters = final_state.get("parameters", {})
            sub_questions = final_state.get("sub_questions", [])
            year_distribution = final_state.get("overall_year_distribution", {})
            retrieval_results = final_state.get("retrieval_results", [])

            # æ·±åº¦åˆ†ææ¨¡å¼ç›¸å…³ä¿¡æ¯
            kg_expansion_info = final_state.get("kg_expansion_info", None)
            reasoning_steps = final_state.get("reasoning_steps", [])

            # è®¡ç®—æ£€ç´¢åˆ°çš„æ–‡æ¡£æ€»æ•°
            total_docs = sum(len(r.get('chunks', [])) for r in retrieval_results)

            status.update(
                label=f"âœ… å®Œæˆï¼è€—æ—¶ {total_time:.1f} ç§’",
                state="complete"
            )

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": final_answer,
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "deep_mode": deep_thinking_mode,  # è®°å½•ä½¿ç”¨çš„æ¨¡å¼
                "metadata": {
                    "time": total_time,
                    "parameters": parameters,
                    "sub_questions": sub_questions,
                    "year_distribution": year_distribution,
                    "total_docs": total_docs,
                    "retrieval_results": retrieval_results,
                    # æ·±åº¦æ¨¡å¼ä¸“å±ä¿¡æ¯
                    "kg_expansion_info": kg_expansion_info,
                    "reasoning_steps": reasoning_steps
                }
            })

            logger.info(f"[Streamlit] é—®é¢˜å¤„ç†æˆåŠŸï¼Œè€—æ—¶ {total_time:.2f}ç§’")

        except Exception as e:
            status.update(label="âŒ å¤„ç†å¤±è´¥", state="error")
            error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
            st.error(error_msg)
            logger.error(f"[Streamlit] é—®é¢˜å¤„ç†å¤±è´¥: {str(e)}")

            st.session_state.chat_history.append({
                "role": "assistant",
                "content": error_msg,
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "error": True
            })


def display_chat_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²"""
    for msg in st.session_state.chat_history:
        role = msg["role"]
        content = msg["content"]
        timestamp = msg.get("timestamp", "")

        if role == "user":
            # ç”¨æˆ·é—®é¢˜
            deep_mode_label = " ğŸ§ " if msg.get("deep_mode") else ""
            st.markdown(f"""
            <div class="question-box">
                <b>ğŸ‘¤ ç”¨æˆ·{deep_mode_label}</b> <span style="color: #999; font-size: 0.85rem;">{timestamp}</span><br/>
                {content}
            </div>
            """, unsafe_allow_html=True)

        else:
            # åŠ©æ‰‹å›ç­”
            if msg.get("error"):
                # é”™è¯¯æ¶ˆæ¯
                st.markdown(f"""
                <div style="background-color: #ffebee; padding: 1rem; border-radius: 10px; border-left: 5px solid #f44336; margin: 1rem 0;">
                    <b>âŒ ç³»ç»Ÿ</b> <span style="color: #999; font-size: 0.85rem;">{timestamp}</span><br/>
                    {content}
                </div>
                """, unsafe_allow_html=True)
            else:
                # æ­£å¸¸ç­”æ¡ˆ
                deep_mode_label = " ğŸ§  æ·±åº¦åˆ†æ" if msg.get("deep_mode") else ""
                st.markdown(f"""
                <div class="answer-box">
                    <b>ğŸ¤– RAGç³»ç»Ÿ{deep_mode_label}</b> <span style="color: #999; font-size: 0.85rem;">{timestamp}</span>
                </div>
                """, unsafe_allow_html=True)

                # æ˜¾ç¤ºç­”æ¡ˆå†…å®¹
                st.markdown(content)

                # æ˜¾ç¤ºå…ƒæ•°æ®ï¼ˆå¯æŠ˜å ï¼‰
                metadata = msg.get("metadata", {})
                if metadata:
                    with st.expander("ğŸ“Š æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯", expanded=False):
                        col1, col2, col3 = st.columns(3)

                        with col1:
                            st.metric("â±ï¸ å¤„ç†æ—¶é—´", f"{metadata.get('time', 0):.1f} ç§’")
                        with col2:
                            st.metric("ğŸ“„ æ£€ç´¢æ–‡æ¡£æ•°", metadata.get('total_docs', 0))
                        with col3:
                            year_dist = metadata.get('year_distribution', {})
                            st.metric("ğŸ“… è¦†ç›–å¹´ä»½", len(year_dist))

                        # æå–å‚æ•°
                        params = metadata.get('parameters', {})
                        if params:
                            st.markdown("**ğŸ“‹ æå–çš„æŸ¥è¯¢å‚æ•°:**")
                            st.json(params)

                        # å­é—®é¢˜
                        sub_qs = metadata.get('sub_questions', [])
                        if sub_qs:
                            st.markdown(f"**âœ‚ï¸ é—®é¢˜åˆ†è§£ ({len(sub_qs)}ä¸ªå­é—®é¢˜):**")
                            for i, sq in enumerate(sub_qs, 1):
                                st.markdown(f"{i}. {sq}")

                        # ã€æ·±åº¦åˆ†ææ¨¡å¼ã€‘æ˜¾ç¤ºæ¨ç†æ­¥éª¤
                        reasoning_steps = metadata.get('reasoning_steps', [])
                        if reasoning_steps:
                            st.markdown("**ğŸ§  æ·±åº¦åˆ†ææ¨ç†è¿‡ç¨‹:**")
                            for step in reasoning_steps:
                                st.markdown(f"- {step}")

                        # ã€æ·±åº¦åˆ†ææ¨¡å¼ã€‘æ˜¾ç¤ºçŸ¥è¯†å›¾è°±æ‰©å±•ä¿¡æ¯
                        kg_info = metadata.get('kg_expansion_info')
                        if kg_info and kg_info.get('triggered'):
                            st.markdown("**ğŸ”— çŸ¥è¯†å›¾è°±æ‰©å±•:**")
                            st.markdown(f"- æ‰©å±•çº§åˆ«: `{kg_info.get('level', 'N/A')}`")
                            st.markdown(f"- è¯„åˆ†: `{kg_info.get('score', 0)}`")

                            # æ˜¾ç¤ºåŒ¹é…çš„ä¸»é¢˜
                            matched_topics = kg_info.get('matched_topics', [])
                            if matched_topics:
                                st.markdown(f"- åŒ¹é…ä¸»é¢˜: {', '.join(matched_topics[:5])}")

                            # æ˜¾ç¤ºæ‰©å±•æŸ¥è¯¢ï¼ˆå¯æŠ˜å ï¼‰
                            expansion_queries = kg_info.get('expansion_queries', [])
                            if expansion_queries:
                                with st.expander(f"æŸ¥çœ‹æ‰©å±•æŸ¥è¯¢ ({len(expansion_queries)}ä¸ª)", expanded=False):
                                    for i, q in enumerate(expansion_queries[:15], 1):
                                        st.markdown(f"{i}. {q}")
                                    if len(expansion_queries) > 15:
                                        st.caption(f"...è¿˜æœ‰ {len(expansion_queries) - 15} ä¸ªæ‰©å±•æŸ¥è¯¢")

                        # å¹´ä»½åˆ†å¸ƒ
                        year_dist = metadata.get('year_distribution', {})
                        if year_dist:
                            st.markdown("**ğŸ“… æ–‡æ¡£å¹´ä»½åˆ†å¸ƒ:**")
                            st.bar_chart(
                                {year: count for year, count in sorted(year_dist.items())}
                            )

                        # æ£€ç´¢æ¥æºï¼ˆå‰5ä¸ªï¼‰
                        retrieval_results = metadata.get('retrieval_results', [])
                        if retrieval_results:
                            st.markdown("**ğŸ” æ£€ç´¢æ¥æºç¤ºä¾‹ (å‰5ä¸ª):**")
                            shown = 0
                            for r in retrieval_results:
                                chunks = r.get('chunks', [])
                                for chunk in chunks[:5-shown]:
                                    chunk_meta = chunk.get('metadata', {})
                                    st.markdown(f"""
                                    <div class="metadata-box">
                                        <b>å¹´ä»½:</b> {chunk_meta.get('year', 'N/A')} |
                                        <b>å…šæ´¾:</b> {chunk_meta.get('group', 'N/A')} |
                                        <b>å‘è¨€äºº:</b> {chunk_meta.get('speaker', 'N/A')}<br/>
                                        <b>ç›¸ä¼¼åº¦:</b> {chunk.get('score', 0):.3f}
                                    </div>
                                    """, unsafe_allow_html=True)
                                    shown += 1
                                    if shown >= 5:
                                        break
                                if shown >= 5:
                                    break


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–
    initialize_session_state()

    # æ ‡é¢˜
    st.markdown('<div class="main-header">ğŸ‡©ğŸ‡ª å¾·å›½è®®ä¼šRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Intelligentes Frage-Antwort-System fÃ¼r Deutsche Bundestagsreden (1949-2025)</div>', unsafe_allow_html=True)

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
        st.markdown("""
        **ç³»ç»Ÿä»‹ç»:**
        - ğŸ“š æ•°æ®èŒƒå›´: 1949-2025å¹´å¾·å›½è”é‚¦è®®é™¢æ¼”è®²
        - ğŸ” æ£€ç´¢æ–¹å¼: æ··åˆæ£€ç´¢ (è¯­ä¹‰ + å…ƒæ•°æ®)
        - ğŸ¤– LLM: Gemini 2.5 Pro
        - ğŸ“Š å‘é‡æ•°æ®åº“: Pinecone (173,355æ–‡æ¡£)

        **æ”¯æŒçš„é—®é¢˜ç±»å‹:**
        - å•å¹´ä»½/å¤šå¹´ä»½æŸ¥è¯¢
        - å…šæ´¾è§‚ç‚¹å¯¹æ¯”
        - æ”¿ç­–å˜åŒ–åˆ†æ
        - å‘è¨€äººè§‚ç‚¹æ€»ç»“

        **ç¤ºä¾‹é—®é¢˜:**
        """)

        # 7ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆå¾·è¯­ç‰ˆ - ä¸test_langgraph_complete.pyä¸€è‡´ï¼‰
        example_questions = [
            "Bitte fassen Sie die wichtigsten VerÃ¤nderungen in der FlÃ¼chtlingspolitik der CDU/CSU seit 2015 zusammen.",
            "Welche Positionen vertraten die verschiedenen Parteien im Deutschen Bundestag 2017 zur Reform des FachkrÃ¤fteeinwanderungsgesetzes?",
            "Was waren die Hauptpositionen und Forderungen der GrÃ¼nen zur Migrationsfrage im Deutschen Bundestag 2015?",
            "Wie haben sich die Diskussionen der verschiedenen Parteien im Deutschen Bundestag Ã¼ber die FamilienzusammenfÃ¼hrung von FlÃ¼chtlingen zwischen 2015 und 2018 entwickelt?",
            "Bitte vergleichen Sie die Positionen der Unionsparteien und der GrÃ¼nen zur Integrationspolitik zwischen 2015 und 2017.",
            "Wie haben sich die Positionen der CDU/CSU zur Migrationspolitik zwischen 2017 und 2019 im Vergleich verÃ¤ndert?",
            "Welche wichtigen Ansichten und VorschlÃ¤ge vertrat die AfD zur FlÃ¼chtlingspolitik im Jahr 2018?"
        ]

        for i, eq in enumerate(example_questions, 1):
            if st.button(
                f"ç¤ºä¾‹ {i}",
                key=f"example_{i}",
                on_click=lambda q=eq: setattr(st.session_state, 'user_input', q)
            ):
                pass  # å›è°ƒå‡½æ•°ç›´æ¥è®¾ç½®user_input

        st.markdown("---")

        if st.button(
            "ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²",
            on_click=lambda: setattr(st.session_state, 'chat_history', [])
        ):
            pass  # å›è°ƒå‡½æ•°å·²ç»æ¸…é™¤äº†å†å²

        st.markdown("---")

        # æ·±åº¦åˆ†ææ¨¡å¼å¼€å…³
        st.markdown("**ğŸ§  æ·±åº¦åˆ†ææ¨¡å¼:**")
        deep_mode = st.toggle(
            "å¯ç”¨æ·±åº¦åˆ†æ",
            value=st.session_state.deep_thinking_mode,
            key="deep_mode_toggle",
            help="å¼€å¯åå°†å¼ºåˆ¶è¿›è¡ŒçŸ¥è¯†å›¾è°±æ‰©å±•ï¼Œè·å–æ›´å…¨é¢çš„æ£€ç´¢ç»“æœã€‚é€‚ç”¨äºå¯¹æ ‡å‡†ç»“æœä¸æ»¡æ„æ—¶ä½¿ç”¨ã€‚"
        )
        st.session_state.deep_thinking_mode = deep_mode

        if deep_mode:
            st.warning("â±ï¸ æ·±åº¦æ¨¡å¼é¢„è®¡éœ€è¦ 3-5 åˆ†é’Ÿ")
            st.caption("å°†å¼ºåˆ¶å¯ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ï¼Œç”Ÿæˆæ›´è¯¦ç»†çš„åˆ†ææŠ¥å‘Š")
        else:
            st.caption("æ ‡å‡†æ¨¡å¼ï¼Œé¢„è®¡ 2-3 åˆ†é’Ÿ")

        st.markdown("---")
        st.markdown("**ç³»ç»ŸçŠ¶æ€:**")
        if st.session_state.workflow:
            st.success("âœ… å·²åˆå§‹åŒ–")
        else:
            st.info("â³ å¾…åˆå§‹åŒ–")

    # åŠ è½½å·¥ä½œæµ
    if not load_workflow():
        st.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return

    # æ˜¾ç¤ºå¯¹è¯å†å²
    if st.session_state.chat_history:
        st.markdown("## ğŸ’¬ å¯¹è¯å†å²")
        display_chat_history()
    else:
        st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨å¾·å›½è®®ä¼šRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚")

    # è¾“å…¥åŒºåŸŸ
    st.markdown("---")
    st.markdown("## â“ è¾“å…¥æ‚¨çš„é—®é¢˜")

    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns([5, 1])

    with col1:
        user_input = st.text_area(
            "æ”¯æŒå¾·è¯­å’Œä¸­æ–‡è¾“å…¥:",
            height=100,
            placeholder="ä¾‹å¦‚: Welche Positionen vertrat die CDU/CSU zur FlÃ¼chtlingspolitik 2015?\næˆ–: 2015å¹´åŸºæ°‘ç›Ÿå¯¹éš¾æ°‘æ”¿ç­–çš„ç«‹åœºæ˜¯ä»€ä¹ˆï¼Ÿ",
            key="user_input"
        )

    with col2:
        st.markdown("<br/>", unsafe_allow_html=True)
        submit_button = st.button("ğŸš€ æäº¤é—®é¢˜", type="primary")

    # å¤„ç†æäº¤
    if submit_button:
        if user_input.strip():
            process_question(user_input)
            st.rerun()
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜")

    # é¡µè„š
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #999; font-size: 0.85rem;">
        Â© 2025 å¾·å›½è®®ä¼šRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ | Powered by LangGraph + Gemini 2.5 Pro + Pinecone
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
